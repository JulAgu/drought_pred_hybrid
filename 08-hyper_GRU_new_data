import os
import numpy as np
import pandas as pd
from sklearn.metrics import f1_score, mean_absolute_error, root_mean_squared_error
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import optuna
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm
import pickle
import utilities

# set up the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
print(torch.cuda.get_device_name(device=None))


EXPE_NAME = "MH_Hyper_GRU"

class HybridModel(nn.Module):
    def __init__(
        self,
        num_numerical_features,
        num_time_series_features,
        hidden_size,
        num_lstm_layers,
        list_unic_cat,
        embedding_dims,
        num_fc_tabular_layers,
        num_fc_combined_layers,
        output_size,
        dropout
    ):
        super(HybridModel, self).__init__()
        self.num_lstm_layers = num_lstm_layers
        self.hidden_size = hidden_size

        self.embeddings = nn.ModuleList(
                [
                    nn.Embedding(num_embeddings=i, embedding_dim=dimension)
                    for i, dimension in zip(list_unic_cat, embedding_dims)
                ]
            )
        
        tabular_total_size = num_numerical_features + sum(embedding_dims)
        tabular_fc_layers = []
        for _ in range(num_fc_tabular_layers):
            tabular_fc_layers.append(nn.Linear(tabular_total_size, tabular_total_size))
            tabular_fc_layers.append(nn.ReLU())
        self.tabular_fc_layers = nn.Sequential(
            *tabular_fc_layers, nn.Linear(tabular_total_size, tabular_total_size)
        )

        # TS branch
        self.lstm = nn.GRU(
            input_size=num_time_series_features,
            hidden_size=hidden_size,
            num_layers=num_lstm_layers,
            batch_first=True,
            dropout=dropout,
        )
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.attention = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)

        combined_fc_layers = []
        input_dim = tabular_total_size + hidden_size

        for _ in range(num_fc_combined_layers):
            combined_fc_layers.append(nn.Linear(input_dim, hidden_size))
            combined_fc_layers.append(nn.ReLU())
            input_dim = hidden_size
        self.combined_fc_layers = nn.Sequential(
            *combined_fc_layers, nn.Linear(hidden_size, output_size)
        )

    def forward(self, time_series_data, numerical_data, categorical_data):
        batch_size = time_series_data.size(0)
        h0 = torch.zeros(self.num_lstm_layers, batch_size, self.hidden_size).to(device)

        time_series_data = time_series_data.to(torch.float32)
        numerical_data = numerical_data.to(torch.float32)
        categorical_data = categorical_data.to(torch.int64)

        embeddings = [emb(categorical_data[:, i]) for i, emb in enumerate(self.embeddings)]
        x_cat = torch.cat(embeddings, dim=1)
        x_tabular = torch.cat((x_cat, numerical_data), dim=1)
        x1 = self.tabular_fc_layers(x_tabular)

        # Pass the time series data through the LSTM and the attention mechanism
        lstm_out, _ = self.lstm(time_series_data, h0,)
        lstm_out = self.layer_norm(lstm_out) # Apply layer normalization
        attn_weights = F.softmax(self.attention(lstm_out), dim=1)
        context_vector = torch.sum(attn_weights * lstm_out, dim=1)
        # Pass the data through the attention mechanism
        # context_vector = lstm_out[:, -1, :]  # Last time step output

        # Combined MLPs and output
        x2 = self.dropout(context_vector)
        x = torch.cat((x1, x2), dim=1)
        x = self.combined_fc_layers(x)
        return x


with open("data/ready_to_use_data.pkl", "rb") as f:
    data = pickle.load(f)
# Split the data
X_time_train = data["X_time_train"]
X_tabular_train = data["X_tabular_train"]
X_tabular_cat_train = data["X_tabular_cat_train"]
y_target_train = data["y_target_train"]
X_time_valid = data["X_time_valid"]
X_tabular_valid = data["X_tabular_validation"]
X_tabular_cat_valid = data["X_tabular_cat_valid"]
y_target_valid = data["y_target_valid"]
X_time_test = data["X_time_test"]
X_tabular_test = data["X_tabular_test"]
X_tabular_cat_test = data["X_tabular_cat_test"]
y_target_test = data["y_target_test"]

# Get the number of unique values for each categorical feature
list_cat = [len(np.unique(X_tabular_cat_train[:,i])) + 1 for i in range(X_tabular_cat_train.shape[1])]

def objective(trial):
    """
    Fonction objectif pour Optuna.
    """
    # Fixing a seed at each iteration to warrant reproducibility
    torch.manual_seed(42)
    np.random.seed(42)

    # set up the device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    print(torch.cuda.get_device_name(device=None))

    # Initializing the model
    output_weeks = 6
    batch_size = trial.suggest_int("batch_size", 64, 128, step=64)
    # Hyperparameters
    epochs = trial.suggest_int("num_epochs_entire", 10, 30, step=5)
    hidden_size = trial.suggest_int("hidden_size", 400, 750, step=50)
    num_lstm_layers = trial.suggest_int("num_lstm_layers", 2, 10)
    num_fc_tabular_layers = trial.suggest_int("num_fc_tabular_layers", 1, 5)
    num_fc_combined_layers = trial.suggest_int("num_fc_combined_layers", 1, 5)
    dropout = trial.suggest_float("dropout", 0.1, 0.6, step=0.1)
    lr = trial.suggest_float("lr", 1e-5, 1e-3, log=True)
    clip = trial.suggest_int("clip", 1, 5)
    # early stop parameters
    early_stop_patience = 10
    early_stop_min_delta = 0.001
    
    #  Prepare the datasets
    train_data = TensorDataset(
        torch.tensor(X_time_train),
        torch.tensor(X_tabular_train),
        torch.tensor(X_tabular_cat_train),
        torch.tensor(y_target_train[:, :output_weeks]),
    )
    valid_data = TensorDataset(
        torch.tensor(X_time_valid),
        torch.tensor(X_tabular_valid),
        torch.tensor(X_tabular_cat_valid),
        torch.tensor(y_target_valid[:, :output_weeks]),
    )

    test_data = TensorDataset(
        torch.tensor(X_time_test),
        torch.tensor(X_tabular_test),
        torch.tensor(X_tabular_cat_test),
        torch.tensor(y_target_test[:, :output_weeks]),
    )

    # DataLoaders with sampler for training and default for validation
    train_loader = DataLoader(
        train_data, batch_size=batch_size, drop_last=False
    )

    valid_loader = DataLoader(
        valid_data, shuffle=False, batch_size=batch_size, drop_last=False
    )

    test_loader = DataLoader(
        test_data, shuffle=False, batch_size=batch_size, drop_last=False
    )

    class2id, id2class = utilities.setup_encoders_targets()
    model_name = f"{EXPE_NAME}_{trial.number}"
    model = HybridModel(
        num_numerical_features=X_tabular_train.shape[-1],
        num_time_series_features=X_time_train.shape[-1],
        hidden_size=hidden_size,
        list_unic_cat=list_cat,
        embedding_dims=[570, 4, 4, 4, 4, 4, 4, 4],
        num_lstm_layers=num_lstm_layers,
        num_fc_tabular_layers=num_fc_tabular_layers,
        num_fc_combined_layers=num_fc_combined_layers,
        output_size=output_weeks,
        dropout=dropout,
    )

    early_stopping = utilities.EarlyStoppingObject(
        patience=early_stop_patience,
        min_delta=early_stop_min_delta,
        verbose=False,
    )
    writer = SummaryWriter(f"{ROOT_TENSORBOARD}{model_name}/")
    
    model.to(device)
    
    loss_function = nn.MSELoss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)
    
    counter = 0
    valid_loss_min = np.inf

    for i in range(epochs):
        for k, (inputs, static, static_cat, labels) in tqdm(enumerate(train_loader),
                                                        desc=f"epoch {i+1}/{epochs}",
                                                        total=len(train_loader),):
            model.train()
            counter += 1
            inputs, labels, static, static_cat = (
                inputs.to(device),
                labels.to(device),
                static.to(device),
                static_cat.to(device),
            )
            
            model.zero_grad()
            output = model(inputs, static, static_cat)
            loss = loss_function(output, labels.float())
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), clip)
            optimizer.step()
            scheduler.step()

            with torch.no_grad():
                if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:
                    val_losses = []
                    model.eval()
                    labels = []
                    preds = []
                    raw_labels = []
                    raw_preds = []
                    for inp, stat, stat_cat, lab in valid_loader:
                        inp, lab, stat, stat_cat = inp.to(device), lab.to(device), stat.to(device), stat_cat.to(device)
                        out = model(inp, stat, stat_cat)
                        val_loss = loss_function(out, lab.float())
                        val_losses.append(val_loss.item())
                        for labs in lab:
                            labels.append([int(l.round()) for l in labs])
                            raw_labels.append([float(l) for l in labs])
                        for pred in out:
                            preds.append([int(p.round()) for p in pred])
                            raw_preds.append([float(p) for p in pred])
                    # log data
                    labels = np.array(labels)
                    preds = np.clip(np.array(preds), 0, 5)
                    raw_preds = np.array(raw_preds)
                    raw_labels = np.array(raw_labels)
                    for i in range(output_weeks):
                        log_dict = {
                            "loss": float(loss),
                            "epoch": counter / len(train_loader),
                            "step": counter,
                            "lr": optimizer.param_groups[0]["lr"],
                            "week": i + 1,
                        }
                        # w = f'week_{i+1}_'
                        w = ""
                        log_dict[f"{w}validation_loss"] = np.mean(val_losses)
                        log_dict[f"{w}macro_f1"] = f1_score(
                            labels[:, i], preds[:, i], average="macro"
                        )
                        log_dict[f"{w}micro_f1"] = f1_score(
                            labels[:, i], preds[:, i], average="micro"
                        )
                        log_dict[f"{w}mae"] = mean_absolute_error(
                            raw_labels[:, i], raw_preds[:, i]
                        )
                        print(log_dict)
                        writer.add_scalars("Loss(MSE)", {'train': loss,
                                                        'validation': log_dict[f"{w}validation_loss"]},
                                                        counter)
                        writer.add_scalars("F1(MSE)", {'macro': log_dict[f"{w}macro_f1"],
                                                    'micro': log_dict[f"{w}micro_f1"]},
                                                    counter)
                        writer.add_scalar("MAE", log_dict[f"{w}mae"],
                                        counter)
                        writer.add_scalar("Learning-Rate", log_dict["lr"],
                                        counter)
                        for j, f1 in enumerate(
                            f1_score(labels[:, i], preds[:, i], average=None)
                        ):
                            log_dict[f"{w}{id2class[j]}_f1"] = f1
                        model.train()
                    if np.mean(val_losses) <= valid_loss_min:
                        torch.save(model.state_dict(), f"{ROOT_MODELS_WEIGHTS}{model_name}.pt")
                        print(
                            "Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...".format(
                                valid_loss_min, np.mean(val_losses)
                            )
                        )
        
                        valid_loss_min = np.mean(val_losses)

        early_stopping(valid_loss_min)
        if early_stopping.early_stop:
            print("Early stopping triggered")
            break

    # Evaluation over the test set
    model.load_state_dict(torch.load(f"{ROOT_MODELS_WEIGHTS}{model_name}.pt", weights_only=True))

    for name, loader in zip(["valid_loader", "test_loader"], [valid_loader, test_loader]):
        dict_map = {
        "y_pred": [],
        "y_pred_rounded": [],
        "y_true": [],
        "week": [],
        }
        i = 0
        for x, static, static_cat, y in tqdm(
            loader,# ou test_loader
            desc="validation predictions...",
        ):
            x, static, static_cat, y = x.to(device), static.to(device), static_cat.to(device), y.to(device)
            with torch.no_grad():
                pred = model(x, static, static_cat).clone().detach()
            for w in range(output_weeks):
                dict_map["y_pred"] += [float(p[w]) for p in pred]
                dict_map["y_pred_rounded"] += [int(p.round()[w]) for p in pred]
                # dict_map["fips"] += [f[1][0] for f in valid_fips[i : i + len(x)]]
                # dict_map["date"] += [f[1][1] for f in valid_fips[i : i + len(x)]]
                dict_map["y_true"] += [float(item[w]) for item in y]
                dict_map["week"] += [w] * len(x)
            i += len(x)
        df = pd.DataFrame(dict_map)
        #Put the title ogf the model
        with open(f"{ROOT_RESULTS}_all_results.txt", "a") as f:
            f.write(f"{model_name}\n")
            f.write(f"-----------------------------------------------------------------------\n")
            f.write(f"-----------------------------------------------------------------------\n")
            f.write(f"{name}\n")
        for w in range(6):
            wdf = df[df['week']==w]
            mae = mean_absolute_error(wdf['y_true'], wdf['y_pred']).round(3)
            f1 = f1_score(wdf['y_true'].round(),wdf['y_pred'].round(), average='macro').round(3)
            with open(f"{ROOT_RESULTS}_all_results.txt", "a") as f:
                f.write(f"Week {w+1} MAE: {mae} F1: {f1}\n")
        
        y_true_roc = df['y_true'].round()
        y_pred_roc = df['y_pred'].round()

        mae = mean_absolute_error(df['y_true'], df['y_pred'])
        rmse = root_mean_squared_error(df['y_true'], df['y_pred'])
        f1 = f1_score(y_true_roc, y_pred_roc, average='macro')

        results = pd.DataFrame({'Model': [f"{name}"], 'MAE': [mae], 'RMSE': [rmse], 'F1': [f1]})
        with open(f"{ROOT_RESULTS}_all_results.txt", "a") as f:
            f.write(results.to_string(index=False))

    return valid_loss_min


def optimize():
    """
    Fonction générique qui'enveloppe les expériences.
    """
    # Optuna study
    # Uncomment when re-running the optimization to delete the previous study
    optuna.delete_study(
        storage="sqlite:///optim_cible.sqlite3",
        study_name=EXPE_NAME,
    )
    study = optuna.create_study(
        storage="sqlite:///optim_cible.sqlite3",
        study_name=EXPE_NAME,
        direction="minimize",
    )
    study.optimize(objective, n_trials=25)  # Adjust the number of trials as needed
    print("\n")
    print("Best trial:")
    trial = study.best_trial
    print("    Number: {}".format(trial.number))
    print("    Value: {}".format(trial.value))

    print("    Params: ")
    for key, value in trial.params.items():
        print("       {}: {}".format(key, value))


if __name__ == "__main__":
    # Paths
    ROOT_RESULTS = f"results/{EXPE_NAME}/"
    ROOT_TENSORBOARD = f"runs/{EXPE_NAME}/"
    ROOT_MODELS_WEIGHTS = f"models/{EXPE_NAME}/"

    # Eliminate the previous results
    os.system(f"rm -rf {ROOT_RESULTS}")
    os.system(f"rm -rf {ROOT_TENSORBOARD}")
    os.system(f"rm -rf {ROOT_MODELS_WEIGHTS}")

    # Create the directories if they don't exist
    os.makedirs(ROOT_RESULTS, exist_ok=True)
    os.makedirs(ROOT_TENSORBOARD, exist_ok=True)
    os.makedirs(ROOT_MODELS_WEIGHTS, exist_ok=True)

    optimize()