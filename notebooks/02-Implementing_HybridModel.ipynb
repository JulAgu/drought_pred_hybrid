{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First contact with the dataset\n",
    "This Notebook has as objective to replicate the results of Minixhofer et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** Just to have a normalized methodology to save and visualize the results of all the experiments trought this projet I add and configure a Tensorboard-SummaryWriter. In the same way I have changed the training cycle to put the results in the tensorboard format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data in a unique dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soil_data.csv', 'train_timeseries', 'counties.geojson', 'test_timeseries', 'validation_timeseries', 'counties.zip']\n"
     ]
    }
   ],
   "source": [
    "filesList = os.listdir('../src')\n",
    "print(filesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDic = {\"train\": pd.read_csv(\"../src/train_timeseries/train_timeseries.csv\"),\n",
    "           \"test\": pd.read_csv(\"../src/test_timeseries/test_timeseries.csv\"),\n",
    "           \"validation\": pd.read_csv(\"../src/validation_timeseries/validation_timeseries.csv\"),\n",
    "           \"soil\" : pd.read_csv(\"../src/soil_data.csv\"),\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fips', 'date', 'PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
       "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
       "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
       "       'WS50M_RANGE', 'score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDic[\"train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: dataDic[k].set_index(['fips', 'date'])\n",
    "    for k in dataDic.keys() if k != \"soil\"\n",
    "}\n",
    "\n",
    "dfs[\"soil\"] = dataDic[\"soil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PRECTOT</th>\n",
       "      <th>PS</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2MDEW</th>\n",
       "      <th>T2MWET</th>\n",
       "      <th>T2M_MAX</th>\n",
       "      <th>T2M_MIN</th>\n",
       "      <th>T2M_RANGE</th>\n",
       "      <th>TS</th>\n",
       "      <th>WS10M</th>\n",
       "      <th>WS10M_MAX</th>\n",
       "      <th>WS10M_MIN</th>\n",
       "      <th>WS10M_RANGE</th>\n",
       "      <th>WS50M</th>\n",
       "      <th>WS50M_MAX</th>\n",
       "      <th>WS50M_MIN</th>\n",
       "      <th>WS50M_RANGE</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1001</th>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>0.22</td>\n",
       "      <td>100.51</td>\n",
       "      <td>9.65</td>\n",
       "      <td>14.74</td>\n",
       "      <td>13.51</td>\n",
       "      <td>13.51</td>\n",
       "      <td>20.96</td>\n",
       "      <td>11.46</td>\n",
       "      <td>9.50</td>\n",
       "      <td>14.65</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.85</td>\n",
       "      <td>6.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>0.20</td>\n",
       "      <td>100.55</td>\n",
       "      <td>10.42</td>\n",
       "      <td>16.69</td>\n",
       "      <td>14.71</td>\n",
       "      <td>14.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>12.61</td>\n",
       "      <td>10.18</td>\n",
       "      <td>16.60</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.60</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>3.65</td>\n",
       "      <td>100.15</td>\n",
       "      <td>11.76</td>\n",
       "      <td>18.49</td>\n",
       "      <td>16.52</td>\n",
       "      <td>16.52</td>\n",
       "      <td>22.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>7.41</td>\n",
       "      <td>18.41</td>\n",
       "      <td>4.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.67</td>\n",
       "      <td>7.53</td>\n",
       "      <td>9.52</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.66</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>15.95</td>\n",
       "      <td>100.29</td>\n",
       "      <td>6.42</td>\n",
       "      <td>11.40</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.10</td>\n",
       "      <td>18.09</td>\n",
       "      <td>2.16</td>\n",
       "      <td>15.92</td>\n",
       "      <td>11.31</td>\n",
       "      <td>3.84</td>\n",
       "      <td>5.67</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.59</td>\n",
       "      <td>6.73</td>\n",
       "      <td>9.31</td>\n",
       "      <td>3.74</td>\n",
       "      <td>5.58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>0.00</td>\n",
       "      <td>101.15</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>10.82</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>13.48</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">56043</th>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>0.16</td>\n",
       "      <td>82.88</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-7.97</td>\n",
       "      <td>-13.49</td>\n",
       "      <td>-12.81</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-13.60</td>\n",
       "      <td>12.21</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.63</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.02</td>\n",
       "      <td>8.58</td>\n",
       "      <td>10.39</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>0.02</td>\n",
       "      <td>83.33</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-14.10</td>\n",
       "      <td>-13.84</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-13.56</td>\n",
       "      <td>11.07</td>\n",
       "      <td>-10.55</td>\n",
       "      <td>6.50</td>\n",
       "      <td>11.43</td>\n",
       "      <td>4.11</td>\n",
       "      <td>7.32</td>\n",
       "      <td>9.92</td>\n",
       "      <td>14.49</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>0.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-7.96</td>\n",
       "      <td>-13.30</td>\n",
       "      <td>-13.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-14.51</td>\n",
       "      <td>14.93</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>4.29</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2.03</td>\n",
       "      <td>4.22</td>\n",
       "      <td>6.56</td>\n",
       "      <td>10.07</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>1.22</td>\n",
       "      <td>82.49</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-7.40</td>\n",
       "      <td>-7.33</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-6.86</td>\n",
       "      <td>10.62</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7.34</td>\n",
       "      <td>1.99</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.28</td>\n",
       "      <td>10.12</td>\n",
       "      <td>3.24</td>\n",
       "      <td>6.89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>0.44</td>\n",
       "      <td>82.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>-11.98</td>\n",
       "      <td>-11.82</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-11.61</td>\n",
       "      <td>10.66</td>\n",
       "      <td>-10.17</td>\n",
       "      <td>2.31</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.37</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19300680 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PRECTOT      PS   QV2M    T2M  T2MDEW  T2MWET  T2M_MAX  \\\n",
       "fips  date                                                                 \n",
       "1001  2000-01-01     0.22  100.51   9.65  14.74   13.51   13.51    20.96   \n",
       "      2000-01-02     0.20  100.55  10.42  16.69   14.71   14.71    22.80   \n",
       "      2000-01-03     3.65  100.15  11.76  18.49   16.52   16.52    22.73   \n",
       "      2000-01-04    15.95  100.29   6.42  11.40    6.09    6.10    18.09   \n",
       "      2000-01-05     0.00  101.15   2.95   3.86   -3.29   -3.20    10.82   \n",
       "...                   ...     ...    ...    ...     ...     ...      ...   \n",
       "56043 2016-12-27     0.16   82.88   1.63  -7.97  -13.49  -12.81    -1.39   \n",
       "      2016-12-28     0.02   83.33   1.41  -8.71  -14.10  -13.84    -2.49   \n",
       "      2016-12-29     0.00   83.75   1.59  -7.96  -13.30  -13.03     0.42   \n",
       "      2016-12-30     1.22   82.49   2.63  -2.94   -7.40   -7.33     3.76   \n",
       "      2016-12-31     0.44   82.19   1.75  -7.56  -11.98  -11.82    -0.95   \n",
       "\n",
       "                  T2M_MIN  T2M_RANGE     TS  WS10M  WS10M_MAX  WS10M_MIN  \\\n",
       "fips  date                                                                 \n",
       "1001  2000-01-01    11.46       9.50  14.65   2.20       2.94       1.49   \n",
       "      2000-01-02    12.61      10.18  16.60   2.52       3.43       1.83   \n",
       "      2000-01-03    15.32       7.41  18.41   4.03       5.33       2.66   \n",
       "      2000-01-04     2.16      15.92  11.31   3.84       5.67       2.08   \n",
       "      2000-01-05    -2.66      13.48   2.65   1.60       2.50       0.52   \n",
       "...                   ...        ...    ...    ...        ...        ...   \n",
       "56043 2016-12-27   -13.60      12.21  -9.41   5.90       7.63       3.61   \n",
       "      2016-12-28   -13.56      11.07 -10.55   6.50      11.43       4.11   \n",
       "      2016-12-29   -14.51      14.93 -10.29   4.29       6.24       2.03   \n",
       "      2016-12-30    -6.86      10.62  -4.14   4.98       7.34       1.99   \n",
       "      2016-12-31   -11.61      10.66 -10.17   2.31       3.47       0.41   \n",
       "\n",
       "                  WS10M_RANGE  WS50M  WS50M_MAX  WS50M_MIN  WS50M_RANGE  score  \n",
       "fips  date                                                                      \n",
       "1001  2000-01-01         1.46   4.85       6.04       3.23         2.81    NaN  \n",
       "      2000-01-02         1.60   5.33       6.13       3.72         2.41    NaN  \n",
       "      2000-01-03         2.67   7.53       9.52       5.87         3.66    NaN  \n",
       "      2000-01-04         3.59   6.73       9.31       3.74         5.58    1.0  \n",
       "      2000-01-05         1.98   2.94       4.85       0.65         4.19    NaN  \n",
       "...                       ...    ...        ...        ...          ...    ...  \n",
       "56043 2016-12-27         4.02   8.58      10.39       5.92         4.47    0.0  \n",
       "      2016-12-28         7.32   9.92      14.49       7.26         7.22    NaN  \n",
       "      2016-12-29         4.22   6.56      10.07       3.20         6.87    NaN  \n",
       "      2016-12-30         5.35   7.28      10.12       3.24         6.89    NaN  \n",
       "      2016-12-31         3.06   3.37       5.26       0.66         4.60    NaN  \n",
       "\n",
       "[19300680 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation pour les données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to encode the cycling feature: year-day, using sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (\n",
    "        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add** A dictionary to encode the categorical soil variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load the data (Modified from Minixhofer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadXY(\n",
    "    df,\n",
    "    random_state=42,\n",
    "    window_size=180, # how many days in the past (default/competition: 180)\n",
    "    target_size=6, # how many weeks into the future (default/competition: 6)\n",
    "    fuse_past=True, # add the past drought observations? (default: True)\n",
    "    return_fips=False, # return the county identifier (do not use for predictions)\n",
    "    encode_season=True, # encode the season using the function above (default: True) \n",
    "    use_prev_year=False, # add observations from 1 year prior?\n",
    "):\n",
    "    dico_trad = {}\n",
    "    df = dfs[df]\n",
    "    soil_df = dfs[\"soil\"]\n",
    "    for cat in [\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\", \"SQ5\", \"SQ6\", \"SQ7\"]:\n",
    "        dico_trad[cat] = {j: i for i,j in enumerate(sorted(soil_df[cat].unique()))}\n",
    "        soil_df[cat] = soil_df[cat].map(dico_trad[cat])\n",
    "    time_data_cols = sorted(\n",
    "        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n",
    "    )\n",
    "    static_data_cols = sorted(\n",
    "        [c for c in soil_df.columns if c not in [\"fips\", \"lat\", \"lon\",\n",
    "                                                 \"SQ1\", \"SQ2\", \"SQ3\",\n",
    "                                                 \"SQ4\", \"SQ5\", \"SQ6\",\n",
    "                                                 \"SQ7\"]]\n",
    "    )\n",
    "    static_cat_cols = sorted(\n",
    "        [c for c in soil_df.columns if c in [\"SQ1\", \"SQ2\", \"SQ3\",\n",
    "                                             \"SQ4\", \"SQ5\", \"SQ6\",\n",
    "                                             \"SQ7\"]]\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=[\"score\"])\n",
    "    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n",
    "    X_static_cat = np.empty((len(df) // window_size, len(static_cat_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if use_prev_year:\n",
    "        add_dim += len(time_data_cols)\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "        if use_prev_year:\n",
    "            add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty(\n",
    "        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n",
    "    )\n",
    "    y_past = np.empty((len(df) // window_size, window_size))\n",
    "    y_target = np.empty((len(df) // window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0) == fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df[\"score\"].values\n",
    "        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n",
    "        X_s_cat = soil_df[soil_df[\"fips\"] == fips][static_cat_cols].values[0]\n",
    "        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n",
    "            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n",
    "            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n",
    "            if use_prev_year:\n",
    "                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n",
    "                    continue\n",
    "                X_time[count, :, -len(time_data_cols) :] = X[\n",
    "                    i - 365 : i + window_size - 365\n",
    "                ]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i : i + window_size])\n",
    "            else:\n",
    "                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n",
    "                    y[i : i + window_size]\n",
    "                )\n",
    "            if encode_season:\n",
    "                enc_dates = [\n",
    "                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n",
    "                ]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n",
    "            temp_y = y[i + window_size : i + window_size + target_size * 7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            X_static_cat[count] = X_s_cat\n",
    "            count += 1\n",
    "    print(f\"loaded {count} samples\")\n",
    "    results = [X_static[:count], X_static_cat[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}\n",
    "\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n",
    "        X_time[:, :, index] = (\n",
    "            scaler_dict[index]\n",
    "            .transform(X_time[:, :, index].reshape(-1, 1))\n",
    "            .reshape(-1, X_time.shape[-2])\n",
    "        )\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:, index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:, index] = (\n",
    "            scaler_dict_static[index]\n",
    "            .transform(X_static[:, index].reshape(-1, 1))\n",
    "            .reshape(1, -1)\n",
    "        )\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n",
    "        y_past[:, :] = (\n",
    "            scaler_dict_past[index]\n",
    "            .transform(y_past.reshape(-1, 1))\n",
    "            .reshape(-1, y_past.shape[-1])\n",
    "        )\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [09:33<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 103390 samples\n",
      "train shape (103390, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [00:51<00:00, 60.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8748 samples\n",
      "validation shape (8748, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:23<00:00,  1.10s/it]\n",
      "100%|██████████| 22/22 [00:00<00:00, 336.59it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 30.02it/s]\n",
      "100%|██████████| 22/22 [00:00<00:00, 8162.29it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tabular_train, X_tabular_cat_train, X_time_train, y_target_train = loadXY(\"train\")\n",
    "print(\"train shape\", X_time_train.shape)\n",
    "X_tabular_validation, X_tabular_cat_validation, X_time_valid, y_target_valid, valid_fips = loadXY(\"validation\", return_fips=True)\n",
    "print(\"validation shape\", X_time_valid.shape)\n",
    "X_tabular_train, X_time_train = normalize(X_tabular_train, X_time_train, fit=True)\n",
    "X_tabular_validation, X_time_valid = normalize(X_tabular_validation, X_time_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.56228848e-01,  8.17409819e-01,  6.51668564e-01, -2.83773085e-01,\n",
       "       -3.14493385e-01,  2.79404984e+01, -1.97667772e-01,  9.97399986e-01,\n",
       "        1.00506757e-01, -3.72835005e-01,  4.31893688e-01,  3.36440100e-02,\n",
       "        1.99835526e-01, -5.95441595e-01,  2.23973455e-01,  2.08807389e-02,\n",
       "       -1.77997076e-01,  5.69166279e-01,  1.13585746e+00,  3.01293900e-01,\n",
       "        0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tabular_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 1., 3., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tabular_cat_train[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.55963303e+00,  5.12135922e-01,  4.82894737e-01, ...,\n",
       "          8.30131398e-01,  6.75855651e-01, -1.32960775e-01],\n",
       "        [ 2.28440367e+00,  4.87864078e-01,  2.34210526e-01, ...,\n",
       "          8.30131398e-01,  6.73180468e-01, -1.44851493e-01],\n",
       "        [-4.58715596e-02,  4.66019417e-01,  3.22368421e-01, ...,\n",
       "          8.30131398e-01,  6.70303333e-01, -1.56694133e-01],\n",
       "        ...,\n",
       "        [-8.25688073e-02,  6.18932039e-01, -3.59210526e-01, ...,\n",
       "          3.91880129e+00, -7.11799149e-01,  9.73796555e-02],\n",
       "        [-8.25688073e-02,  7.18446602e-01, -5.90789474e-01, ...,\n",
       "          3.91880129e+00, -7.10350132e-01,  1.09484829e-01],\n",
       "        [-8.25688073e-02,  7.50000000e-01, -5.00000000e-01, ...,\n",
       "          3.91880129e+00, -7.08695333e-01,  1.21563125e-01]],\n",
       "\n",
       "       [[-8.25688073e-02,  7.03883495e-01, -4.40789474e-01, ...,\n",
       "          3.91871828e+00, -7.06835240e-01,  1.33610987e-01],\n",
       "        [-8.25688073e-02,  6.82038835e-01, -4.31578947e-01, ...,\n",
       "          3.95185238e+00, -7.04770402e-01,  1.45624862e-01],\n",
       "        [-7.33944954e-02,  6.72330097e-01, -3.61842105e-01, ...,\n",
       "          3.98498648e+00, -7.02501427e-01,  1.57601210e-01],\n",
       "        ...,\n",
       "        [ 3.21100917e-01,  5.75242718e-01,  7.60526316e-01, ...,\n",
       "          0.00000000e+00,  6.90734214e-01, -2.43628603e-02],\n",
       "        [-6.42201835e-02,  5.65533981e-01,  7.10526316e-01, ...,\n",
       "          0.00000000e+00,  6.89904860e-01, -3.65272307e-02],\n",
       "        [-8.25688073e-02,  5.53398058e-01,  7.14473684e-01, ...,\n",
       "          0.00000000e+00,  6.88868626e-01, -4.86754469e-02]],\n",
       "\n",
       "       [[-8.25688073e-02,  5.41262136e-01,  7.80263158e-01, ...,\n",
       "          0.00000000e+00,  6.87625817e-01, -6.08039286e-02],\n",
       "        [-8.25688073e-02,  4.61165049e-01,  9.06578947e-01, ...,\n",
       "          0.00000000e+00,  6.86176799e-01, -7.29091017e-02],\n",
       "        [-8.25688073e-02,  4.17475728e-01,  1.05789474e+00, ...,\n",
       "          0.00000000e+00,  6.84522001e-01, -8.49873986e-02],\n",
       "        ...,\n",
       "        [-8.25688073e-02,  5.41262136e-01, -1.05263158e-02, ...,\n",
       "          0.00000000e+00, -7.16152188e-01,  2.43844168e-02],\n",
       "        [-8.25688073e-02,  5.60679612e-01,  6.57894737e-03, ...,\n",
       "          0.00000000e+00, -7.15944697e-01,  3.65757269e-02],\n",
       "        [ 4.58715596e-03,  5.31553398e-01,  2.64473684e-01, ...,\n",
       "          0.00000000e+00, -7.15529775e-01,  4.87616474e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.84403670e+00, -3.64077670e+00, -4.90789474e-01, ...,\n",
       "          0.00000000e+00,  5.43735743e-01, -4.17713183e-01],\n",
       "        [-5.50458716e-02, -3.65533981e+00, -5.07894737e-01, ...,\n",
       "          0.00000000e+00,  5.36234367e-01, -4.27274039e-01],\n",
       "        [ 1.05504587e-01, -3.78883495e+00, -2.69736842e-01, ...,\n",
       "          0.00000000e+00,  5.28571398e-01, -4.36703586e-01],\n",
       "        ...,\n",
       "        [ 6.88073394e-02, -3.96116505e+00, -4.05263158e-01, ...,\n",
       "          6.69583986e-01, -6.09403500e-01,  3.94326669e-01],\n",
       "        [ 1.74770642e+00, -3.83980583e+00, -3.68421053e-01, ...,\n",
       "          7.64503581e-01, -6.02916393e-01,  4.04614914e-01],\n",
       "        [ 3.94495413e-01, -3.76456311e+00, -5.14473684e-01, ...,\n",
       "          8.59423177e-01, -5.96255166e-01,  4.14789308e-01]],\n",
       "\n",
       "       [[ 2.29357798e-01, -3.60194175e+00, -5.85526316e-01, ...,\n",
       "          6.69583986e-01, -5.89421782e-01,  4.24846850e-01],\n",
       "        [-8.25688073e-02, -3.59223301e+00, -6.35526316e-01, ...,\n",
       "          6.69583986e-01, -5.82418254e-01,  4.34784577e-01],\n",
       "        [ 2.88990826e-01, -3.72572816e+00, -6.07894737e-01, ...,\n",
       "          6.69583986e-01, -5.75246648e-01,  4.44599561e-01],\n",
       "        ...,\n",
       "        [ 9.17431193e-03, -3.53883495e+00, -4.73684211e-01, ...,\n",
       "          1.22589061e+00,  5.97674425e-01, -3.36845055e-01],\n",
       "        [-8.25688073e-02, -3.58495146e+00, -4.92105263e-01, ...,\n",
       "          1.22458612e+00,  5.91541242e-01, -3.47351876e-01],\n",
       "        [ 4.58715596e-02, -3.58009709e+00, -3.48684211e-01, ...,\n",
       "          1.22328163e+00,  5.85230167e-01, -3.57750942e-01]],\n",
       "\n",
       "       [[-8.25688073e-02, -3.62864078e+00, -3.26315789e-01, ...,\n",
       "          1.74533941e+00,  5.78743061e-01, -3.68039188e-01],\n",
       "        [-8.25688073e-02, -3.71116505e+00, -2.25000000e-01, ...,\n",
       "          1.65832978e+00,  5.72081834e-01, -3.78213581e-01],\n",
       "        [ 3.64678899e+00, -3.71359223e+00,  1.18421053e-02, ...,\n",
       "          1.57132015e+00,  5.65248450e-01, -3.88271123e-01],\n",
       "        ...,\n",
       "        [ 1.62844037e+00, -3.74029126e+00, -2.63157895e-03, ...,\n",
       "          4.64873583e-03, -6.39162145e-01,  3.41284900e-01],\n",
       "        [ 6.74311927e-01, -3.64805825e+00, -8.28947368e-02, ...,\n",
       "          4.64873583e-03, -6.33573270e-01,  3.52096288e-01],\n",
       "        [-9.17431193e-03, -3.74271845e+00, -9.21052632e-03, ...,\n",
       "          4.64873583e-03, -6.27801241e-01,  3.62809301e-01]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_time_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Really important line !!!\n",
    "list_cat = [dfs[\"soil\"][cat].nunique() for cat in [\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\", \"SQ5\", \"SQ6\", \"SQ7\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categorical_features = 7\n",
    "num_numerical_features = 22\n",
    "num_time_series_features = 21\n",
    "hidden_size = 200\n",
    "num_lstm_layers = 2\n",
    "embedding_dims = 50\n",
    "learning_rate = 7e-5\n",
    "num_fc_tabular_layers = 2\n",
    "num_fc_combined_layers = 2\n",
    "num_epochs_lstm = 10  # Change this\n",
    "num_epochs_entire = 10 # Change this\n",
    "batch_size = 128\n",
    "output_weeks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 128\n",
    "# output_weeks = 6\n",
    "# use_static = True\n",
    "# hidden_dim = 512\n",
    "# n_layers = 2\n",
    "# ffnn_layers = 2\n",
    "# dropout = 0.1\n",
    "# one_cycle = True\n",
    "# lr = 7e-5\n",
    "# epochs = 10\n",
    "# clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_time_train).type(torch.FloatTensor),\n",
    "    torch.tensor(X_tabular_train).type(torch.FloatTensor),\n",
    "    torch.tensor(X_tabular_cat_train).type(torch.LongTensor),\n",
    "    torch.tensor(y_target_train[:, :output_weeks]).type(torch.FloatTensor),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.tensor(X_time_valid).type(torch.FloatTensor),\n",
    "    torch.tensor(X_tabular_validation).type(torch.FloatTensor),\n",
    "    torch.tensor(X_tabular_cat_validation).type(torch.LongTensor),\n",
    "    torch.tensor(y_target_valid[:, :output_weeks]).type(torch.FloatTensor),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_time, X_static, X_static_cat, y_target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 22]),\n",
       " torch.Size([128, 7]),\n",
       " torch.Size([128, 180, 21]),\n",
       " torch.Size([128, 6]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_static.shape, X_static_cat.shape, X_time.shape, y_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(y_target, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_test = nn.CrossEntropyLoss()\n",
    "print(cross_test(torch.tensor([[0.1, 0.2, 0.7]]), torch.tensor([2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_categorical_features,\n",
    "        list_unic_cat,\n",
    "        num_numerical_features,\n",
    "        num_time_series_features,\n",
    "        hidden_size,\n",
    "        num_lstm_layers,\n",
    "        embedding_dims,\n",
    "        num_fc_tabular_layers,\n",
    "        num_fc_combined_layers,\n",
    "        output_size,\n",
    "    ):\n",
    "        super(HybridModel, self).__init__()\n",
    "\n",
    "        # Embeddings for categorical variables\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(num_embeddings=i, embedding_dim=embedding_dims)\n",
    "                for i in list_unic_cat\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        total_embedding_dim = num_categorical_features * embedding_dims\n",
    "\n",
    "        # Tabular part: dinamic creation of layers\n",
    "        tabular_fc_layers = []\n",
    "        input_size = total_embedding_dim + num_numerical_features\n",
    "        for _ in range(num_fc_tabular_layers):\n",
    "            tabular_fc_layers.append(nn.Linear(input_size, 128))\n",
    "            tabular_fc_layers.append(nn.ReLU())\n",
    "            input_size = 128\n",
    "        self.tabular_fc_layers = nn.Sequential(\n",
    "            *tabular_fc_layers, nn.Linear(128, 64), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Temporal series\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_time_series_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Atenttion\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # Combined part\n",
    "        self.fc_after_context = nn.Linear(hidden_size, 64)\n",
    "        combined_fc_layers = []\n",
    "        input_dim = (64 + 64)  # Assuming 128 from tabular output and 64 from LSTM output after attention\n",
    "        for _ in range(num_fc_combined_layers):\n",
    "            combined_fc_layers.append(nn.Linear(input_dim, 64))\n",
    "            combined_fc_layers.append(nn.ReLU())\n",
    "            input_dim = 64\n",
    "        self.combined_fc_layers = nn.Sequential(\n",
    "            *combined_fc_layers, nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, categorical_data, numerical_data, time_series_data):\n",
    "        # Embeddings for categorical data\n",
    "        embeddings = [\n",
    "            emb(categorical_data[:, i]) for i, emb in enumerate(self.embeddings)\n",
    "        ]\n",
    "        x_cat = torch.cat(embeddings, dim=1)\n",
    "\n",
    "        # Concatenate categorical and numerical data\n",
    "        x_tabular = torch.cat((x_cat, numerical_data), dim=1)\n",
    "\n",
    "        # Pass the tabular data through FC layers\n",
    "        x1 = self.tabular_fc_layers(x_tabular)\n",
    "\n",
    "        # Pass the time series data through the LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(time_series_data)\n",
    "        # Pass the data through the attention mechanism\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        droped_out = self.dropout(context_vector)\n",
    "\n",
    "        x2 = torch.relu(self.fc_after_context(droped_out))\n",
    "\n",
    "        # Concatenate the outputs from the tabular and the temporal data and pass it through FC layers\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.combined_fc_layers(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(808, 69)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I initialize a Tensorboard Writer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('../logs/HybridModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  50%|█████     | 407/808 [00:34<03:34,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5886046886444092, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 1, 'validation_loss': np.float64(1.1733881108786748), 'macro_f1': np.float64(0.13287963153914897), 'micro_f1': np.float64(0.6628943758573388), 'mae': np.float64(0.5994869498093328)}\n",
      "{'loss': 1.5886046886444092, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 2, 'validation_loss': np.float64(1.1733881108786748), 'macro_f1': np.float64(0.13275553721282157), 'micro_f1': np.float64(0.6618655692729767), 'mae': np.float64(0.5980574556242103)}\n",
      "{'loss': 1.5886046886444092, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 3, 'validation_loss': np.float64(1.1733881108786748), 'macro_f1': np.float64(0.13290718720645175), 'micro_f1': np.float64(0.6631229995427527), 'mae': np.float64(0.7155682356324623)}\n",
      "{'loss': 1.5886046886444092, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 4, 'validation_loss': np.float64(1.1733881108786748), 'macro_f1': np.float64(0.1332921726006723), 'micro_f1': np.float64(0.6663237311385459), 'mae': np.float64(0.6055416743988472)}\n",
      "{'loss': 1.5886046886444092, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 5, 'validation_loss': np.float64(1.1733881108786748), 'macro_f1': np.float64(0.1334156040038393), 'micro_f1': np.float64(0.6673525377229081), 'mae': np.float64(0.6170416723040059)}\n",
      "{'loss': 1.5886046886444092, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 6, 'validation_loss': np.float64(1.1733881108786748), 'macro_f1': np.float64(0.13327844859129162), 'micro_f1': np.float64(0.6662094192958391), 'mae': np.float64(0.6046594905803891)}\n",
      "Validation loss decreased (inf --> 1.173388).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 808/808 [01:09<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3227063417434692, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 1, 'validation_loss': np.float64(0.8102497981078383), 'macro_f1': np.float64(0.16948519547272592), 'micro_f1': np.float64(0.5555555555555556), 'mae': np.float64(0.6751894175637752)}\n",
      "{'loss': 1.3227063417434692, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 2, 'validation_loss': np.float64(0.8102497981078383), 'macro_f1': np.float64(0.10103908649617287), 'micro_f1': np.float64(0.21250571559213535), 'mae': np.float64(0.7774371078244114)}\n",
      "{'loss': 1.3227063417434692, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 3, 'validation_loss': np.float64(0.8102497981078383), 'macro_f1': np.float64(0.17090317825863088), 'micro_f1': np.float64(0.5254915409236397), 'mae': np.float64(0.6692232408236752)}\n",
      "{'loss': 1.3227063417434692, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 4, 'validation_loss': np.float64(0.8102497981078383), 'macro_f1': np.float64(0.0817626406126978), 'micro_f1': np.float64(0.23285322359396435), 'mae': np.float64(0.7243659746984191)}\n",
      "{'loss': 1.3227063417434692, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 5, 'validation_loss': np.float64(0.8102497981078383), 'macro_f1': np.float64(0.09497882064532692), 'micro_f1': np.float64(0.19135802469135801), 'mae': np.float64(0.8157771883437107)}\n",
      "{'loss': 1.3227063417434692, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 6, 'validation_loss': np.float64(0.8102497981078383), 'macro_f1': np.float64(0.07783458914335621), 'micro_f1': np.float64(0.22062185642432555), 'mae': np.float64(0.7386359166913689)}\n",
      "Validation loss decreased (1.173388 --> 0.810250).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:  50%|█████     | 407/808 [00:35<04:03,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.682614266872406, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 1, 'validation_loss': np.float64(0.5924713624560315), 'macro_f1': np.float64(0.261688681993346), 'micro_f1': np.float64(0.5692729766803841), 'mae': np.float64(0.5658560298058959)}\n",
      "{'loss': 0.682614266872406, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 2, 'validation_loss': np.float64(0.5924713624560315), 'macro_f1': np.float64(0.2834471956564339), 'micro_f1': np.float64(0.569387288523091), 'mae': np.float64(0.5671519964969427)}\n",
      "{'loss': 0.682614266872406, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 3, 'validation_loss': np.float64(0.5924713624560315), 'macro_f1': np.float64(0.2819904124046037), 'micro_f1': np.float64(0.5775034293552812), 'mae': np.float64(0.5396382211391137)}\n",
      "{'loss': 0.682614266872406, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 4, 'validation_loss': np.float64(0.5924713624560315), 'macro_f1': np.float64(0.24827125363528277), 'micro_f1': np.float64(0.5497256515775034), 'mae': np.float64(0.6119000759352913)}\n",
      "{'loss': 0.682614266872406, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 5, 'validation_loss': np.float64(0.5924713624560315), 'macro_f1': np.float64(0.26125887440885065), 'micro_f1': np.float64(0.5606995884773662), 'mae': np.float64(0.60470331336275)}\n",
      "{'loss': 0.682614266872406, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 6, 'validation_loss': np.float64(0.5924713624560315), 'macro_f1': np.float64(0.2541606133031151), 'micro_f1': np.float64(0.5625285779606767), 'mae': np.float64(0.5925206028796481)}\n",
      "Validation loss decreased (0.810250 --> 0.592471).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 808/808 [01:10<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.42485830187797546, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 1, 'validation_loss': np.float64(0.40237428831017535), 'macro_f1': np.float64(0.4425948179304811), 'micro_f1': np.float64(0.6966163694558757), 'mae': np.float64(0.38199450352082986)}\n",
      "{'loss': 0.42485830187797546, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 2, 'validation_loss': np.float64(0.40237428831017535), 'macro_f1': np.float64(0.41247041125500034), 'micro_f1': np.float64(0.6791266575217193), 'mae': np.float64(0.40770335698466265)}\n",
      "{'loss': 0.42485830187797546, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 3, 'validation_loss': np.float64(0.40237428831017535), 'macro_f1': np.float64(0.39126910417309957), 'micro_f1': np.float64(0.6558070416095108), 'mae': np.float64(0.4355482700787529)}\n",
      "{'loss': 0.42485830187797546, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 4, 'validation_loss': np.float64(0.40237428831017535), 'macro_f1': np.float64(0.36317313808159096), 'micro_f1': np.float64(0.6228852309099223), 'mae': np.float64(0.481466646506768)}\n",
      "{'loss': 0.42485830187797546, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 5, 'validation_loss': np.float64(0.40237428831017535), 'macro_f1': np.float64(0.3399491980986533), 'micro_f1': np.float64(0.6239140374942844), 'mae': np.float64(0.508554957286265)}\n",
      "{'loss': 0.42485830187797546, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 6, 'validation_loss': np.float64(0.40237428831017535), 'macro_f1': np.float64(0.32568603566023946), 'micro_f1': np.float64(0.6060813900320073), 'mae': np.float64(0.5276737166982746)}\n",
      "Validation loss decreased (0.592471 --> 0.402374).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  50%|█████     | 407/808 [00:35<04:09,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3733770251274109, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 1, 'validation_loss': np.float64(0.3048547545744889), 'macro_f1': np.float64(0.5464634831105389), 'micro_f1': np.float64(0.7767489711934157), 'mae': np.float64(0.26841535920413584)}\n",
      "{'loss': 0.3733770251274109, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 2, 'validation_loss': np.float64(0.3048547545744889), 'macro_f1': np.float64(0.49798969445748814), 'micro_f1': np.float64(0.7481710105166895), 'mae': np.float64(0.3013880558338603)}\n",
      "{'loss': 0.3733770251274109, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 3, 'validation_loss': np.float64(0.3048547545744889), 'macro_f1': np.float64(0.46544074940528857), 'micro_f1': np.float64(0.7256515775034293), 'mae': np.float64(0.34565562713968484)}\n",
      "{'loss': 0.3733770251274109, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 4, 'validation_loss': np.float64(0.3048547545744889), 'macro_f1': np.float64(0.4160213408815063), 'micro_f1': np.float64(0.6912437128486512), 'mae': np.float64(0.39175673495481655)}\n",
      "{'loss': 0.3733770251274109, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 5, 'validation_loss': np.float64(0.3048547545744889), 'macro_f1': np.float64(0.3923944302396302), 'micro_f1': np.float64(0.67466849565615), 'mae': np.float64(0.43598764617027624)}\n",
      "{'loss': 0.3733770251274109, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 6, 'validation_loss': np.float64(0.3048547545744889), 'macro_f1': np.float64(0.3530618763128979), 'micro_f1': np.float64(0.6539780521262003), 'mae': np.float64(0.4652160566678492)}\n",
      "Validation loss decreased (0.402374 --> 0.304855).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 808/808 [01:10<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.30743539333343506, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 1, 'validation_loss': np.float64(0.26335468391577405), 'macro_f1': np.float64(0.677171212230751), 'micro_f1': np.float64(0.8305898491083676), 'mae': np.float64(0.21467656809979582)}\n",
      "{'loss': 0.30743539333343506, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 2, 'validation_loss': np.float64(0.26335468391577405), 'macro_f1': np.float64(0.6016733911469613), 'micro_f1': np.float64(0.7904663923182441), 'mae': np.float64(0.2647743278932365)}\n",
      "{'loss': 0.30743539333343506, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 3, 'validation_loss': np.float64(0.26335468391577405), 'macro_f1': np.float64(0.5290282101533196), 'micro_f1': np.float64(0.7525148605395519), 'mae': np.float64(0.3140586959235387)}\n",
      "{'loss': 0.30743539333343506, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 4, 'validation_loss': np.float64(0.26335468391577405), 'macro_f1': np.float64(0.4584746797322336), 'micro_f1': np.float64(0.7187928669410151), 'mae': np.float64(0.362030385148021)}\n",
      "{'loss': 0.30743539333343506, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 5, 'validation_loss': np.float64(0.26335468391577405), 'macro_f1': np.float64(0.4216446736362283), 'micro_f1': np.float64(0.6952446273433928), 'mae': np.float64(0.4045464236295316)}\n",
      "{'loss': 0.30743539333343506, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 6, 'validation_loss': np.float64(0.26335468391577405), 'macro_f1': np.float64(0.37724694653323576), 'micro_f1': np.float64(0.6728395061728395), 'mae': np.float64(0.44113675497523563)}\n",
      "Validation loss decreased (0.304855 --> 0.263355).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:  50%|█████     | 407/808 [00:35<04:10,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3117940425872803, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 1, 'validation_loss': np.float64(0.24230611632051674), 'macro_f1': np.float64(0.7175195230469368), 'micro_f1': np.float64(0.848079561042524), 'mae': np.float64(0.18519502068489782)}\n",
      "{'loss': 0.3117940425872803, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 2, 'validation_loss': np.float64(0.24230611632051674), 'macro_f1': np.float64(0.6527466926364504), 'micro_f1': np.float64(0.80612711476909), 'mae': np.float64(0.2312402873451452)}\n",
      "{'loss': 0.3117940425872803, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 3, 'validation_loss': np.float64(0.24230611632051674), 'macro_f1': np.float64(0.5937945261296119), 'micro_f1': np.float64(0.771490626428898), 'mae': np.float64(0.27802060277496854)}\n",
      "{'loss': 0.3117940425872803, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 4, 'validation_loss': np.float64(0.24230611632051674), 'macro_f1': np.float64(0.5149145145847046), 'micro_f1': np.float64(0.7381115683584819), 'mae': np.float64(0.32870376035203575)}\n",
      "{'loss': 0.3117940425872803, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 5, 'validation_loss': np.float64(0.24230611632051674), 'macro_f1': np.float64(0.4754171000639755), 'micro_f1': np.float64(0.71639231824417), 'mae': np.float64(0.37778306694699676)}\n",
      "{'loss': 0.3117940425872803, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 6, 'validation_loss': np.float64(0.24230611632051674), 'macro_f1': np.float64(0.40643088256728027), 'micro_f1': np.float64(0.6923868312757202), 'mae': np.float64(0.41641757133285423)}\n",
      "Validation loss decreased (0.263355 --> 0.242306).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 808/808 [01:11<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.19234295189380646, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 1, 'validation_loss': np.float64(0.23017254412390184), 'macro_f1': np.float64(0.7061783971928953), 'micro_f1': np.float64(0.8568815729309557), 'mae': np.float64(0.17576925486526657)}\n",
      "{'loss': 0.19234295189380646, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 2, 'validation_loss': np.float64(0.23017254412390184), 'macro_f1': np.float64(0.6541712183399417), 'micro_f1': np.float64(0.8155006858710563), 'mae': np.float64(0.2241289531294612)}\n",
      "{'loss': 0.19234295189380646, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 3, 'validation_loss': np.float64(0.23017254412390184), 'macro_f1': np.float64(0.5911507491941492), 'micro_f1': np.float64(0.7789208962048468), 'mae': np.float64(0.27622370832890636)}\n",
      "{'loss': 0.19234295189380646, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 4, 'validation_loss': np.float64(0.23017254412390184), 'macro_f1': np.float64(0.5044185831716042), 'micro_f1': np.float64(0.7413122999542753), 'mae': np.float64(0.32814627360316184)}\n",
      "{'loss': 0.19234295189380646, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 5, 'validation_loss': np.float64(0.23017254412390184), 'macro_f1': np.float64(0.4458669019163344), 'micro_f1': np.float64(0.7155921353452218), 'mae': np.float64(0.3754570873294187)}\n",
      "{'loss': 0.19234295189380646, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 6, 'validation_loss': np.float64(0.23017254412390184), 'macro_f1': np.float64(0.39268254801785735), 'micro_f1': np.float64(0.6911294010059442), 'mae': np.float64(0.41393907363594407)}\n",
      "Validation loss decreased (0.242306 --> 0.230173).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:  50%|█████     | 407/808 [00:35<04:09,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2893592417240143, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 1, 'validation_loss': np.float64(0.2354024387896061), 'macro_f1': np.float64(0.7246549282222013), 'micro_f1': np.float64(0.8559670781893004), 'mae': np.float64(0.1641908312147015)}\n",
      "{'loss': 0.2893592417240143, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 2, 'validation_loss': np.float64(0.2354024387896061), 'macro_f1': np.float64(0.6596593924707699), 'micro_f1': np.float64(0.8161865569272977), 'mae': np.float64(0.20880640162532907)}\n",
      "{'loss': 0.2893592417240143, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 3, 'validation_loss': np.float64(0.2354024387896061), 'macro_f1': np.float64(0.6135120379625463), 'micro_f1': np.float64(0.7800640146319159), 'mae': np.float64(0.25736719962217214)}\n",
      "{'loss': 0.2893592417240143, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 4, 'validation_loss': np.float64(0.2354024387896061), 'macro_f1': np.float64(0.5325239975992807), 'micro_f1': np.float64(0.7456561499771376), 'mae': np.float64(0.3070300914848707)}\n",
      "{'loss': 0.2893592417240143, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 5, 'validation_loss': np.float64(0.2354024387896061), 'macro_f1': np.float64(0.471221460502506), 'micro_f1': np.float64(0.7197073616826704), 'mae': np.float64(0.35098103858862745)}\n",
      "{'loss': 0.2893592417240143, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 6, 'validation_loss': np.float64(0.2354024387896061), 'macro_f1': np.float64(0.39819751810631576), 'micro_f1': np.float64(0.695016003657979), 'mae': np.float64(0.3931730293367922)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 808/808 [01:11<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.36580950021743774, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 1, 'validation_loss': np.float64(0.22500768336264984), 'macro_f1': np.float64(0.7538028809696051), 'micro_f1': np.float64(0.8691129401005944), 'mae': np.float64(0.17147523218675237)}\n",
      "{'loss': 0.36580950021743774, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 2, 'validation_loss': np.float64(0.22500768336264984), 'macro_f1': np.float64(0.6861334768543887), 'micro_f1': np.float64(0.8256744398719708), 'mae': np.float64(0.21945650411844114)}\n",
      "{'loss': 0.36580950021743774, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 3, 'validation_loss': np.float64(0.22500768336264984), 'macro_f1': np.float64(0.6412393546174723), 'micro_f1': np.float64(0.7862368541380887), 'mae': np.float64(0.27202264996668024)}\n",
      "{'loss': 0.36580950021743774, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 4, 'validation_loss': np.float64(0.22500768336264984), 'macro_f1': np.float64(0.5701103849917297), 'micro_f1': np.float64(0.7450845907636031), 'mae': np.float64(0.3241776072291551)}\n",
      "{'loss': 0.36580950021743774, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 5, 'validation_loss': np.float64(0.22500768336264984), 'macro_f1': np.float64(0.5028953295331123), 'micro_f1': np.float64(0.7178783721993599), 'mae': np.float64(0.3724768314930984)}\n",
      "{'loss': 0.36580950021743774, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 6, 'validation_loss': np.float64(0.22500768336264984), 'macro_f1': np.float64(0.45037567437349724), 'micro_f1': np.float64(0.6917009602194787), 'mae': np.float64(0.4164567471925352)}\n",
      "Validation loss decreased (0.230173 --> 0.225008).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:  50%|█████     | 407/808 [00:35<04:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.23386359214782715, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 1, 'validation_loss': np.float64(0.2253026554118032), 'macro_f1': np.float64(0.7200483770568632), 'micro_f1': np.float64(0.8718564243255601), 'mae': np.float64(0.16071433109676814)}\n",
      "{'loss': 0.23386359214782715, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 2, 'validation_loss': np.float64(0.2253026554118032), 'macro_f1': np.float64(0.6460258278163221), 'micro_f1': np.float64(0.823045267489712), 'mae': np.float64(0.2111546583722211)}\n",
      "{'loss': 0.23386359214782715, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 3, 'validation_loss': np.float64(0.2253026554118032), 'macro_f1': np.float64(0.5888120353794805), 'micro_f1': np.float64(0.7830361225422954), 'mae': np.float64(0.26464974494892923)}\n",
      "{'loss': 0.23386359214782715, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 4, 'validation_loss': np.float64(0.2253026554118032), 'macro_f1': np.float64(0.5205226683156711), 'micro_f1': np.float64(0.7459990855052584), 'mae': np.float64(0.3197992817503298)}\n",
      "{'loss': 0.23386359214782715, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 5, 'validation_loss': np.float64(0.2253026554118032), 'macro_f1': np.float64(0.4491724610668005), 'micro_f1': np.float64(0.716506630086877), 'mae': np.float64(0.36815828006718904)}\n",
      "{'loss': 0.23386359214782715, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 6, 'validation_loss': np.float64(0.2253026554118032), 'macro_f1': np.float64(0.39823361920476946), 'micro_f1': np.float64(0.6895290352080475), 'mae': np.float64(0.4103681520020388)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 808/808 [01:10<00:00, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26488155126571655, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 1, 'validation_loss': np.float64(0.22395657170293987), 'macro_f1': np.float64(0.75646557013388), 'micro_f1': np.float64(0.872085048010974), 'mae': np.float64(0.16371772425661935)}\n",
      "{'loss': 0.26488155126571655, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 2, 'validation_loss': np.float64(0.22395657170293987), 'macro_f1': np.float64(0.6891215428336798), 'micro_f1': np.float64(0.8262459990855052), 'mae': np.float64(0.21556654313168105)}\n",
      "{'loss': 0.26488155126571655, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 3, 'validation_loss': np.float64(0.22395657170293987), 'macro_f1': np.float64(0.6349932943926787), 'micro_f1': np.float64(0.7854366712391404), 'mae': np.float64(0.26812294587023744)}\n",
      "{'loss': 0.26488155126571655, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 4, 'validation_loss': np.float64(0.22395657170293987), 'macro_f1': np.float64(0.5761827381399437), 'micro_f1': np.float64(0.7482853223593965), 'mae': np.float64(0.3183671744203381)}\n",
      "{'loss': 0.26488155126571655, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 5, 'validation_loss': np.float64(0.22395657170293987), 'macro_f1': np.float64(0.5316167040728422), 'micro_f1': np.float64(0.7192501143118427), 'mae': np.float64(0.3668043898539151)}\n",
      "{'loss': 0.26488155126571655, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 6, 'validation_loss': np.float64(0.22395657170293987), 'macro_f1': np.float64(0.4597551250960814), 'micro_f1': np.float64(0.6914723365340649), 'mae': np.float64(0.40615137518876443)}\n",
      "Validation loss decreased (0.225008 --> 0.223957).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  50%|█████     | 407/808 [00:35<04:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.233161062002182, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 1, 'validation_loss': np.float64(0.21988838662703833), 'macro_f1': np.float64(0.7463314495670618), 'micro_f1': np.float64(0.8735711019661637), 'mae': np.float64(0.14974365337342643)}\n",
      "{'loss': 0.233161062002182, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 2, 'validation_loss': np.float64(0.21988838662703833), 'macro_f1': np.float64(0.6899807950167435), 'micro_f1': np.float64(0.8296753543667124), 'mae': np.float64(0.20045258908590305)}\n",
      "{'loss': 0.233161062002182, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 3, 'validation_loss': np.float64(0.21988838662703833), 'macro_f1': np.float64(0.6270480135858142), 'micro_f1': np.float64(0.7898948331047096), 'mae': np.float64(0.25265945930176237)}\n",
      "{'loss': 0.233161062002182, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 4, 'validation_loss': np.float64(0.21988838662703833), 'macro_f1': np.float64(0.5639146576052658), 'micro_f1': np.float64(0.7529721079103795), 'mae': np.float64(0.30386955594506593)}\n",
      "{'loss': 0.233161062002182, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 5, 'validation_loss': np.float64(0.21988838662703833), 'macro_f1': np.float64(0.5093347191062384), 'micro_f1': np.float64(0.7239368998628258), 'mae': np.float64(0.35178620196095417)}\n",
      "{'loss': 0.233161062002182, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 6, 'validation_loss': np.float64(0.21988838662703833), 'macro_f1': np.float64(0.41045512790919664), 'micro_f1': np.float64(0.6974165523548239), 'mae': np.float64(0.39303510057463714)}\n",
      "Validation loss decreased (0.223957 --> 0.219888).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 808/808 [01:11<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.35186290740966797, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 1, 'validation_loss': np.float64(0.21739354394916174), 'macro_f1': np.float64(0.7470608657059801), 'micro_f1': np.float64(0.8788294467306813), 'mae': np.float64(0.1519082824484153)}\n",
      "{'loss': 0.35186290740966797, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 2, 'validation_loss': np.float64(0.21739354394916174), 'macro_f1': np.float64(0.6863746724294462), 'micro_f1': np.float64(0.8309327846364883), 'mae': np.float64(0.20275314242484335)}\n",
      "{'loss': 0.35186290740966797, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 3, 'validation_loss': np.float64(0.21739354394916174), 'macro_f1': np.float64(0.629978600401459), 'micro_f1': np.float64(0.789551897576589), 'mae': np.float64(0.2570684143128312)}\n",
      "{'loss': 0.35186290740966797, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 4, 'validation_loss': np.float64(0.21739354394916174), 'macro_f1': np.float64(0.5672405966028288), 'micro_f1': np.float64(0.7513717421124828), 'mae': np.float64(0.3120181211335949)}\n",
      "{'loss': 0.35186290740966797, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 5, 'validation_loss': np.float64(0.21739354394916174), 'macro_f1': np.float64(0.5163553765815444), 'micro_f1': np.float64(0.7217649748513946), 'mae': np.float64(0.3575664436660635)}\n",
      "{'loss': 0.35186290740966797, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 6, 'validation_loss': np.float64(0.21739354394916174), 'macro_f1': np.float64(0.4344714880363694), 'micro_f1': np.float64(0.6954732510288066), 'mae': np.float64(0.40017856465331675)}\n",
      "Validation loss decreased (0.219888 --> 0.217394).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:  50%|█████     | 407/808 [00:35<04:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.22014831006526947, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 1, 'validation_loss': np.float64(0.22259975818620212), 'macro_f1': np.float64(0.7567816721381053), 'micro_f1': np.float64(0.8728852309099223), 'mae': np.float64(0.16595080061926432)}\n",
      "{'loss': 0.22014831006526947, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 2, 'validation_loss': np.float64(0.22259975818620212), 'macro_f1': np.float64(0.6933043995736622), 'micro_f1': np.float64(0.8238454503886603), 'mae': np.float64(0.22051319878830564)}\n",
      "{'loss': 0.22014831006526947, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 3, 'validation_loss': np.float64(0.22259975818620212), 'macro_f1': np.float64(0.6342780527380163), 'micro_f1': np.float64(0.7852080475537265), 'mae': np.float64(0.26968920683772796)}\n",
      "{'loss': 0.22014831006526947, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 4, 'validation_loss': np.float64(0.22259975818620212), 'macro_f1': np.float64(0.5604405528153166), 'micro_f1': np.float64(0.7441700960219478), 'mae': np.float64(0.32339451104481304)}\n",
      "{'loss': 0.22014831006526947, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 5, 'validation_loss': np.float64(0.22259975818620212), 'macro_f1': np.float64(0.48026094583946705), 'micro_f1': np.float64(0.7146776406035665), 'mae': np.float64(0.37136432114618145)}\n",
      "{'loss': 0.22014831006526947, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 6, 'validation_loss': np.float64(0.22259975818620212), 'macro_f1': np.float64(0.4191758961931084), 'micro_f1': np.float64(0.6874714220393233), 'mae': np.float64(0.41393904334730697)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 808/808 [01:10<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.21695873141288757, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 1, 'validation_loss': np.float64(0.21615426172164903), 'macro_f1': np.float64(0.769932711527065), 'micro_f1': np.float64(0.8835162322816644), 'mae': np.float64(0.15163859907036978)}\n",
      "{'loss': 0.21695873141288757, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 2, 'validation_loss': np.float64(0.21615426172164903), 'macro_f1': np.float64(0.7024054587666405), 'micro_f1': np.float64(0.8328760859625057), 'mae': np.float64(0.20523496172161604)}\n",
      "{'loss': 0.21695873141288757, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 3, 'validation_loss': np.float64(0.21615426172164903), 'macro_f1': np.float64(0.6309566296477928), 'micro_f1': np.float64(0.7898948331047096), 'mae': np.float64(0.2583488266062124)}\n",
      "{'loss': 0.21695873141288757, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 4, 'validation_loss': np.float64(0.21615426172164903), 'macro_f1': np.float64(0.570112979114919), 'micro_f1': np.float64(0.7478280749885688), 'mae': np.float64(0.3123513594849304)}\n",
      "{'loss': 0.21695873141288757, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 5, 'validation_loss': np.float64(0.21615426172164903), 'macro_f1': np.float64(0.5234004978874018), 'micro_f1': np.float64(0.7210791037951532), 'mae': np.float64(0.36189948869307914)}\n",
      "{'loss': 0.21695873141288757, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 6, 'validation_loss': np.float64(0.21615426172164903), 'macro_f1': np.float64(0.42606559472391564), 'micro_f1': np.float64(0.6911294010059442), 'mae': np.float64(0.4032312043720834)}\n",
      "Validation loss decreased (0.217394 --> 0.216154).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  50%|█████     | 407/808 [00:35<04:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17218248546123505, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 1, 'validation_loss': np.float64(0.21700713173418806), 'macro_f1': np.float64(0.7597430922629131), 'micro_f1': np.float64(0.8771147690900777), 'mae': np.float64(0.15425598572436347)}\n",
      "{'loss': 0.17218248546123505, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 2, 'validation_loss': np.float64(0.21700713173418806), 'macro_f1': np.float64(0.6997003245304286), 'micro_f1': np.float64(0.8292181069958847), 'mae': np.float64(0.2066701749414078)}\n",
      "{'loss': 0.17218248546123505, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 3, 'validation_loss': np.float64(0.21700713173418806), 'macro_f1': np.float64(0.6494965473074373), 'micro_f1': np.float64(0.7897805212620027), 'mae': np.float64(0.25888861616252473)}\n",
      "{'loss': 0.17218248546123505, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 4, 'validation_loss': np.float64(0.21700713173418806), 'macro_f1': np.float64(0.5719722177585917), 'micro_f1': np.float64(0.7486282578875172), 'mae': np.float64(0.31096861216306043)}\n",
      "{'loss': 0.17218248546123505, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 5, 'validation_loss': np.float64(0.21700713173418806), 'macro_f1': np.float64(0.525992444218253), 'micro_f1': np.float64(0.7221079103795153), 'mae': np.float64(0.3598717185189663)}\n",
      "{'loss': 0.17218248546123505, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 6, 'validation_loss': np.float64(0.21700713173418806), 'macro_f1': np.float64(0.4612112739422165), 'micro_f1': np.float64(0.6942158207590307), 'mae': np.float64(0.4011423046253281)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 808/808 [01:10<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.23346787691116333, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 1, 'validation_loss': np.float64(0.21721695169158603), 'macro_f1': np.float64(0.7703359983827184), 'micro_f1': np.float64(0.877914951989026), 'mae': np.float64(0.15779049922312352)}\n",
      "{'loss': 0.23346787691116333, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 2, 'validation_loss': np.float64(0.21721695169158603), 'macro_f1': np.float64(0.7021273721883804), 'micro_f1': np.float64(0.8291037951531779), 'mae': np.float64(0.2090569390370108)}\n",
      "{'loss': 0.23346787691116333, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 3, 'validation_loss': np.float64(0.21721695169158603), 'macro_f1': np.float64(0.6443951713935259), 'micro_f1': np.float64(0.7889803383630544), 'mae': np.float64(0.26093002627904016)}\n",
      "{'loss': 0.23346787691116333, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 4, 'validation_loss': np.float64(0.21721695169158603), 'macro_f1': np.float64(0.5699320788361207), 'micro_f1': np.float64(0.7478280749885688), 'mae': np.float64(0.3139413425210409)}\n",
      "{'loss': 0.23346787691116333, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 5, 'validation_loss': np.float64(0.21721695169158603), 'macro_f1': np.float64(0.5285757233670374), 'micro_f1': np.float64(0.7209647919524462), 'mae': np.float64(0.3619052292802601)}\n",
      "{'loss': 0.23346787691116333, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 6, 'validation_loss': np.float64(0.21721695169158603), 'macro_f1': np.float64(0.45841276201914966), 'micro_f1': np.float64(0.6923868312757202), 'mae': np.float64(0.40425955034099326)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  50%|█████     | 407/808 [00:35<04:04,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27507051825523376, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 1, 'validation_loss': np.float64(0.21586166937714038), 'macro_f1': np.float64(0.7695783301634155), 'micro_f1': np.float64(0.880429812528578), 'mae': np.float64(0.15447442143486148)}\n",
      "{'loss': 0.27507051825523376, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 2, 'validation_loss': np.float64(0.21586166937714038), 'macro_f1': np.float64(0.7011688814589112), 'micro_f1': np.float64(0.831390032007316), 'mae': np.float64(0.2065214219692887)}\n",
      "{'loss': 0.27507051825523376, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 3, 'validation_loss': np.float64(0.21586166937714038), 'macro_f1': np.float64(0.6331086862601194), 'micro_f1': np.float64(0.7893232738911752), 'mae': np.float64(0.258840336747746)}\n",
      "{'loss': 0.27507051825523376, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 4, 'validation_loss': np.float64(0.21586166937714038), 'macro_f1': np.float64(0.5756800874762854), 'micro_f1': np.float64(0.7501143118427069), 'mae': np.float64(0.3120013387310353)}\n",
      "{'loss': 0.27507051825523376, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 5, 'validation_loss': np.float64(0.21586166937714038), 'macro_f1': np.float64(0.5208559432407451), 'micro_f1': np.float64(0.7217649748513946), 'mae': np.float64(0.3600956676325673)}\n",
      "{'loss': 0.27507051825523376, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 6, 'validation_loss': np.float64(0.21586166937714038), 'macro_f1': np.float64(0.46055372255715926), 'micro_f1': np.float64(0.69387288523091), 'mae': np.float64(0.4019310144609804)}\n",
      "Validation loss decreased (0.216154 --> 0.215862).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 808/808 [01:11<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.21028056740760803, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 1, 'validation_loss': np.float64(0.21604836246241693), 'macro_f1': np.float64(0.7676129362869686), 'micro_f1': np.float64(0.8802011888431641), 'mae': np.float64(0.15364417431386415)}\n",
      "{'loss': 0.21028056740760803, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 2, 'validation_loss': np.float64(0.21604836246241693), 'macro_f1': np.float64(0.6989052064326254), 'micro_f1': np.float64(0.831275720164609), 'mae': np.float64(0.2060145811782345)}\n",
      "{'loss': 0.21028056740760803, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 3, 'validation_loss': np.float64(0.21604836246241693), 'macro_f1': np.float64(0.638953548436669), 'micro_f1': np.float64(0.7897805212620027), 'mae': np.float64(0.25820790535856036)}\n",
      "{'loss': 0.21028056740760803, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 4, 'validation_loss': np.float64(0.21604836246241693), 'macro_f1': np.float64(0.5695083178067021), 'micro_f1': np.float64(0.7490855052583447), 'mae': np.float64(0.31115787338578493)}\n",
      "{'loss': 0.21028056740760803, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 5, 'validation_loss': np.float64(0.21604836246241693), 'macro_f1': np.float64(0.5295288896973179), 'micro_f1': np.float64(0.7227937814357568), 'mae': np.float64(0.35932724148944717)}\n",
      "{'loss': 0.21028056740760803, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 6, 'validation_loss': np.float64(0.21604836246241693), 'macro_f1': np.float64(0.4595769519657739), 'micro_f1': np.float64(0.6939871970736168), 'mae': np.float64(0.40122944899220375)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU\")\n",
    "\n",
    "model = HybridModel(\n",
    "    num_categorical_features,\n",
    "    list_cat,\n",
    "    num_numerical_features,\n",
    "    num_time_series_features,\n",
    "    hidden_size,\n",
    "    num_lstm_layers,\n",
    "    embedding_dims,\n",
    "    num_fc_tabular_layers,\n",
    "    num_fc_combined_layers,\n",
    "    output_size=output_weeks,\n",
    ")\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=num_epochs_entire)\n",
    "valid_loss_min = np.inf\n",
    "counter = 0\n",
    "# Seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model.train()\n",
    "for i in range(num_epochs_entire):\n",
    "    for k, batch in tqdm(enumerate(train_loader), desc=f\"Epoch {i+1}/{num_epochs_entire}\", total=len(train_loader)):\n",
    "        X_time, X_static, X_static_cat, y_target = [data.to(device) for data in batch]\n",
    "        model.train()\n",
    "        counter += 1\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_static_cat, X_static, X_time)\n",
    "        loss = criterion(output, y_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n",
    "                model.eval()\n",
    "                labels = []\n",
    "                raw_labels = []\n",
    "                preds = []\n",
    "                raw_preds = []\n",
    "                val_losses = []\n",
    "                for batch in valid_loader:\n",
    "                    X_time_val, X_static_val, X_static_cat_val, y_target_val = [data.to(device) for data in batch]\n",
    "                    output = model(X_static_cat_val, X_static_val, X_time_val)\n",
    "                    val_loss = criterion(output, y_target_val)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    for label in y_target_val:\n",
    "                        labels.append([int(l.round()) for l in label])\n",
    "                        raw_labels.append([float(l) for l in label])\n",
    "                    for pred in output:\n",
    "                        preds.append([int(p.round()) for p in pred])\n",
    "                        raw_preds.append([float(p) for p in pred])\n",
    "                    \n",
    "                labels = np.array(labels)\n",
    "                preds = np.clip(np.array(preds), 0, 5)\n",
    "                raw_labels = np.array(raw_labels)\n",
    "                raw_preds = np.array(raw_preds)\n",
    "\n",
    "                for i in range(output_weeks):\n",
    "                    log_dict = {\n",
    "                        \"loss\": float(loss),\n",
    "                        \"epoch\": counter / len(train_loader),\n",
    "                        \"step\": counter,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"week\": i + 1,\n",
    "                    }\n",
    "                    # w = f'week_{i+1}_'\n",
    "                    w = \"\"\n",
    "                    log_dict[f\"{w}validation_loss\"] = np.mean(val_losses)\n",
    "                    log_dict[f\"{w}macro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"macro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}micro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"micro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}mae\"] = mean_absolute_error(\n",
    "                        raw_labels[:, i], raw_preds[:, i]\n",
    "                    )\n",
    "                    print(log_dict)\n",
    "                    writer.add_scalars(\"Loss(MSE)\", {'train': loss,\n",
    "                                                     'validation': log_dict[f\"{w}validation_loss\"]},\n",
    "                                                     counter)\n",
    "                    writer.add_scalars(\"F1(MSE)\", {'macro': log_dict[f\"{w}macro_f1\"],\n",
    "                                                   'micro': log_dict[f\"{w}micro_f1\"]},\n",
    "                                                   counter)\n",
    "                    writer.add_scalar(\"MAE\", log_dict[f\"{w}mae\"],\n",
    "                                      counter)\n",
    "                    writer.add_scalar(\"Learning-Rate\", log_dict[\"lr\"],\n",
    "                                      counter)\n",
    "                    for j, f1 in enumerate(\n",
    "                        f1_score(labels[:, i], preds[:, i], average=None)\n",
    "                    ):\n",
    "                        log_dict[f\"{w}{id2class[j]}_f1\"] = f1\n",
    "                    model.train()\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), \"./state_dict_HybridModel.pt\")\n",
    "                    print(\n",
    "                        \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                            valid_loss_min, np.mean(val_losses)\n",
    "                        )\n",
    "                    )\n",
    "                    valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying the best model params (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1445559/1259028130.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./state_dict_HybridModel.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HybridModel(\n",
       "  (embeddings): ModuleList(\n",
       "    (0-4): 5 x Embedding(7, 50)\n",
       "    (5): Embedding(6, 50)\n",
       "    (6): Embedding(8, 50)\n",
       "  )\n",
       "  (tabular_fc_layers): Sequential(\n",
       "    (0): Linear(in_features=372, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(21, 200, num_layers=2, batch_first=True)\n",
       "  (attention): Linear(in_features=200, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc_after_context): Linear(in_features=200, out_features=64, bias=True)\n",
       "  (combined_fc_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = HybridModel(\n",
    "    num_categorical_features,\n",
    "    list_cat,\n",
    "    num_numerical_features,\n",
    "    num_time_series_features,\n",
    "    hidden_size,\n",
    "    num_lstm_layers,\n",
    "    embedding_dims,\n",
    "    num_fc_tabular_layers,\n",
    "    num_fc_combined_layers,\n",
    "    output_size=output_weeks,\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(\"./state_dict_HybridModel.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, static, static_cat):\n",
    "    out = model(static_cat, static, x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation predictions...: 100%|██████████| 69/69 [00:03<00:00, 22.86it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_map = {\n",
    "    \"y_pred\": [],\n",
    "    \"y_pred_rounded\": [],\n",
    "    \"fips\": [],\n",
    "    \"date\": [],\n",
    "    \"y_true\": [],\n",
    "    \"week\": [],\n",
    "}\n",
    "i = 0\n",
    "for x, static, static_cat, y in tqdm(\n",
    "    valid_loader,\n",
    "    desc=\"Validation predictions...\",\n",
    "):\n",
    "    x, static, y = x.to(device), static.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = predict(x, static, static_cat).clone().detach()\n",
    "    for w in range(output_weeks):\n",
    "        dict_map[\"y_pred\"] += [float(p[w]) for p in pred]\n",
    "        dict_map[\"y_pred_rounded\"] += [int(p.round()[w]) for p in pred]\n",
    "        dict_map[\"fips\"] += [f[1][0] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"date\"] += [f[1][1] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"y_true\"] += [float(item[w]) for item in y]\n",
    "        dict_map[\"week\"] += [w] * len(x)\n",
    "    i += len(x)\n",
    "df = pd.DataFrame(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 1 MAE 0.16 F1 0.749\n",
      "Week 2 MAE 0.213 F1 0.687\n",
      "Week 3 MAE 0.264 F1 0.642\n",
      "Week 4 MAE 0.316 F1 0.571\n",
      "Week 5 MAE 0.363 F1 0.499\n",
      "Week 6 MAE 0.405 F1 0.452\n"
     ]
    }
   ],
   "source": [
    "for w in range(6):\n",
    "    wdf = df[df['week']==w]\n",
    "    mae = mean_absolute_error(wdf['y_true'], wdf['y_pred']).round(3)\n",
    "    f1 = f1_score(wdf['y_true'].round(),wdf['y_pred'].round(), average='macro').round(3)\n",
    "    print(f\"Week {w+1}\", f\"MAE {mae}\", f\"F1 {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
