{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDic = {\"train\": pd.read_csv(\"../src/train_timeseries/train_timeseries.csv\"),\n",
    "           \"test\": pd.read_csv(\"../src/test_timeseries/test_timeseries.csv\"),\n",
    "           \"validation\": pd.read_csv(\"../src/validation_timeseries/validation_timeseries.csv\"),\n",
    "           \"soil\" : pd.read_csv(\"../src/soil_data.csv\"),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: dataDic[k].set_index(['fips', 'date'])\n",
    "    for k in dataDic.keys() if k != \"soil\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (\n",
    "        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadXY(\n",
    "    df,\n",
    "    random_state=42,\n",
    "    window_size=180, # how many days in the past (default/competition: 180)\n",
    "    target_size=6, # how many weeks into the future (default/competition: 6)\n",
    "    fuse_past=True, # add the past drought observations? (default: True)\n",
    "    return_fips=False, # return the county identifier (do not use for predictions)\n",
    "    encode_season=True, # encode the season using the function above (default: True) \n",
    "    use_prev_year=False, # add observations from 1 year prior?\n",
    "):\n",
    "    df = dfs[df]\n",
    "    soil_df = dfs[\"soil\"]\n",
    "    time_data_cols = sorted(\n",
    "        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n",
    "    )\n",
    "    static_data_cols = sorted(\n",
    "        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n",
    "    )\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=[\"score\"])\n",
    "    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if use_prev_year:\n",
    "        add_dim += len(time_data_cols)\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "        if use_prev_year:\n",
    "            add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty(\n",
    "        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n",
    "    )\n",
    "    y_past = np.empty((len(df) // window_size, window_size))\n",
    "    y_target = np.empty((len(df) // window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0) == fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df[\"score\"].values\n",
    "        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n",
    "            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n",
    "            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n",
    "            if use_prev_year:\n",
    "                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n",
    "                    continue\n",
    "                X_time[count, :, -len(time_data_cols) :] = X[\n",
    "                    i - 365 : i + window_size - 365\n",
    "                ]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i : i + window_size])\n",
    "            else:\n",
    "                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n",
    "                    y[i : i + window_size]\n",
    "                )\n",
    "            if encode_season:\n",
    "                enc_dates = [\n",
    "                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n",
    "                ]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n",
    "            temp_y = y[i + window_size : i + window_size + target_size * 7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    print(f\"loaded {count} samples\")\n",
    "    results = [X_static[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}\n",
    "\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n",
    "        X_time[:, :, index] = (\n",
    "            scaler_dict[index]\n",
    "            .transform(X_time[:, :, index].reshape(-1, 1))\n",
    "            .reshape(-1, X_time.shape[-2])\n",
    "        )\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:, index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:, index] = (\n",
    "            scaler_dict_static[index]\n",
    "            .transform(X_static[:, index].reshape(-1, 1))\n",
    "            .reshape(1, -1)\n",
    "        )\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n",
    "        y_past[:, :] = (\n",
    "            scaler_dict_past[index]\n",
    "            .transform(y_past.reshape(-1, 1))\n",
    "            .reshape(-1, y_past.shape[-1])\n",
    "        )\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    X_tabular_train = data[\"X_tabular_train\"]\n",
    "    X_time_train = data[\"X_time_train\"]\n",
    "    y_target_train = data[\"y_target_train\"]\n",
    "    X_tabular_validation = data[\"X_tabular_validation\"]\n",
    "    X_time_valid = data[\"X_time_valid\"]\n",
    "    y_target_valid = data[\"y_target_valid\"]\n",
    "    valid_fips = data[\"valid_fips\"]\n",
    "    X_tabular_test = data[\"X_tabular_test\"]\n",
    "    X_time_test = data[\"X_time_test\"]\n",
    "    y_target_test = data[\"y_target_test\"]\n",
    "    test_fips = data[\"test_fips\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:38<00:00,  1.04it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 359.82it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 173.48it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 11999.73it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 178.98it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 13119.50it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tabular_train, X_time_train = normalize(X_tabular_train, X_time_train, fit=True)\n",
    "X_tabular_validation, X_time_valid = normalize(X_tabular_validation, X_time_valid)\n",
    "X_tabular_test, X_time_test = normalize(X_tabular_test, X_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97099, 180, 40) (2457, 180, 40) (2477, 180, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X_time_train.shape, X_time_valid.shape, X_time_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_weeks = 6\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_time_train),\n",
    "    torch.tensor(X_tabular_train),\n",
    "    torch.tensor(y_target_train[:, :output_weeks]),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.tensor(X_time_valid),\n",
    "    torch.tensor(X_tabular_validation),\n",
    "    torch.tensor(y_target_valid[:, :output_weeks]),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "\n",
    "test_data = TensorDataset(\n",
    "    torch.tensor(X_time_test),\n",
    "    torch.tensor(X_tabular_test),\n",
    "    torch.tensor(y_target_test[:, :output_weeks]),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        if d_model % 2 != 0:\n",
    "            pe = torch.zeros(max_len, d_model+1)\n",
    "        else:\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        print(div_term.shape)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        if d_model % 2 != 0:\n",
    "            pe = pe[:, :-1]\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class DroughtNetTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, output_size, num_input_features, hidden_dim, n_layers, ffnn_layers,\n",
    "        drop_prob, static_dim, num_heads, input_length, init_dim=128):\n",
    "        super(DroughtNetTransformer, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.init_linear = nn.Linear(num_input_features, init_dim)\n",
    "        self.pos_encoder = PositionalEncoding(init_dim, drop_prob, input_length)\n",
    "        encoder_layers = TransformerEncoderLayer(init_dim, num_heads, hidden_dim, drop_prob)\n",
    "        encoder_norm = LayerNorm(init_dim)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers, encoder_norm)\n",
    "        self.ninp = num_input_features\n",
    "        # todo: add several layers given ffnn_layers\n",
    "        # todo: add static\n",
    "        self.ffnn_layers = []\n",
    "        if ffnn_layers == 1:\n",
    "            self.final = nn.Linear(init_dim*input_length, output_size)\n",
    "        else:\n",
    "            self.final = nn.Linear(hidden_dim, output_size)\n",
    "            \n",
    "        for i in range(ffnn_layers-1):\n",
    "            if i == 0:\n",
    "                self.ffnn_layers.append(nn.Linear(init_dim*input_length+static_dim, hidden_dim))\n",
    "            else:\n",
    "                self.ffnn_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                \n",
    "        self.ffnn_layers = nn.ModuleList(self.ffnn_layers)\n",
    "\n",
    "        self.init_dim = init_dim\n",
    "        self.input_length = input_length\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        pass # possibly use initalization here\n",
    "\n",
    "    def forward(self, x, static=None):\n",
    "        # todo add static\n",
    "        batch_size = x.size(0)\n",
    "        x = x.cuda().to(dtype=torch.float32)\n",
    "        if static is not None:\n",
    "            static = static.cuda().to(dtype=torch.float32)\n",
    "        x = self.init_linear(x)\n",
    "        x = x * math.sqrt(self.ninp)\n",
    "        output = self.pos_encoder(x)\n",
    "        output = self.transformer_encoder(x)\n",
    "        output = output.reshape(\n",
    "            batch_size,\n",
    "            self.init_dim*self.input_length\n",
    "        )\n",
    "        for i in range(len(self.ffnn_layers)):\n",
    "            if i == 0 and static is not None:\n",
    "                output = self.ffnn_layers[i](torch.cat((output, static), 1))\n",
    "            else:\n",
    "                output = self.ffnn_layers[i](output)\n",
    "        output = self.final(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 128\n",
    "n_layers = 4\n",
    "ffnn_layers = 2\n",
    "dropout = 0.1\n",
    "static_dim = X_tabular_train.shape[1]\n",
    "n_heads = 2\n",
    "window_size = 180\n",
    "lr = 7e-5\n",
    "epochs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/7:  51%|█████     | 385/759 [00:12<00:36, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7980451583862305, 'epoch': 0.5006587615283268, 'step': 380, 'lr': 1.1802751618442933e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.7980451583862305, 'epoch': 0.5006587615283268, 'step': 380, 'lr': 1.1802751618442933e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.7980451583862305, 'epoch': 0.5006587615283268, 'step': 380, 'lr': 1.1802751618442933e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.7980451583862305, 'epoch': 0.5006587615283268, 'step': 380, 'lr': 1.1802751618442933e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.7980451583862305, 'epoch': 0.5006587615283268, 'step': 380, 'lr': 1.1802751618442933e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.7980451583862305, 'epoch': 0.5006587615283268, 'step': 380, 'lr': 1.1802751618442933e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (inf --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/7: 100%|██████████| 759/759 [00:24<00:00, 31.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4573363065719604, 'epoch': 1.0, 'step': 759, 'lr': 3.3920537677468094e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.4573363065719604, 'epoch': 1.0, 'step': 759, 'lr': 3.3920537677468094e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.4573363065719604, 'epoch': 1.0, 'step': 759, 'lr': 3.3920537677468094e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.4573363065719604, 'epoch': 1.0, 'step': 759, 'lr': 3.3920537677468094e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.4573363065719604, 'epoch': 1.0, 'step': 759, 'lr': 3.3920537677468094e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.4573363065719604, 'epoch': 1.0, 'step': 759, 'lr': 3.3920537677468094e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/7:  51%|█████     | 386/759 [00:12<00:38,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.11326265335083, 'epoch': 1.5006587615283267, 'step': 1139, 'lr': 5.741210937787608e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.11326265335083, 'epoch': 1.5006587615283267, 'step': 1139, 'lr': 5.741210937787608e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.11326265335083, 'epoch': 1.5006587615283267, 'step': 1139, 'lr': 5.741210937787608e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.11326265335083, 'epoch': 1.5006587615283267, 'step': 1139, 'lr': 5.741210937787608e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.11326265335083, 'epoch': 1.5006587615283267, 'step': 1139, 'lr': 5.741210937787608e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.11326265335083, 'epoch': 1.5006587615283267, 'step': 1139, 'lr': 5.741210937787608e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/7: 100%|██████████| 759/759 [00:24<00:00, 31.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3277134895324707, 'epoch': 2.0, 'step': 1518, 'lr': 6.963406348756813e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.3277134895324707, 'epoch': 2.0, 'step': 1518, 'lr': 6.963406348756813e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.3277134895324707, 'epoch': 2.0, 'step': 1518, 'lr': 6.963406348756813e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.3277134895324707, 'epoch': 2.0, 'step': 1518, 'lr': 6.963406348756813e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.3277134895324707, 'epoch': 2.0, 'step': 1518, 'lr': 6.963406348756813e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.3277134895324707, 'epoch': 2.0, 'step': 1518, 'lr': 6.963406348756813e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/7:  51%|█████     | 386/759 [00:12<00:38,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2867095470428467, 'epoch': 2.5006587615283267, 'step': 1898, 'lr': 6.884404865301213e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.2867095470428467, 'epoch': 2.5006587615283267, 'step': 1898, 'lr': 6.884404865301213e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.2867095470428467, 'epoch': 2.5006587615283267, 'step': 1898, 'lr': 6.884404865301213e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.2867095470428467, 'epoch': 2.5006587615283267, 'step': 1898, 'lr': 6.884404865301213e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.2867095470428467, 'epoch': 2.5006587615283267, 'step': 1898, 'lr': 6.884404865301213e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.2867095470428467, 'epoch': 2.5006587615283267, 'step': 1898, 'lr': 6.884404865301213e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/7: 100%|██████████| 759/759 [00:24<00:00, 31.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.041424512863159, 'epoch': 3.0, 'step': 2277, 'lr': 6.431696711982113e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.041424512863159, 'epoch': 3.0, 'step': 2277, 'lr': 6.431696711982113e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.041424512863159, 'epoch': 3.0, 'step': 2277, 'lr': 6.431696711982113e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.041424512863159, 'epoch': 3.0, 'step': 2277, 'lr': 6.431696711982113e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.041424512863159, 'epoch': 3.0, 'step': 2277, 'lr': 6.431696711982113e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.041424512863159, 'epoch': 3.0, 'step': 2277, 'lr': 6.431696711982113e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/7:  51%|█████     | 386/759 [00:12<00:38,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.099496603012085, 'epoch': 3.5006587615283267, 'step': 2657, 'lr': 5.6787505961414953e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.099496603012085, 'epoch': 3.5006587615283267, 'step': 2657, 'lr': 5.6787505961414953e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.099496603012085, 'epoch': 3.5006587615283267, 'step': 2657, 'lr': 5.6787505961414953e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.099496603012085, 'epoch': 3.5006587615283267, 'step': 2657, 'lr': 5.6787505961414953e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.099496603012085, 'epoch': 3.5006587615283267, 'step': 2657, 'lr': 5.6787505961414953e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.099496603012085, 'epoch': 3.5006587615283267, 'step': 2657, 'lr': 5.6787505961414953e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/7: 100%|██████████| 759/759 [00:24<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.200443744659424, 'epoch': 4.0, 'step': 3036, 'lr': 4.7060118397295637e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.200443744659424, 'epoch': 4.0, 'step': 3036, 'lr': 4.7060118397295637e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.200443744659424, 'epoch': 4.0, 'step': 3036, 'lr': 4.7060118397295637e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.200443744659424, 'epoch': 4.0, 'step': 3036, 'lr': 4.7060118397295637e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.200443744659424, 'epoch': 4.0, 'step': 3036, 'lr': 4.7060118397295637e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.200443744659424, 'epoch': 4.0, 'step': 3036, 'lr': 4.7060118397295637e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/7:  51%|█████     | 386/759 [00:12<00:38,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.996777057647705, 'epoch': 4.500658761528327, 'step': 3416, 'lr': 3.607761507918826e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.996777057647705, 'epoch': 4.500658761528327, 'step': 3416, 'lr': 3.607761507918826e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.996777057647705, 'epoch': 4.500658761528327, 'step': 3416, 'lr': 3.607761507918826e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.996777057647705, 'epoch': 4.500658761528327, 'step': 3416, 'lr': 3.607761507918826e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.996777057647705, 'epoch': 4.500658761528327, 'step': 3416, 'lr': 3.607761507918826e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.996777057647705, 'epoch': 4.500658761528327, 'step': 3416, 'lr': 3.607761507918826e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/7: 100%|██████████| 759/759 [00:24<00:00, 30.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8801796436309814, 'epoch': 5.0, 'step': 3795, 'lr': 2.501337481423089e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 2.8801796436309814, 'epoch': 5.0, 'step': 3795, 'lr': 2.501337481423089e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 2.8801796436309814, 'epoch': 5.0, 'step': 3795, 'lr': 2.501337481423089e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 2.8801796436309814, 'epoch': 5.0, 'step': 3795, 'lr': 2.501337481423089e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 2.8801796436309814, 'epoch': 5.0, 'step': 3795, 'lr': 2.501337481423089e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 2.8801796436309814, 'epoch': 5.0, 'step': 3795, 'lr': 2.501337481423089e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/7:  51%|█████     | 386/759 [00:12<00:41,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7360825538635254, 'epoch': 5.500658761528327, 'step': 4175, 'lr': 1.4939780449954502e-05, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.7360825538635254, 'epoch': 5.500658761528327, 'step': 4175, 'lr': 1.4939780449954502e-05, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.7360825538635254, 'epoch': 5.500658761528327, 'step': 4175, 'lr': 1.4939780449954502e-05, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.7360825538635254, 'epoch': 5.500658761528327, 'step': 4175, 'lr': 1.4939780449954502e-05, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.7360825538635254, 'epoch': 5.500658761528327, 'step': 4175, 'lr': 1.4939780449954502e-05, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.7360825538635254, 'epoch': 5.500658761528327, 'step': 4175, 'lr': 1.4939780449954502e-05, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/7: 100%|██████████| 759/759 [00:24<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.945360779762268, 'epoch': 6.0, 'step': 4554, 'lr': 6.933102286160286e-06, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.945360779762268, 'epoch': 6.0, 'step': 4554, 'lr': 6.933102286160286e-06, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.945360779762268, 'epoch': 6.0, 'step': 4554, 'lr': 6.933102286160286e-06, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.945360779762268, 'epoch': 6.0, 'step': 4554, 'lr': 6.933102286160286e-06, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.945360779762268, 'epoch': 6.0, 'step': 4554, 'lr': 6.933102286160286e-06, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.945360779762268, 'epoch': 6.0, 'step': 4554, 'lr': 6.933102286160286e-06, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/7:  51%|█████     | 386/759 [00:12<00:38,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.247196912765503, 'epoch': 6.500658761528327, 'step': 4934, 'lr': 1.7693741148608977e-06, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.247196912765503, 'epoch': 6.500658761528327, 'step': 4934, 'lr': 1.7693741148608977e-06, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.247196912765503, 'epoch': 6.500658761528327, 'step': 4934, 'lr': 1.7693741148608977e-06, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.247196912765503, 'epoch': 6.500658761528327, 'step': 4934, 'lr': 1.7693741148608977e-06, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.247196912765503, 'epoch': 6.500658761528327, 'step': 4934, 'lr': 1.7693741148608977e-06, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.247196912765503, 'epoch': 6.500658761528327, 'step': 4934, 'lr': 1.7693741148608977e-06, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/7: 100%|██████████| 759/759 [00:24<00:00, 30.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.837167501449585, 'epoch': 7.0, 'step': 5313, 'lr': 2.9248706347149187e-10, 'week': 1, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14087242937375957), 'micro_f1': np.float64(0.7057387057387058), 'mae': np.float64(0.6366035369382649)}\n",
      "{'loss': 1.837167501449585, 'epoch': 7.0, 'step': 5313, 'lr': 2.9248706347149187e-10, 'week': 2, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14084210101765637), 'micro_f1': np.float64(0.7138787138787139), 'mae': np.float64(0.7029591009501307)}\n",
      "{'loss': 1.837167501449585, 'epoch': 7.0, 'step': 5313, 'lr': 2.9248706347149187e-10, 'week': 3, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.1625640301523715), 'micro_f1': np.float64(0.671957671957672), 'mae': np.float64(0.6085960524943783)}\n",
      "{'loss': 1.837167501449585, 'epoch': 7.0, 'step': 5313, 'lr': 2.9248706347149187e-10, 'week': 4, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14835184515984415), 'micro_f1': np.float64(0.6597476597476597), 'mae': np.float64(0.6652948712682719)}\n",
      "{'loss': 1.837167501449585, 'epoch': 7.0, 'step': 5313, 'lr': 2.9248706347149187e-10, 'week': 5, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14341960692001512), 'micro_f1': np.float64(0.47985347985347987), 'mae': np.float64(0.6946222177480507)}\n",
      "{'loss': 1.837167501449585, 'epoch': 7.0, 'step': 5313, 'lr': 2.9248706347149187e-10, 'week': 6, 'validation_loss': np.float64(1.1378739580512047), 'macro_f1': np.float64(0.14414414414414414), 'micro_f1': np.float64(0.7619047619047619), 'mae': np.float64(0.5838103734347332)}\n",
      "Validation loss decreased (1.137874 --> 1.137874).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU\")\n",
    "\n",
    "model = DroughtNetTransformer(\n",
    "            output_weeks,\n",
    "            X_time_train.shape[-1],\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            ffnn_layers,\n",
    "            dropout,\n",
    "            static_dim,\n",
    "            n_heads,\n",
    "            window_size\n",
    "        )\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                                                steps_per_epoch=len(train_loader),\n",
    "                                                epochs=epochs)\n",
    "\n",
    "counter = 0\n",
    "valid_loss_min = np.inf\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for k, (inputs, static, labels) in tqdm(\n",
    "            enumerate(train_loader),\n",
    "            desc=f\"epoch {i+1}/{epochs}\",\n",
    "            total=len(train_loader),\n",
    "        ):\n",
    "        model.train()\n",
    "        counter += 1\n",
    "        inputs, labels, static = (\n",
    "                inputs.to(device),\n",
    "                labels.to(device),\n",
    "                static.to(device),\n",
    "            )\n",
    "        model.zero_grad()\n",
    "        output = model(inputs, static)\n",
    "        loss = loss_function(output, labels.float())\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                labels = []\n",
    "                preds = []\n",
    "                raw_labels = []\n",
    "                raw_preds = []\n",
    "                for inp, stat, lab in valid_loader:\n",
    "                    inp, stat, lab = (\n",
    "                        inp.to(device),\n",
    "                        stat.to(device),\n",
    "                        lab.to(device),\n",
    "                    )\n",
    "                    out = model(inp, stat)\n",
    "                    val_loss = loss_function(out, lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    for labs in lab:\n",
    "                        labels.append([int(l.round()) for l in labs])\n",
    "                        raw_labels.append([float(l) for l in labs])\n",
    "                    for pred in out:\n",
    "                        preds.append([int(p.round()) for p in pred])\n",
    "                        raw_preds.append([float(p) for p in pred])\n",
    "\n",
    "                    # log data\n",
    "                labels = np.array(labels)\n",
    "                preds = np.clip(np.array(preds), 0, 5)\n",
    "                raw_preds = np.array(raw_preds)\n",
    "                raw_labels = np.array(raw_labels)\n",
    "\n",
    "                for i in range(output_weeks):\n",
    "                    log_dict = {\n",
    "                        \"loss\": float(loss),\n",
    "                        \"epoch\": counter / len(train_loader),\n",
    "                        \"step\": counter,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"week\": i + 1,\n",
    "                    }\n",
    "                    # w = f'week_{i+1}_'\n",
    "                    w = \"\"\n",
    "                    log_dict[f\"{w}validation_loss\"] = np.mean(val_losses)\n",
    "                    log_dict[f\"{w}macro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"macro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}micro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"micro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}mae\"] = mean_absolute_error(\n",
    "                        raw_labels[:, i], raw_preds[:, i]\n",
    "                    )\n",
    "                    print(log_dict)\n",
    "                    # writer.add_scalars(\"Loss(MSE)\", {'train': loss,\n",
    "                    #                                  'validation': log_dict[f\"{w}validation_loss\"]},\n",
    "                    #                                  counter)\n",
    "                    # writer.add_scalars(\"F1(MSE)\", {'macro': log_dict[f\"{w}macro_f1\"],\n",
    "                    #                                'micro': log_dict[f\"{w}micro_f1\"]},\n",
    "                    #                                counter)\n",
    "                    # writer.add_scalar(\"MAE\", log_dict[f\"{w}mae\"],\n",
    "                    #                   counter)\n",
    "                    # writer.add_scalar(\"Learning-Rate\", log_dict[\"lr\"],\n",
    "                    #                   counter)\n",
    "                    for j, f1 in enumerate(\n",
    "                        f1_score(labels[:, i], preds[:, i], average=None)\n",
    "                    ):\n",
    "                        log_dict[f\"{w}{id2class[j]}_f1\"] = f1\n",
    "                    model.train()\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), \"./Transformer_first_test.pt\")\n",
    "                    print(\n",
    "                        \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                            valid_loss_min, np.mean(val_losses)\n",
    "                        )\n",
    "                    )\n",
    "                    valid_loss_min = np.mean(val_losses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
