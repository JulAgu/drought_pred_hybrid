{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('../runs/HM_Scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soil_data.csv', 'train_timeseries', 'counties.geojson', 'test_timeseries', 'validation_timeseries', 'counties.zip']\n"
     ]
    }
   ],
   "source": [
    "filesList = os.listdir('../src')\n",
    "print(filesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDic = {\"train\": pd.read_csv(\"../src/train_timeseries/train_timeseries.csv\"),\n",
    "           \"test\": pd.read_csv(\"../src/test_timeseries/test_timeseries.csv\"),\n",
    "           \"validation\": pd.read_csv(\"../src/validation_timeseries/validation_timeseries.csv\"),\n",
    "           \"soil\" : pd.read_csv(\"../src/soil_data.csv\"),\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fips', 'date', 'PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
       "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
       "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
       "       'WS50M_RANGE', 'score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDic[\"train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: dataDic[k].set_index(['fips', 'date'])\n",
    "    for k in dataDic.keys() if k != \"soil\"\n",
    "}\n",
    "\n",
    "dfs[\"soil\"] = dataDic[\"soil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_weeks = 6\n",
    "hidden_dim = 360\n",
    "n_layers = 3\n",
    "ffnn_layers = 4\n",
    "dropout = 0.4\n",
    "lr = 7e-5\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (\n",
    "        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadXY(\n",
    "    df,\n",
    "    random_state=42,\n",
    "    window_size=180, # how many days in the past (default/competition: 180)\n",
    "    target_size=6, # how many weeks into the future (default/competition: 6)\n",
    "    fuse_past=True, # add the past drought observations? (default: True)\n",
    "    return_fips=False, # return the county identifier (do not use for predictions)\n",
    "    encode_season=True, # encode the season using the function above (default: True) \n",
    "    use_prev_year=False, # add observations from 1 year prior?\n",
    "):\n",
    "    df = dfs[df]\n",
    "    soil_df = dfs[\"soil\"]\n",
    "    time_data_cols = sorted(\n",
    "        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n",
    "    )\n",
    "    static_data_cols = sorted(\n",
    "        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n",
    "    )\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=[\"score\"])\n",
    "    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if use_prev_year:\n",
    "        add_dim += len(time_data_cols)\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "        if use_prev_year:\n",
    "            add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty(\n",
    "        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n",
    "    )\n",
    "    y_past = np.empty((len(df) // window_size, window_size))\n",
    "    y_target = np.empty((len(df) // window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0) == fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df[\"score\"].values\n",
    "        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n",
    "            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n",
    "            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n",
    "            if use_prev_year:\n",
    "                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n",
    "                    continue\n",
    "                X_time[count, :, -len(time_data_cols) :] = X[\n",
    "                    i - 365 : i + window_size - 365\n",
    "                ]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i : i + window_size])\n",
    "            else:\n",
    "                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n",
    "                    y[i : i + window_size]\n",
    "                )\n",
    "            if encode_season:\n",
    "                enc_dates = [\n",
    "                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n",
    "                ]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n",
    "            temp_y = y[i + window_size : i + window_size + target_size * 7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    print(f\"loaded {count} samples\")\n",
    "    results = [X_static[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}\n",
    "\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n",
    "        X_time[:, :, index] = (\n",
    "            scaler_dict[index]\n",
    "            .transform(X_time[:, :, index].reshape(-1, 1))\n",
    "            .reshape(-1, X_time.shape[-2])\n",
    "        )\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:, index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:, index] = (\n",
    "            scaler_dict_static[index]\n",
    "            .transform(X_static[:, index].reshape(-1, 1))\n",
    "            .reshape(1, -1)\n",
    "        )\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n",
    "        y_past[:, :] = (\n",
    "            scaler_dict_past[index]\n",
    "            .transform(y_past.reshape(-1, 1))\n",
    "            .reshape(-1, y_past.shape[-1])\n",
    "        )\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [09:36<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 103390 samples\n",
      "train shape (103390, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [00:48<00:00, 63.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8748 samples\n",
      "validation shape (8748, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:21<00:00,  1.02s/it]\n",
      "100%|██████████| 30/30 [00:00<00:00, 337.67it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 31.18it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 8448.88it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tabular_train, X_time_train, y_target_train = loadXY(\"train\")\n",
    "print(\"train shape\", X_time_train.shape)\n",
    "X_tabular_validation, X_time_valid, y_target_valid, valid_fips = loadXY(\"validation\", return_fips=True)\n",
    "print(\"validation shape\", X_time_valid.shape)\n",
    "X_tabular_train, X_time_train = normalize(X_tabular_train, X_time_train, fit=True)\n",
    "X_tabular_validation, X_time_valid = normalize(X_tabular_validation, X_time_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [00:46<00:00, 66.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8768 samples\n",
      "test shape (8768, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 31.13it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 8606.64it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tabular_test, X_time_test, y_target_test, test_fips = loadXY(\"test\", return_fips=True)\n",
    "print(\"test shape\", X_time_test.shape)\n",
    "X_tabular_test, X_time_test = normalize(X_tabular_test, X_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # export all\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/X_tabular_train.npy\", X_tabular_train)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/X_time_train.npy\", X_time_train)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/y_target_train.npy\", y_target_train)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/X_tabular_validation.npy\", X_tabular_validation)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/X_time_valid.npy\", X_time_valid)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/y_target_valid.npy\", y_target_valid)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/X_tabular_test.npy\", X_tabular_test)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/X_time_test.npy\", X_time_test)\n",
    "# np.save(\"../data/processed_no_cat_and_no_tensors/y_target_test.npy\", y_target_test)\n",
    "# with open(f\"../data/processed_no_cat_and_no_tensors/valid_fips.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(valid_fips, f)\n",
    "# with open(f\"../data/processed_no_cat_and_no_tensors/test_fips.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(test_fips, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_time_train),\n",
    "    torch.tensor(X_tabular_train),\n",
    "    torch.tensor(y_target_train[:, :output_weeks]),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.tensor(X_time_valid),\n",
    "    torch.tensor(X_tabular_validation),\n",
    "    torch.tensor(y_target_valid[:, :output_weeks]),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_numerical_features,\n",
    "        num_time_series_features,\n",
    "        hidden_size,\n",
    "        num_lstm_layers,\n",
    "        num_fc_tabular_layers,\n",
    "        num_fc_combined_layers,\n",
    "        output_size,\n",
    "        dropout,\n",
    "        ablation_TS=False,\n",
    "        ablation_tabular=False,\n",
    "        ablation_attention=False,\n",
    "    ):\n",
    "        super(HybridModel, self).__init__()\n",
    "        \n",
    "        self.ablation_tabular = ablation_tabular\n",
    "        self.ablation_TS = ablation_TS\n",
    "        self.ablation_attention = ablation_attention\n",
    "\n",
    "        if not self.ablation_tabular:\n",
    "            # Static data branch\n",
    "            tabular_fc_layers = []\n",
    "            input_size = num_numerical_features\n",
    "            for _ in range(num_fc_tabular_layers):\n",
    "                tabular_fc_layers.append(nn.Linear(input_size, 128))\n",
    "                tabular_fc_layers.append(nn.ReLU())\n",
    "                input_size = 128\n",
    "            self.tabular_fc_layers = nn.Sequential(\n",
    "                *tabular_fc_layers, nn.Linear(128, 64), nn.ReLU()\n",
    "            )\n",
    "\n",
    "        if not self.ablation_TS:\n",
    "            # TS branch\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=num_time_series_features,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_lstm_layers,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "            # Atenttion\n",
    "            self.attention = nn.Linear(hidden_size, 1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Combined part\n",
    "        self.fc_after_context = nn.Linear(hidden_size, 64)\n",
    "        combined_fc_layers = []\n",
    "        if not self.ablation_tabular and not self.ablation_TS:\n",
    "            input_dim = 64 + 64  # Assuming 64 from tabular output and 64 from LSTM output after attention\n",
    "        else:\n",
    "            input_dim = 64\n",
    "        for _ in range(num_fc_combined_layers):\n",
    "            combined_fc_layers.append(nn.Linear(input_dim, 64))\n",
    "            combined_fc_layers.append(nn.ReLU())\n",
    "            input_dim = 64\n",
    "        self.combined_fc_layers = nn.Sequential(\n",
    "            *combined_fc_layers, nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, time_series_data, numerical_data):\n",
    "        numerical_data = numerical_data.to(torch.float32)\n",
    "        time_series_data = time_series_data.to(torch.float32)\n",
    "        if not self.ablation_tabular:\n",
    "            # Pass the tabular data through FC layers\n",
    "            x1 = self.tabular_fc_layers(numerical_data)\n",
    "        if not self.ablation_TS:\n",
    "            # Pass the time series data through the LSTM\n",
    "            lstm_out, (hn, cn) = self.lstm(time_series_data)\n",
    "            # Pass the data through the attention mechanism\n",
    "            if not self.ablation_attention:\n",
    "                attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "                context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "            else:\n",
    "                context_vector = lstm_out[:, -1, :]  # Last time step output\n",
    "            \n",
    "            context_vector = self.bn_lstm_out(context_vector)\n",
    "            droped_out = self.dropout(context_vector)\n",
    "            x2 = torch.relu(self.fc_after_context(droped_out))\n",
    "\n",
    "        # Concatenate the outputs from the tabular and the temporal data and pass it through FC layers\n",
    "        if not self.ablation_tabular and not self.ablation_TS:\n",
    "            x = torch.cat((x1, x2), dim=1)\n",
    "        elif not self.ablation_tabular:\n",
    "            x = x1\n",
    "        else:\n",
    "            x = x2\n",
    "\n",
    "        x = self.combined_fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10:  50%|█████     | 404/808 [04:05<37:05,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.536420226097107, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 1, 'validation_loss': np.float64(1.1974370693922907), 'macro_f1': np.float64(0.1329893360853113), 'micro_f1': np.float64(0.6628943758573388), 'mae': np.float64(0.5638914506719982)}\n",
      "{'loss': 1.536420226097107, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 2, 'validation_loss': np.float64(1.1974370693922907), 'macro_f1': np.float64(0.1385221992759396), 'micro_f1': np.float64(0.6620941929583904), 'mae': np.float64(0.6818223081362413)}\n",
      "{'loss': 1.536420226097107, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 3, 'validation_loss': np.float64(1.1974370693922907), 'macro_f1': np.float64(0.13290718720645175), 'micro_f1': np.float64(0.6631229995427527), 'mae': np.float64(0.7225723570303477)}\n",
      "{'loss': 1.536420226097107, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 4, 'validation_loss': np.float64(1.1974370693922907), 'macro_f1': np.float64(0.1332921726006723), 'micro_f1': np.float64(0.6663237311385459), 'mae': np.float64(0.6478082965226963)}\n",
      "{'loss': 1.536420226097107, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 5, 'validation_loss': np.float64(1.1974370693922907), 'macro_f1': np.float64(0.1397250544055797), 'micro_f1': np.float64(0.6651806127114769), 'mae': np.float64(0.6053662981746769)}\n",
      "{'loss': 1.536420226097107, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 6, 'validation_loss': np.float64(1.1974370693922907), 'macro_f1': np.float64(0.1359053708384567), 'micro_f1': np.float64(0.6652949245541838), 'mae': np.float64(0.572863711399835)}\n",
      "Validation loss decreased (inf --> 1.197437).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 100%|██████████| 808/808 [08:10<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6369934678077698, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 1, 'validation_loss': np.float64(0.6657752779082976), 'macro_f1': np.float64(0.28258583372298246), 'micro_f1': np.float64(0.6547782350251486), 'mae': np.float64(0.46834351842660693)}\n",
      "{'loss': 0.6369934678077698, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 2, 'validation_loss': np.float64(0.6657752779082976), 'macro_f1': np.float64(0.3265359312885872), 'micro_f1': np.float64(0.6490626428898034), 'mae': np.float64(0.5106135849832998)}\n",
      "{'loss': 0.6369934678077698, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 3, 'validation_loss': np.float64(0.6657752779082976), 'macro_f1': np.float64(0.18055588242180964), 'micro_f1': np.float64(0.655578417924097), 'mae': np.float64(0.5226629727440415)}\n",
      "{'loss': 0.6369934678077698, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 4, 'validation_loss': np.float64(0.6657752779082976), 'macro_f1': np.float64(0.21361779515232548), 'micro_f1': np.float64(0.6340877914951989), 'mae': np.float64(0.5444948979430408)}\n",
      "{'loss': 0.6369934678077698, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 5, 'validation_loss': np.float64(0.6657752779082976), 'macro_f1': np.float64(0.2566591271369426), 'micro_f1': np.float64(0.6359167809785093), 'mae': np.float64(0.5326521163791418)}\n",
      "{'loss': 0.6369934678077698, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 6, 'validation_loss': np.float64(0.6657752779082976), 'macro_f1': np.float64(0.24370574275475318), 'micro_f1': np.float64(0.6263145861911295), 'mae': np.float64(0.5440317020291879)}\n",
      "Validation loss decreased (1.197437 --> 0.665775).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10:  50%|█████     | 404/808 [04:05<37:00,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5274914503097534, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 1, 'validation_loss': np.float64(0.39313294042063796), 'macro_f1': np.float64(0.4655164447654623), 'micro_f1': np.float64(0.7516003657978967), 'mae': np.float64(0.298836478499859)}\n",
      "{'loss': 0.5274914503097534, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 2, 'validation_loss': np.float64(0.39313294042063796), 'macro_f1': np.float64(0.43168414647503855), 'micro_f1': np.float64(0.7349108367626886), 'mae': np.float64(0.3305892503354407)}\n",
      "{'loss': 0.5274914503097534, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 3, 'validation_loss': np.float64(0.39313294042063796), 'macro_f1': np.float64(0.36330098222187496), 'micro_f1': np.float64(0.7136488340192044), 'mae': np.float64(0.3709554733992836)}\n",
      "{'loss': 0.5274914503097534, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 4, 'validation_loss': np.float64(0.39313294042063796), 'macro_f1': np.float64(0.38095587081522075), 'micro_f1': np.float64(0.6966163694558757), 'mae': np.float64(0.3953087458627446)}\n",
      "{'loss': 0.5274914503097534, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 5, 'validation_loss': np.float64(0.39313294042063796), 'macro_f1': np.float64(0.345483335661345), 'micro_f1': np.float64(0.6879286694101509), 'mae': np.float64(0.42510923194621836)}\n",
      "{'loss': 0.5274914503097534, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 6, 'validation_loss': np.float64(0.39313294042063796), 'macro_f1': np.float64(0.3129002769600326), 'micro_f1': np.float64(0.6682670324645633), 'mae': np.float64(0.46041384210246555)}\n",
      "Validation loss decreased (0.665775 --> 0.393133).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10: 100%|██████████| 808/808 [08:11<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.21270079910755157, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 1, 'validation_loss': np.float64(0.32792191515150276), 'macro_f1': np.float64(0.44880283518443553), 'micro_f1': np.float64(0.7769775948788294), 'mae': np.float64(0.2539081094172593)}\n",
      "{'loss': 0.21270079910755157, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 2, 'validation_loss': np.float64(0.32792191515150276), 'macro_f1': np.float64(0.40167446929096245), 'micro_f1': np.float64(0.7411979881115683), 'mae': np.float64(0.3064746330854442)}\n",
      "{'loss': 0.21270079910755157, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 3, 'validation_loss': np.float64(0.32792191515150276), 'macro_f1': np.float64(0.38031850581623594), 'micro_f1': np.float64(0.7288523090992227), 'mae': np.float64(0.3350773828053125)}\n",
      "{'loss': 0.21270079910755157, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 4, 'validation_loss': np.float64(0.32792191515150276), 'macro_f1': np.float64(0.356172071425299), 'micro_f1': np.float64(0.7170781893004116), 'mae': np.float64(0.3721862946973259)}\n",
      "{'loss': 0.21270079910755157, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 5, 'validation_loss': np.float64(0.32792191515150276), 'macro_f1': np.float64(0.32845671257114956), 'micro_f1': np.float64(0.7054183813443072), 'mae': np.float64(0.4002139520145767)}\n",
      "{'loss': 0.21270079910755157, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 6, 'validation_loss': np.float64(0.32792191515150276), 'macro_f1': np.float64(0.31030540958446035), 'micro_f1': np.float64(0.6891860996799268), 'mae': np.float64(0.43512118561950264)}\n",
      "Validation loss decreased (0.393133 --> 0.327922).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10:  50%|█████     | 404/808 [04:05<36:59,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.35108041763305664, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 1, 'validation_loss': np.float64(0.304158723656682), 'macro_f1': np.float64(0.43076240530156423), 'micro_f1': np.float64(0.785893918609968), 'mae': np.float64(0.27297030320464405)}\n",
      "{'loss': 0.35108041763305664, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 2, 'validation_loss': np.float64(0.304158723656682), 'macro_f1': np.float64(0.38381687959329785), 'micro_f1': np.float64(0.7409693644261546), 'mae': np.float64(0.33923720586156547)}\n",
      "{'loss': 0.35108041763305664, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 3, 'validation_loss': np.float64(0.304158723656682), 'macro_f1': np.float64(0.35157676597804155), 'micro_f1': np.float64(0.7240512117055327), 'mae': np.float64(0.3618634608527147)}\n",
      "{'loss': 0.35108041763305664, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 4, 'validation_loss': np.float64(0.304158723656682), 'macro_f1': np.float64(0.3135351433695886), 'micro_f1': np.float64(0.6751257430269776), 'mae': np.float64(0.42735850379220836)}\n",
      "{'loss': 0.35108041763305664, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 5, 'validation_loss': np.float64(0.304158723656682), 'macro_f1': np.float64(0.2981553495309212), 'micro_f1': np.float64(0.6239140374942844), 'mae': np.float64(0.47900121332804013)}\n",
      "{'loss': 0.35108041763305664, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 6, 'validation_loss': np.float64(0.304158723656682), 'macro_f1': np.float64(0.2763954754420512), 'micro_f1': np.float64(0.6355738454503886), 'mae': np.float64(0.4956678002833551)}\n",
      "Validation loss decreased (0.327922 --> 0.304159).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10: 100%|██████████| 808/808 [08:11<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.33217960596084595, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 1, 'validation_loss': np.float64(0.5416933061636012), 'macro_f1': np.float64(0.16582348851081682), 'micro_f1': np.float64(0.6762688614540466), 'mae': np.float64(0.3860365805638084)}\n",
      "{'loss': 0.33217960596084595, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 2, 'validation_loss': np.float64(0.5416933061636012), 'macro_f1': np.float64(0.16307033035286073), 'micro_f1': np.float64(0.6723822588020119), 'mae': np.float64(0.40675130964608763)}\n",
      "{'loss': 0.33217960596084595, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 3, 'validation_loss': np.float64(0.5416933061636012), 'macro_f1': np.float64(0.16835420823931913), 'micro_f1': np.float64(0.6751257430269776), 'mae': np.float64(0.41673772781807356)}\n",
      "{'loss': 0.33217960596084595, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 4, 'validation_loss': np.float64(0.5416933061636012), 'macro_f1': np.float64(0.17416576489392746), 'micro_f1': np.float64(0.6771833561957019), 'mae': np.float64(0.43282058700056625)}\n",
      "{'loss': 0.33217960596084595, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 5, 'validation_loss': np.float64(0.5416933061636012), 'macro_f1': np.float64(0.17751875234362965), 'micro_f1': np.float64(0.6760402377686329), 'mae': np.float64(0.4367954368692923)}\n",
      "{'loss': 0.33217960596084595, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 6, 'validation_loss': np.float64(0.5416933061636012), 'macro_f1': np.float64(0.17445861090930645), 'micro_f1': np.float64(0.6684956561499772), 'mae': np.float64(0.4553976964481781)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10:  50%|█████     | 404/808 [04:05<36:52,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2981545329093933, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 1, 'validation_loss': np.float64(0.33610727843167126), 'macro_f1': np.float64(0.4968798269565699), 'micro_f1': np.float64(0.7323959762231367), 'mae': np.float64(0.3233684248063041)}\n",
      "{'loss': 0.2981545329093933, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 2, 'validation_loss': np.float64(0.33610727843167126), 'macro_f1': np.float64(0.4564662121974871), 'micro_f1': np.float64(0.6744398719707362), 'mae': np.float64(0.3824414005152019)}\n",
      "{'loss': 0.2981545329093933, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 3, 'validation_loss': np.float64(0.33610727843167126), 'macro_f1': np.float64(0.42905206293987525), 'micro_f1': np.float64(0.6620941929583904), 'mae': np.float64(0.41217887988791496)}\n",
      "{'loss': 0.2981545329093933, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 4, 'validation_loss': np.float64(0.33610727843167126), 'macro_f1': np.float64(0.3748450209904377), 'micro_f1': np.float64(0.6101966163694559), 'mae': np.float64(0.46024502578714444)}\n",
      "{'loss': 0.2981545329093933, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 5, 'validation_loss': np.float64(0.33610727843167126), 'macro_f1': np.float64(0.35456073323338516), 'micro_f1': np.float64(0.5531550068587106), 'mae': np.float64(0.5099210329210404)}\n",
      "{'loss': 0.2981545329093933, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 6, 'validation_loss': np.float64(0.33610727843167126), 'macro_f1': np.float64(0.33651453333939846), 'micro_f1': np.float64(0.5462962962962963), 'mae': np.float64(0.5293958235469267)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10:  54%|█████▍    | 440/808 [04:26<03:42,  1.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m inputs, labels, static \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 36\u001b[0m     \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     37\u001b[0m     labels\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     38\u001b[0m     static\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     41\u001b[0m output\u001b[38;5;241m=\u001b[39m model(inputs, static)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU\")\n",
    "\n",
    "model = HybridModel(\n",
    "    num_numerical_features=X_tabular_train.shape[-1],\n",
    "    num_time_series_features=X_time_train.shape[-1],\n",
    "    hidden_size=hidden_dim,\n",
    "    num_lstm_layers=n_layers,\n",
    "    num_fc_tabular_layers=ffnn_layers,\n",
    "    num_fc_combined_layers=2,\n",
    "    output_size=output_weeks,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "loss_function = nn.HubberLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "counter = 0\n",
    "valid_loss_min = np.inf\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for k, (inputs, static, labels) in tqdm(enumerate(train_loader),\n",
    "                                            desc=f\"epoch {i+1}/{epochs}\",\n",
    "                                            total=len(train_loader),):\n",
    "        model.train()\n",
    "        counter += 1\n",
    "        inputs, labels, static = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "            static.to(device),\n",
    "        )\n",
    "        model.zero_grad()\n",
    "        output= model(inputs, static)\n",
    "        loss = loss_function(output, labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                labels = []\n",
    "                preds = []\n",
    "                raw_labels = []\n",
    "                raw_preds = []\n",
    "                for inp, stat, lab in valid_loader:\n",
    "                    inp, lab, stat = inp.to(device), lab.to(device), stat.to(device)\n",
    "                    out = model(inp, stat)\n",
    "                    val_loss = loss_function(out, lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    for labs in lab:\n",
    "                        labels.append([int(l.round()) for l in labs])\n",
    "                        raw_labels.append([float(l) for l in labs])\n",
    "                    for pred in out:\n",
    "                        preds.append([int(p.round()) for p in pred])\n",
    "                        raw_preds.append([float(p) for p in pred])\n",
    "                # log data\n",
    "                labels = np.array(labels)\n",
    "                preds = np.clip(np.array(preds), 0, 5)\n",
    "                raw_preds = np.array(raw_preds)\n",
    "                raw_labels = np.array(raw_labels)\n",
    "                for i in range(output_weeks):\n",
    "                    log_dict = {\n",
    "                        \"loss\": float(loss),\n",
    "                        \"epoch\": counter / len(train_loader),\n",
    "                        \"step\": counter,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"week\": i + 1,\n",
    "                    }\n",
    "                    # w = f'week_{i+1}_'\n",
    "                    w = \"\"\n",
    "                    log_dict[f\"{w}validation_loss\"] = np.mean(val_losses)\n",
    "                    log_dict[f\"{w}macro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"macro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}micro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"micro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}mae\"] = mean_absolute_error(\n",
    "                        raw_labels[:, i], raw_preds[:, i]\n",
    "                    )\n",
    "                    print(log_dict)\n",
    "                    writer.add_scalars(\"Loss(Hubber)\", {'train': loss,\n",
    "                                                     'validation': log_dict[f\"{w}validation_loss\"]},\n",
    "                                                     counter)\n",
    "                    writer.add_scalars(\"F1(MSE)\", {'macro': log_dict[f\"{w}macro_f1\"],\n",
    "                                                   'micro': log_dict[f\"{w}micro_f1\"]},\n",
    "                                                   counter)\n",
    "                    writer.add_scalar(\"MAE\", log_dict[f\"{w}mae\"],\n",
    "                                      counter)\n",
    "                    writer.add_scalar(\"Learning-Rate\", log_dict[\"lr\"],\n",
    "                                      counter)\n",
    "                    for j, f1 in enumerate(\n",
    "                        f1_score(labels[:, i], preds[:, i], average=None)\n",
    "                    ):\n",
    "                        log_dict[f\"{w}{id2class[j]}_f1\"] = f1\n",
    "                    model.train()\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), \"./state_dict.pt\")\n",
    "                    print(\n",
    "                        \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                            valid_loss_min, np.mean(val_losses)\n",
    "                        )\n",
    "                    )\n",
    "                    valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, static):\n",
    "    out= model(torch.tensor(x), static)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation predictions...:   0%|          | 0/69 [00:00<?, ?it/s]/tmp/ipykernel_1022891/447625881.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out= model(torch.tensor(x), static)\n",
      "validation predictions...: 100%|██████████| 69/69 [00:11<00:00,  6.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_map = {\n",
    "    \"y_pred\": [],\n",
    "    \"y_pred_rounded\": [],\n",
    "    # \"fips\": [],\n",
    "    # \"date\": [],\n",
    "    \"y_true\": [],\n",
    "    \"week\": [],\n",
    "}\n",
    "i = 0\n",
    "for x, static, y in tqdm(\n",
    "    valid_loader, # ou test_loader\n",
    "    desc=\"validation predictions...\",):\n",
    "    x, static, y = x.to(device), static.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = predict(x, static).clone().detach()\n",
    "    for w in range(output_weeks):\n",
    "        dict_map[\"y_pred\"] += [float(p[w]) for p in pred]\n",
    "        dict_map[\"y_pred_rounded\"] += [int(p.round()[w]) for p in pred]\n",
    "        # dict_map[\"fips\"] += [f[1][0] for f in valid_fips[i : i + len(x)]]\n",
    "        # dict_map[\"date\"] += [f[1][1] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"y_true\"] += [float(item[w]) for item in y]\n",
    "        dict_map[\"week\"] += [w] * len(x)\n",
    "    i += len(x)\n",
    "df = pd.DataFrame(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 1 MAE 0.148 F1 0.787\n",
      "Week 2 MAE 0.203 F1 0.715\n",
      "Week 3 MAE 0.256 F1 0.663\n",
      "Week 4 MAE 0.31 F1 0.591\n",
      "Week 5 MAE 0.357 F1 0.545\n",
      "Week 6 MAE 0.399 F1 0.499\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for w in range(6):\n",
    "    wdf = df[df['week']==w]\n",
    "    mae = mean_absolute_error(wdf['y_true'], wdf['y_pred']).round(3)\n",
    "    f1 = f1_score(wdf['y_true'].round(),wdf['y_pred'].round(), average='macro').round(3)\n",
    "    print(f\"Week {w+1}\", f\"MAE {mae}\", f\"F1 {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_pred_rounded</th>\n",
       "      <th>y_true</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001810</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040762</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.007563</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031373</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.878264</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52483</th>\n",
       "      <td>0.100800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52484</th>\n",
       "      <td>1.101720</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9211</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52485</th>\n",
       "      <td>0.198933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52486</th>\n",
       "      <td>0.209685</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52487</th>\n",
       "      <td>0.246993</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52488 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y_pred  y_pred_rounded  y_true  week\n",
       "0     -0.001810               0  0.0000     0\n",
       "1      0.040762               0  0.0000     0\n",
       "2     -0.007563               0  0.0000     0\n",
       "3      0.031373               0  0.0000     0\n",
       "4      0.878264               1  0.7767     0\n",
       "...         ...             ...     ...   ...\n",
       "52483  0.100800               0  0.9964     5\n",
       "52484  1.101720               1  1.9211     5\n",
       "52485  0.198933               0  0.0000     5\n",
       "52486  0.209685               0  0.0000     5\n",
       "52487  0.246993               0  0.0000     5\n",
       "\n",
       "[52488 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(abs(df['y_true'] - df['y_pred']), bins=40, alpha=0.7, label='error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted values vs residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['y_pred'], df['y_true'] - df['y_pred'], alpha=0.4)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['y_pred'], df['y_true'], alpha=0.4)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
