{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First contact with the dataset\n",
    "This Notebook has as objective to replicate the results of Minixhofer et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data in a unique dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soil_data.csv', 'train_timeseries', 'test_timeseries', 'validation_timeseries']\n"
     ]
    }
   ],
   "source": [
    "filesList = os.listdir('../src')\n",
    "print(filesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDic = {\"train\": pd.read_csv(\"../src/train_timeseries/train_timeseries.csv\"),\n",
    "           \"test\": pd.read_csv(\"../src/test_timeseries/test_timeseries.csv\"),\n",
    "           \"validation\": pd.read_csv(\"../src/validation_timeseries/validation_timeseries.csv\"),\n",
    "           \"soil\" : pd.read_csv(\"../src/soil_data.csv\"),\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fips', 'date', 'PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
       "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
       "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
       "       'WS50M_RANGE', 'score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDic[\"train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: dataDic[k].set_index(['fips', 'date'])\n",
    "    for k in dataDic.keys() if k != \"soil\"\n",
    "}\n",
    "\n",
    "dfs[\"soil\"] = dataDic[\"soil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PRECTOT</th>\n",
       "      <th>PS</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2MDEW</th>\n",
       "      <th>T2MWET</th>\n",
       "      <th>T2M_MAX</th>\n",
       "      <th>T2M_MIN</th>\n",
       "      <th>T2M_RANGE</th>\n",
       "      <th>TS</th>\n",
       "      <th>WS10M</th>\n",
       "      <th>WS10M_MAX</th>\n",
       "      <th>WS10M_MIN</th>\n",
       "      <th>WS10M_RANGE</th>\n",
       "      <th>WS50M</th>\n",
       "      <th>WS50M_MAX</th>\n",
       "      <th>WS50M_MIN</th>\n",
       "      <th>WS50M_RANGE</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1001</th>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>0.22</td>\n",
       "      <td>100.51</td>\n",
       "      <td>9.65</td>\n",
       "      <td>14.74</td>\n",
       "      <td>13.51</td>\n",
       "      <td>13.51</td>\n",
       "      <td>20.96</td>\n",
       "      <td>11.46</td>\n",
       "      <td>9.50</td>\n",
       "      <td>14.65</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.85</td>\n",
       "      <td>6.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>0.20</td>\n",
       "      <td>100.55</td>\n",
       "      <td>10.42</td>\n",
       "      <td>16.69</td>\n",
       "      <td>14.71</td>\n",
       "      <td>14.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>12.61</td>\n",
       "      <td>10.18</td>\n",
       "      <td>16.60</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.60</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>3.65</td>\n",
       "      <td>100.15</td>\n",
       "      <td>11.76</td>\n",
       "      <td>18.49</td>\n",
       "      <td>16.52</td>\n",
       "      <td>16.52</td>\n",
       "      <td>22.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>7.41</td>\n",
       "      <td>18.41</td>\n",
       "      <td>4.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.67</td>\n",
       "      <td>7.53</td>\n",
       "      <td>9.52</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.66</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>15.95</td>\n",
       "      <td>100.29</td>\n",
       "      <td>6.42</td>\n",
       "      <td>11.40</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.10</td>\n",
       "      <td>18.09</td>\n",
       "      <td>2.16</td>\n",
       "      <td>15.92</td>\n",
       "      <td>11.31</td>\n",
       "      <td>3.84</td>\n",
       "      <td>5.67</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.59</td>\n",
       "      <td>6.73</td>\n",
       "      <td>9.31</td>\n",
       "      <td>3.74</td>\n",
       "      <td>5.58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>0.00</td>\n",
       "      <td>101.15</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>10.82</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>13.48</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">56043</th>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>0.16</td>\n",
       "      <td>82.88</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-7.97</td>\n",
       "      <td>-13.49</td>\n",
       "      <td>-12.81</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-13.60</td>\n",
       "      <td>12.21</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.63</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.02</td>\n",
       "      <td>8.58</td>\n",
       "      <td>10.39</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>0.02</td>\n",
       "      <td>83.33</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-14.10</td>\n",
       "      <td>-13.84</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-13.56</td>\n",
       "      <td>11.07</td>\n",
       "      <td>-10.55</td>\n",
       "      <td>6.50</td>\n",
       "      <td>11.43</td>\n",
       "      <td>4.11</td>\n",
       "      <td>7.32</td>\n",
       "      <td>9.92</td>\n",
       "      <td>14.49</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>0.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-7.96</td>\n",
       "      <td>-13.30</td>\n",
       "      <td>-13.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-14.51</td>\n",
       "      <td>14.93</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>4.29</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2.03</td>\n",
       "      <td>4.22</td>\n",
       "      <td>6.56</td>\n",
       "      <td>10.07</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>1.22</td>\n",
       "      <td>82.49</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-7.40</td>\n",
       "      <td>-7.33</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-6.86</td>\n",
       "      <td>10.62</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7.34</td>\n",
       "      <td>1.99</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.28</td>\n",
       "      <td>10.12</td>\n",
       "      <td>3.24</td>\n",
       "      <td>6.89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>0.44</td>\n",
       "      <td>82.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>-11.98</td>\n",
       "      <td>-11.82</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-11.61</td>\n",
       "      <td>10.66</td>\n",
       "      <td>-10.17</td>\n",
       "      <td>2.31</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.37</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19300680 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PRECTOT      PS   QV2M    T2M  T2MDEW  T2MWET  T2M_MAX  \\\n",
       "fips  date                                                                 \n",
       "1001  2000-01-01     0.22  100.51   9.65  14.74   13.51   13.51    20.96   \n",
       "      2000-01-02     0.20  100.55  10.42  16.69   14.71   14.71    22.80   \n",
       "      2000-01-03     3.65  100.15  11.76  18.49   16.52   16.52    22.73   \n",
       "      2000-01-04    15.95  100.29   6.42  11.40    6.09    6.10    18.09   \n",
       "      2000-01-05     0.00  101.15   2.95   3.86   -3.29   -3.20    10.82   \n",
       "...                   ...     ...    ...    ...     ...     ...      ...   \n",
       "56043 2016-12-27     0.16   82.88   1.63  -7.97  -13.49  -12.81    -1.39   \n",
       "      2016-12-28     0.02   83.33   1.41  -8.71  -14.10  -13.84    -2.49   \n",
       "      2016-12-29     0.00   83.75   1.59  -7.96  -13.30  -13.03     0.42   \n",
       "      2016-12-30     1.22   82.49   2.63  -2.94   -7.40   -7.33     3.76   \n",
       "      2016-12-31     0.44   82.19   1.75  -7.56  -11.98  -11.82    -0.95   \n",
       "\n",
       "                  T2M_MIN  T2M_RANGE     TS  WS10M  WS10M_MAX  WS10M_MIN  \\\n",
       "fips  date                                                                 \n",
       "1001  2000-01-01    11.46       9.50  14.65   2.20       2.94       1.49   \n",
       "      2000-01-02    12.61      10.18  16.60   2.52       3.43       1.83   \n",
       "      2000-01-03    15.32       7.41  18.41   4.03       5.33       2.66   \n",
       "      2000-01-04     2.16      15.92  11.31   3.84       5.67       2.08   \n",
       "      2000-01-05    -2.66      13.48   2.65   1.60       2.50       0.52   \n",
       "...                   ...        ...    ...    ...        ...        ...   \n",
       "56043 2016-12-27   -13.60      12.21  -9.41   5.90       7.63       3.61   \n",
       "      2016-12-28   -13.56      11.07 -10.55   6.50      11.43       4.11   \n",
       "      2016-12-29   -14.51      14.93 -10.29   4.29       6.24       2.03   \n",
       "      2016-12-30    -6.86      10.62  -4.14   4.98       7.34       1.99   \n",
       "      2016-12-31   -11.61      10.66 -10.17   2.31       3.47       0.41   \n",
       "\n",
       "                  WS10M_RANGE  WS50M  WS50M_MAX  WS50M_MIN  WS50M_RANGE  score  \n",
       "fips  date                                                                      \n",
       "1001  2000-01-01         1.46   4.85       6.04       3.23         2.81    NaN  \n",
       "      2000-01-02         1.60   5.33       6.13       3.72         2.41    NaN  \n",
       "      2000-01-03         2.67   7.53       9.52       5.87         3.66    NaN  \n",
       "      2000-01-04         3.59   6.73       9.31       3.74         5.58    1.0  \n",
       "      2000-01-05         1.98   2.94       4.85       0.65         4.19    NaN  \n",
       "...                       ...    ...        ...        ...          ...    ...  \n",
       "56043 2016-12-27         4.02   8.58      10.39       5.92         4.47    0.0  \n",
       "      2016-12-28         7.32   9.92      14.49       7.26         7.22    NaN  \n",
       "      2016-12-29         4.22   6.56      10.07       3.20         6.87    NaN  \n",
       "      2016-12-30         5.35   7.28      10.12       3.24         6.89    NaN  \n",
       "      2016-12-31         3.06   3.37       5.26       0.66         4.60    NaN  \n",
       "\n",
       "[19300680 rows x 19 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation pour les données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to encode the cycling feature: year-day, using sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (\n",
    "        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadXY(\n",
    "    df,\n",
    "    random_state=42, # keep this at 42\n",
    "    window_size=180, # how many days in the past (default/competition: 180)\n",
    "    target_size=6, # how many weeks into the future (default/competition: 6)\n",
    "    fuse_past=True, # add the past drought observations? (default: True)\n",
    "    return_fips=False, # return the county identifier (do not use for predictions)\n",
    "    encode_season=True, # encode the season using the function above (default: True) \n",
    "    use_prev_year=False, # add observations from 1 year prior?\n",
    "):\n",
    "    df = dfs[df]\n",
    "    soil_df = dfs[\"soil\"]\n",
    "    time_data_cols = sorted(\n",
    "        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n",
    "    )\n",
    "    static_data_cols = sorted(\n",
    "        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n",
    "    )\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=[\"score\"])\n",
    "    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if use_prev_year:\n",
    "        add_dim += len(time_data_cols)\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "        if use_prev_year:\n",
    "            add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty(\n",
    "        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n",
    "    )\n",
    "    y_past = np.empty((len(df) // window_size, window_size))\n",
    "    y_target = np.empty((len(df) // window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0) == fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df[\"score\"].values\n",
    "        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n",
    "            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n",
    "            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n",
    "            if use_prev_year:\n",
    "                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n",
    "                    continue\n",
    "                X_time[count, :, -len(time_data_cols) :] = X[\n",
    "                    i - 365 : i + window_size - 365\n",
    "                ]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i : i + window_size])\n",
    "            else:\n",
    "                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n",
    "                    y[i : i + window_size]\n",
    "                )\n",
    "            if encode_season:\n",
    "                enc_dates = [\n",
    "                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n",
    "                ]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n",
    "            temp_y = y[i + window_size : i + window_size + target_size * 7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    print(f\"loaded {count} samples\")\n",
    "    results = [X_static[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}\n",
    "\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n",
    "        X_time[:, :, index] = (\n",
    "            scaler_dict[index]\n",
    "            .transform(X_time[:, :, index].reshape(-1, 1))\n",
    "            .reshape(-1, X_time.shape[-2])\n",
    "        )\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:, index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:, index] = (\n",
    "            scaler_dict_static[index]\n",
    "            .transform(X_static[:, index].reshape(-1, 1))\n",
    "            .reshape(1, -1)\n",
    "        )\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n",
    "        y_past[:, :] = (\n",
    "            scaler_dict_past[index]\n",
    "            .transform(y_past.reshape(-1, 1))\n",
    "            .reshape(-1, y_past.shape[-1])\n",
    "        )\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [04:25<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 103390 samples\n",
      "train shape (103390, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [00:25<00:00, 120.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 8748 samples\n",
      "validation shape (8748, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:16<00:00,  1.27it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 466.52it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 42.65it/s]\n",
      "100%|██████████| 30/30 [00:00<00:00, 13648.89it/s]\n"
     ]
    }
   ],
   "source": [
    "X_tabular_train, X_time_train, y_target_train = loadXY(\"train\")\n",
    "print(\"train shape\", X_time_train.shape)\n",
    "X_tabular_validation, X_time_valid, y_target_valid, valid_fips = loadXY(\"validation\", return_fips=True)\n",
    "print(\"validation shape\", X_time_valid.shape)\n",
    "X_tabular_train, X_time_train = normalize(X_tabular_train, X_time_train, fit=True)\n",
    "X_tabular_validation, X_time_valid = normalize(X_tabular_validation, X_time_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_weeks = 6\n",
    "use_static = True\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "ffnn_layers = 2\n",
    "dropout = 0.1\n",
    "one_cycle = True\n",
    "lr = 7e-5\n",
    "epochs = 10\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_time_train),\n",
    "    torch.tensor(X_tabular_train),\n",
    "    torch.tensor(y_target_train[:, :output_weeks]),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.tensor(X_time_valid),\n",
    "    torch.tensor(X_tabular_validation),\n",
    "    torch.tensor(y_target_valid[:, :output_weeks]),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "class DroughtNetLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size,\n",
    "        num_input_features,\n",
    "        hidden_dim,\n",
    "        n_layers,\n",
    "        ffnn_layers,\n",
    "        drop_prob,\n",
    "        static_dim=0,\n",
    "    ):\n",
    "        super(DroughtNetLSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            num_input_features,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fflayers = []\n",
    "        for i in range(ffnn_layers - 1):\n",
    "            if i == 0:\n",
    "                self.fflayers.append(nn.Linear(hidden_dim + static_dim, hidden_dim))\n",
    "            else:\n",
    "                self.fflayers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fflayers = nn.ModuleList(self.fflayers)\n",
    "        self.final = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, static=None):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        if static is not None:\n",
    "            static = static.to(dtype=torch.float32)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        for i in range(len(self.fflayers)):\n",
    "            if i == 0 and static is not None:\n",
    "                out = self.fflayers[i](torch.cat((out, static), 1))\n",
    "            else:\n",
    "                out = self.fflayers[i](out)\n",
    "        out = self.final(out)\n",
    "\n",
    "        out = out.view(batch_size, -1)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10:  50%|█████     | 404/808 [02:27<26:34,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4094846844673157, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 1, 'validation_loss': np.float64(0.41741873647855676), 'macro_f1': np.float64(0.49162178476306456), 'micro_f1': np.float64(0.7056470050297211), 'mae': np.float64(0.4319537513668917)}\n",
      "{'loss': 0.4094846844673157, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 2, 'validation_loss': np.float64(0.41741873647855676), 'macro_f1': np.float64(0.42621899660617557), 'micro_f1': np.float64(0.6494055784179241), 'mae': np.float64(0.485662974291411)}\n",
      "{'loss': 0.4094846844673157, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 3, 'validation_loss': np.float64(0.41741873647855676), 'macro_f1': np.float64(0.3832968291751599), 'micro_f1': np.float64(0.6251714677640604), 'mae': np.float64(0.5237166664131289)}\n",
      "{'loss': 0.4094846844673157, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 4, 'validation_loss': np.float64(0.41741873647855676), 'macro_f1': np.float64(0.376220674753615), 'micro_f1': np.float64(0.6228852309099223), 'mae': np.float64(0.5272624928439493)}\n",
      "{'loss': 0.4094846844673157, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 5, 'validation_loss': np.float64(0.41741873647855676), 'macro_f1': np.float64(0.35587861201979587), 'micro_f1': np.float64(0.5964791952446273), 'mae': np.float64(0.559913897582139)}\n",
      "{'loss': 0.4094846844673157, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 6, 'validation_loss': np.float64(0.41741873647855676), 'macro_f1': np.float64(0.3260978070043755), 'micro_f1': np.float64(0.5964791952446273), 'mae': np.float64(0.5634948184312351)}\n",
      "Validation loss decreased (inf --> 0.417419).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 100%|██████████| 808/808 [04:56<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.22092455625534058, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 1, 'validation_loss': np.float64(0.24655242415441983), 'macro_f1': np.float64(0.7675159300585278), 'micro_f1': np.float64(0.8699131229995427), 'mae': np.float64(0.21856773082608355)}\n",
      "{'loss': 0.22092455625534058, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 2, 'validation_loss': np.float64(0.24655242415441983), 'macro_f1': np.float64(0.6794433732123664), 'micro_f1': np.float64(0.8182441700960219), 'mae': np.float64(0.2667220282446425)}\n",
      "{'loss': 0.22092455625534058, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 3, 'validation_loss': np.float64(0.24655242415441983), 'macro_f1': np.float64(0.6269059077376175), 'micro_f1': np.float64(0.7801783264746228), 'mae': np.float64(0.3134492456122819)}\n",
      "{'loss': 0.22092455625534058, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 4, 'validation_loss': np.float64(0.24655242415441983), 'macro_f1': np.float64(0.5723915602844878), 'micro_f1': np.float64(0.7408550525834476), 'mae': np.float64(0.36073910359679207)}\n",
      "{'loss': 0.22092455625534058, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 5, 'validation_loss': np.float64(0.24655242415441983), 'macro_f1': np.float64(0.4897050823482911), 'micro_f1': np.float64(0.7098765432098766), 'mae': np.float64(0.40489733712139875)}\n",
      "{'loss': 0.22092455625534058, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 6, 'validation_loss': np.float64(0.24655242415441983), 'macro_f1': np.float64(0.4358694241274937), 'micro_f1': np.float64(0.6870141746684957), 'mae': np.float64(0.4368981165026553)}\n",
      "Validation loss decreased (0.417419 --> 0.246552).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10:  50%|█████     | 404/808 [02:28<26:48,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3058808743953705, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 1, 'validation_loss': np.float64(0.23733237315563188), 'macro_f1': np.float64(0.780181140314852), 'micro_f1': np.float64(0.8810013717421125), 'mae': np.float64(0.18999120643393813)}\n",
      "{'loss': 0.3058808743953705, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 2, 'validation_loss': np.float64(0.23733237315563188), 'macro_f1': np.float64(0.7061587730609968), 'micro_f1': np.float64(0.8278463648834019), 'mae': np.float64(0.24193130400416185)}\n",
      "{'loss': 0.3058808743953705, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 3, 'validation_loss': np.float64(0.23733237315563188), 'macro_f1': np.float64(0.6461879449883297), 'micro_f1': np.float64(0.7866941015089163), 'mae': np.float64(0.2951739036022269)}\n",
      "{'loss': 0.3058808743953705, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 4, 'validation_loss': np.float64(0.23733237315563188), 'macro_f1': np.float64(0.5768901949027048), 'micro_f1': np.float64(0.74519890260631), 'mae': np.float64(0.34409775829519146)}\n",
      "{'loss': 0.3058808743953705, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 5, 'validation_loss': np.float64(0.23733237315563188), 'macro_f1': np.float64(0.5318076012167026), 'micro_f1': np.float64(0.7159350708733425), 'mae': np.float64(0.3891289971196664)}\n",
      "{'loss': 0.3058808743953705, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 6, 'validation_loss': np.float64(0.23733237315563188), 'macro_f1': np.float64(0.5006655335347503), 'micro_f1': np.float64(0.6820987654320988), 'mae': np.float64(0.4340757984556377)}\n",
      "Validation loss decreased (0.246552 --> 0.237332).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10: 100%|██████████| 808/808 [04:56<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24517464637756348, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 1, 'validation_loss': np.float64(0.22640269865160403), 'macro_f1': np.float64(0.6430461783243833), 'micro_f1': np.float64(0.8776863283036123), 'mae': np.float64(0.17175334666851186)}\n",
      "{'loss': 0.24517464637756348, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 2, 'validation_loss': np.float64(0.22640269865160403), 'macro_f1': np.float64(0.56380394980159), 'micro_f1': np.float64(0.8245313214449017), 'mae': np.float64(0.23194525412324735)}\n",
      "{'loss': 0.24517464637756348, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 3, 'validation_loss': np.float64(0.22640269865160403), 'macro_f1': np.float64(0.47319659026853006), 'micro_f1': np.float64(0.7788065843621399), 'mae': np.float64(0.2881543252586252)}\n",
      "{'loss': 0.24517464637756348, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 4, 'validation_loss': np.float64(0.22640269865160403), 'macro_f1': np.float64(0.4116465203954433), 'micro_f1': np.float64(0.7411979881115683), 'mae': np.float64(0.3402061691097329)}\n",
      "{'loss': 0.24517464637756348, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 5, 'validation_loss': np.float64(0.22640269865160403), 'macro_f1': np.float64(0.3757856593902107), 'micro_f1': np.float64(0.7107910379515318), 'mae': np.float64(0.3771016626882927)}\n",
      "{'loss': 0.24517464637756348, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 6, 'validation_loss': np.float64(0.22640269865160403), 'macro_f1': np.float64(0.34460135001102926), 'micro_f1': np.float64(0.6819844535893919), 'mae': np.float64(0.4152662245945304)}\n",
      "Validation loss decreased (0.237332 --> 0.226403).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10:  50%|█████     | 404/808 [02:28<26:32,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24621926248073578, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 1, 'validation_loss': np.float64(0.2279852118505084), 'macro_f1': np.float64(0.7076798848220333), 'micro_f1': np.float64(0.8852309099222679), 'mae': np.float64(0.17500406669844157)}\n",
      "{'loss': 0.24621926248073578, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 2, 'validation_loss': np.float64(0.2279852118505084), 'macro_f1': np.float64(0.6309731875793292), 'micro_f1': np.float64(0.8270461819844536), 'mae': np.float64(0.23403148351083772)}\n",
      "{'loss': 0.24621926248073578, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 3, 'validation_loss': np.float64(0.2279852118505084), 'macro_f1': np.float64(0.5986587970334974), 'micro_f1': np.float64(0.782350251486054), 'mae': np.float64(0.2949911428322209)}\n",
      "{'loss': 0.24621926248073578, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 4, 'validation_loss': np.float64(0.2279852118505084), 'macro_f1': np.float64(0.5486539284157461), 'micro_f1': np.float64(0.7426840420667581), 'mae': np.float64(0.34022456681331814)}\n",
      "{'loss': 0.24621926248073578, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 5, 'validation_loss': np.float64(0.2279852118505084), 'macro_f1': np.float64(0.47246719787312724), 'micro_f1': np.float64(0.7109053497942387), 'mae': np.float64(0.3844562759621431)}\n",
      "{'loss': 0.24621926248073578, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 6, 'validation_loss': np.float64(0.2279852118505084), 'macro_f1': np.float64(0.40648141885431555), 'micro_f1': np.float64(0.6778692272519433), 'mae': np.float64(0.4266569556935971)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10: 100%|██████████| 808/808 [04:56<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25213363766670227, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 1, 'validation_loss': np.float64(0.22551781765144804), 'macro_f1': np.float64(0.7811816570604232), 'micro_f1': np.float64(0.8830589849108368), 'mae': np.float64(0.16936440724378404)}\n",
      "{'loss': 0.25213363766670227, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 2, 'validation_loss': np.float64(0.22551781765144804), 'macro_f1': np.float64(0.7099954484769864), 'micro_f1': np.float64(0.8285322359396433), 'mae': np.float64(0.23013675750972004)}\n",
      "{'loss': 0.25213363766670227, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 3, 'validation_loss': np.float64(0.22551781765144804), 'macro_f1': np.float64(0.6647671499070297), 'micro_f1': np.float64(0.7874942844078646), 'mae': np.float64(0.28225138873771266)}\n",
      "{'loss': 0.25213363766670227, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 4, 'validation_loss': np.float64(0.22551781765144804), 'macro_f1': np.float64(0.6022277706073265), 'micro_f1': np.float64(0.7498856881572931), 'mae': np.float64(0.3289039768144983)}\n",
      "{'loss': 0.25213363766670227, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 5, 'validation_loss': np.float64(0.22551781765144804), 'macro_f1': np.float64(0.5430100544431231), 'micro_f1': np.float64(0.7205075445816187), 'mae': np.float64(0.3784269861117928)}\n",
      "{'loss': 0.25213363766670227, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 6, 'validation_loss': np.float64(0.22551781765144804), 'macro_f1': np.float64(0.5069303505842521), 'micro_f1': np.float64(0.6917009602194787), 'mae': np.float64(0.41429712420635584)}\n",
      "Validation loss decreased (0.226403 --> 0.225518).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10:  48%|████▊     | 387/808 [02:10<02:22,  2.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     h \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m     45\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([e\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m h])\n\u001b[1;32m     46\u001b[0m inputs, labels, static \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 47\u001b[0m     \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     48\u001b[0m     labels\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     49\u001b[0m     static\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_static:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU\")\n",
    "static_dim = 0\n",
    "if use_static:\n",
    "    static_dim = X_tabular_train.shape[-1]\n",
    "model = DroughtNetLSTM(\n",
    "    output_weeks,\n",
    "    X_time_train.shape[-1],\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    ffnn_layers,\n",
    "    dropout,\n",
    "    static_dim,\n",
    ")\n",
    "model.to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "if one_cycle:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "counter = 0\n",
    "valid_loss_min = np.inf\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "\n",
    "    for k, (inputs, static, labels) in tqdm(\n",
    "        enumerate(train_loader),\n",
    "        desc=f\"epoch {i+1}/{epochs}\",\n",
    "        total=len(train_loader),\n",
    "    ):\n",
    "        model.train()\n",
    "        counter += 1\n",
    "        if len(inputs) < batch_size:\n",
    "            h = model.init_hidden(len(inputs))\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, static = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "            static.to(device),\n",
    "        )\n",
    "        model.zero_grad()\n",
    "        if use_static:\n",
    "            output, h = model(inputs, h, static)\n",
    "        else:\n",
    "            output, h = model(inputs, h)\n",
    "        loss = loss_function(output, labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                labels = []\n",
    "                preds = []\n",
    "                raw_labels = []\n",
    "                raw_preds = []\n",
    "                for inp, stat, lab in valid_loader:\n",
    "                    if len(inp) < batch_size:\n",
    "                        val_h = model.init_hidden(len(inp))\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inp, lab, stat = inp.to(device), lab.to(device), stat.to(device)\n",
    "                    if use_static:\n",
    "                        out, val_h = model(inp, val_h, stat)\n",
    "                    else:\n",
    "                        out, val_h = model(inp, val_h)\n",
    "                    val_loss = loss_function(out, lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    for labs in lab:\n",
    "                        labels.append([int(l.round()) for l in labs])\n",
    "                        raw_labels.append([float(l) for l in labs])\n",
    "                    for pred in out:\n",
    "                        preds.append([int(p.round()) for p in pred])\n",
    "                        raw_preds.append([float(p) for p in pred])\n",
    "                # log data\n",
    "                labels = np.array(labels)\n",
    "                preds = np.clip(np.array(preds), 0, 5)\n",
    "                raw_preds = np.array(raw_preds)\n",
    "                raw_labels = np.array(raw_labels)\n",
    "                for i in range(output_weeks):\n",
    "                    log_dict = {\n",
    "                        \"loss\": float(loss),\n",
    "                        \"epoch\": counter / len(train_loader),\n",
    "                        \"step\": counter,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"week\": i + 1,\n",
    "                    }\n",
    "                    # w = f'week_{i+1}_'\n",
    "                    w = \"\"\n",
    "                    log_dict[f\"{w}validation_loss\"] = np.mean(val_losses)\n",
    "                    log_dict[f\"{w}macro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"macro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}micro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"micro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}mae\"] = mean_absolute_error(\n",
    "                        raw_labels[:, i], raw_preds[:, i]\n",
    "                    )\n",
    "                    print(log_dict)\n",
    "                    for j, f1 in enumerate(\n",
    "                        f1_score(labels[:, i], preds[:, i], average=None)\n",
    "                    ):\n",
    "                        log_dict[f\"{w}{id2class[j]}_f1\"] = f1\n",
    "                    model.train()\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), \"./state_dict.pt\")\n",
    "                    print(\n",
    "                        \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                            valid_loss_min, np.mean(val_losses)\n",
    "                        )\n",
    "                    )\n",
    "                    valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, static=None):\n",
    "    if static is None:\n",
    "        out, _ = model(torch.tensor(x), val_h)\n",
    "    else:\n",
    "        out, _ = model(torch.tensor(x), val_h, static)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map = {\n",
    "    \"y_pred\": [],\n",
    "    \"y_pred_rounded\": [],\n",
    "    \"fips\": [],\n",
    "    \"date\": [],\n",
    "    \"y_true\": [],\n",
    "    \"week\": [],\n",
    "}\n",
    "i = 0\n",
    "for x, static, y in tqdm(\n",
    "    valid_loader,\n",
    "    desc=\"validation predictions...\",\n",
    "):\n",
    "    val_h = tuple([each.data.cpu() for each in model.init_hidden(len(x))])\n",
    "    with torch.no_grad():\n",
    "        if use_static:\n",
    "            pred = predict(x, static).clone().detach()\n",
    "        else:\n",
    "            pred = predict(x).clone().detach()\n",
    "    for w in range(output_weeks):\n",
    "        dict_map[\"y_pred\"] += [float(p[w]) for p in pred]\n",
    "        dict_map[\"y_pred_rounded\"] += [int(p.round()[w]) for p in pred]\n",
    "        dict_map[\"fips\"] += [f[1][0] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"date\"] += [f[1][1] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"y_true\"] += [float(item[w]) for item in y]\n",
    "        dict_map[\"week\"] += [w] * len(x)\n",
    "    i += len(x)\n",
    "df = pd.DataFrame(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in range(6):\n",
    "    wdf = df[df['week']==w]\n",
    "    mae = mean_absolute_error(wdf['y_true'], wdf['y_pred']).round(3)\n",
    "    f1 = f1_score(wdf['y_true'].round(),wdf['y_pred'].round(), average='macro').round(3)\n",
    "    print(f\"Week {w+1}\", f\"MAE {mae}\", f\"F1 {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
