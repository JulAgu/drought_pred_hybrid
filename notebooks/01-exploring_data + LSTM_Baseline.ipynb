{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First contact with the dataset\n",
    "This Notebook has as objective to replicate the baseline results from Minixhofer et al. (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import f1_score, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** Just to have a normalized methodology to save and visualize the results of all the experiments trought this projet I add and configure a Tensorboard-SummaryWriter. In the same way I have changed the training cycle to put the results in the tensorboard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('../logs/LSTM_Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data in a unique dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soil_data.csv', 'train_timeseries', 'test_timeseries', 'validation_timeseries']\n"
     ]
    }
   ],
   "source": [
    "filesList = os.listdir('../src')\n",
    "print(filesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDic = {\"train\": pd.read_csv(\"../src/train_timeseries/train_timeseries.csv\"),\n",
    "           \"test\": pd.read_csv(\"../src/test_timeseries/test_timeseries.csv\"),\n",
    "           \"validation\": pd.read_csv(\"../src/validation_timeseries/validation_timeseries.csv\"),\n",
    "           \"soil\" : pd.read_csv(\"../src/soil_data.csv\"),\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fips', 'date', 'PRECTOT', 'PS', 'QV2M', 'T2M', 'T2MDEW', 'T2MWET',\n",
       "       'T2M_MAX', 'T2M_MIN', 'T2M_RANGE', 'TS', 'WS10M', 'WS10M_MAX',\n",
       "       'WS10M_MIN', 'WS10M_RANGE', 'WS50M', 'WS50M_MAX', 'WS50M_MIN',\n",
       "       'WS50M_RANGE', 'score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataDic[\"train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class2id = {\n",
    "    'None': 0,\n",
    "    'D0': 1,\n",
    "    'D1': 2,\n",
    "    'D2': 3,\n",
    "    'D3': 4,\n",
    "    'D4': 5,\n",
    "}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    k: dataDic[k].set_index(['fips', 'date'])\n",
    "    for k in dataDic.keys() if k != \"soil\"\n",
    "}\n",
    "\n",
    "dfs[\"soil\"] = dataDic[\"soil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PRECTOT</th>\n",
       "      <th>PS</th>\n",
       "      <th>QV2M</th>\n",
       "      <th>T2M</th>\n",
       "      <th>T2MDEW</th>\n",
       "      <th>T2MWET</th>\n",
       "      <th>T2M_MAX</th>\n",
       "      <th>T2M_MIN</th>\n",
       "      <th>T2M_RANGE</th>\n",
       "      <th>TS</th>\n",
       "      <th>WS10M</th>\n",
       "      <th>WS10M_MAX</th>\n",
       "      <th>WS10M_MIN</th>\n",
       "      <th>WS10M_RANGE</th>\n",
       "      <th>WS50M</th>\n",
       "      <th>WS50M_MAX</th>\n",
       "      <th>WS50M_MIN</th>\n",
       "      <th>WS50M_RANGE</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fips</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1001</th>\n",
       "      <th>2000-01-01</th>\n",
       "      <td>0.22</td>\n",
       "      <td>100.51</td>\n",
       "      <td>9.65</td>\n",
       "      <td>14.74</td>\n",
       "      <td>13.51</td>\n",
       "      <td>13.51</td>\n",
       "      <td>20.96</td>\n",
       "      <td>11.46</td>\n",
       "      <td>9.50</td>\n",
       "      <td>14.65</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.94</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.46</td>\n",
       "      <td>4.85</td>\n",
       "      <td>6.04</td>\n",
       "      <td>3.23</td>\n",
       "      <td>2.81</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-02</th>\n",
       "      <td>0.20</td>\n",
       "      <td>100.55</td>\n",
       "      <td>10.42</td>\n",
       "      <td>16.69</td>\n",
       "      <td>14.71</td>\n",
       "      <td>14.71</td>\n",
       "      <td>22.80</td>\n",
       "      <td>12.61</td>\n",
       "      <td>10.18</td>\n",
       "      <td>16.60</td>\n",
       "      <td>2.52</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.60</td>\n",
       "      <td>5.33</td>\n",
       "      <td>6.13</td>\n",
       "      <td>3.72</td>\n",
       "      <td>2.41</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>3.65</td>\n",
       "      <td>100.15</td>\n",
       "      <td>11.76</td>\n",
       "      <td>18.49</td>\n",
       "      <td>16.52</td>\n",
       "      <td>16.52</td>\n",
       "      <td>22.73</td>\n",
       "      <td>15.32</td>\n",
       "      <td>7.41</td>\n",
       "      <td>18.41</td>\n",
       "      <td>4.03</td>\n",
       "      <td>5.33</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.67</td>\n",
       "      <td>7.53</td>\n",
       "      <td>9.52</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.66</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>15.95</td>\n",
       "      <td>100.29</td>\n",
       "      <td>6.42</td>\n",
       "      <td>11.40</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.10</td>\n",
       "      <td>18.09</td>\n",
       "      <td>2.16</td>\n",
       "      <td>15.92</td>\n",
       "      <td>11.31</td>\n",
       "      <td>3.84</td>\n",
       "      <td>5.67</td>\n",
       "      <td>2.08</td>\n",
       "      <td>3.59</td>\n",
       "      <td>6.73</td>\n",
       "      <td>9.31</td>\n",
       "      <td>3.74</td>\n",
       "      <td>5.58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>0.00</td>\n",
       "      <td>101.15</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>10.82</td>\n",
       "      <td>-2.66</td>\n",
       "      <td>13.48</td>\n",
       "      <td>2.65</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.94</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.65</td>\n",
       "      <td>4.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">56043</th>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>0.16</td>\n",
       "      <td>82.88</td>\n",
       "      <td>1.63</td>\n",
       "      <td>-7.97</td>\n",
       "      <td>-13.49</td>\n",
       "      <td>-12.81</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-13.60</td>\n",
       "      <td>12.21</td>\n",
       "      <td>-9.41</td>\n",
       "      <td>5.90</td>\n",
       "      <td>7.63</td>\n",
       "      <td>3.61</td>\n",
       "      <td>4.02</td>\n",
       "      <td>8.58</td>\n",
       "      <td>10.39</td>\n",
       "      <td>5.92</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>0.02</td>\n",
       "      <td>83.33</td>\n",
       "      <td>1.41</td>\n",
       "      <td>-8.71</td>\n",
       "      <td>-14.10</td>\n",
       "      <td>-13.84</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>-13.56</td>\n",
       "      <td>11.07</td>\n",
       "      <td>-10.55</td>\n",
       "      <td>6.50</td>\n",
       "      <td>11.43</td>\n",
       "      <td>4.11</td>\n",
       "      <td>7.32</td>\n",
       "      <td>9.92</td>\n",
       "      <td>14.49</td>\n",
       "      <td>7.26</td>\n",
       "      <td>7.22</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>0.00</td>\n",
       "      <td>83.75</td>\n",
       "      <td>1.59</td>\n",
       "      <td>-7.96</td>\n",
       "      <td>-13.30</td>\n",
       "      <td>-13.03</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-14.51</td>\n",
       "      <td>14.93</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>4.29</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2.03</td>\n",
       "      <td>4.22</td>\n",
       "      <td>6.56</td>\n",
       "      <td>10.07</td>\n",
       "      <td>3.20</td>\n",
       "      <td>6.87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>1.22</td>\n",
       "      <td>82.49</td>\n",
       "      <td>2.63</td>\n",
       "      <td>-2.94</td>\n",
       "      <td>-7.40</td>\n",
       "      <td>-7.33</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-6.86</td>\n",
       "      <td>10.62</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>4.98</td>\n",
       "      <td>7.34</td>\n",
       "      <td>1.99</td>\n",
       "      <td>5.35</td>\n",
       "      <td>7.28</td>\n",
       "      <td>10.12</td>\n",
       "      <td>3.24</td>\n",
       "      <td>6.89</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>0.44</td>\n",
       "      <td>82.19</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-7.56</td>\n",
       "      <td>-11.98</td>\n",
       "      <td>-11.82</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-11.61</td>\n",
       "      <td>10.66</td>\n",
       "      <td>-10.17</td>\n",
       "      <td>2.31</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.06</td>\n",
       "      <td>3.37</td>\n",
       "      <td>5.26</td>\n",
       "      <td>0.66</td>\n",
       "      <td>4.60</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19300680 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  PRECTOT      PS   QV2M    T2M  T2MDEW  T2MWET  T2M_MAX  \\\n",
       "fips  date                                                                 \n",
       "1001  2000-01-01     0.22  100.51   9.65  14.74   13.51   13.51    20.96   \n",
       "      2000-01-02     0.20  100.55  10.42  16.69   14.71   14.71    22.80   \n",
       "      2000-01-03     3.65  100.15  11.76  18.49   16.52   16.52    22.73   \n",
       "      2000-01-04    15.95  100.29   6.42  11.40    6.09    6.10    18.09   \n",
       "      2000-01-05     0.00  101.15   2.95   3.86   -3.29   -3.20    10.82   \n",
       "...                   ...     ...    ...    ...     ...     ...      ...   \n",
       "56043 2016-12-27     0.16   82.88   1.63  -7.97  -13.49  -12.81    -1.39   \n",
       "      2016-12-28     0.02   83.33   1.41  -8.71  -14.10  -13.84    -2.49   \n",
       "      2016-12-29     0.00   83.75   1.59  -7.96  -13.30  -13.03     0.42   \n",
       "      2016-12-30     1.22   82.49   2.63  -2.94   -7.40   -7.33     3.76   \n",
       "      2016-12-31     0.44   82.19   1.75  -7.56  -11.98  -11.82    -0.95   \n",
       "\n",
       "                  T2M_MIN  T2M_RANGE     TS  WS10M  WS10M_MAX  WS10M_MIN  \\\n",
       "fips  date                                                                 \n",
       "1001  2000-01-01    11.46       9.50  14.65   2.20       2.94       1.49   \n",
       "      2000-01-02    12.61      10.18  16.60   2.52       3.43       1.83   \n",
       "      2000-01-03    15.32       7.41  18.41   4.03       5.33       2.66   \n",
       "      2000-01-04     2.16      15.92  11.31   3.84       5.67       2.08   \n",
       "      2000-01-05    -2.66      13.48   2.65   1.60       2.50       0.52   \n",
       "...                   ...        ...    ...    ...        ...        ...   \n",
       "56043 2016-12-27   -13.60      12.21  -9.41   5.90       7.63       3.61   \n",
       "      2016-12-28   -13.56      11.07 -10.55   6.50      11.43       4.11   \n",
       "      2016-12-29   -14.51      14.93 -10.29   4.29       6.24       2.03   \n",
       "      2016-12-30    -6.86      10.62  -4.14   4.98       7.34       1.99   \n",
       "      2016-12-31   -11.61      10.66 -10.17   2.31       3.47       0.41   \n",
       "\n",
       "                  WS10M_RANGE  WS50M  WS50M_MAX  WS50M_MIN  WS50M_RANGE  score  \n",
       "fips  date                                                                      \n",
       "1001  2000-01-01         1.46   4.85       6.04       3.23         2.81    NaN  \n",
       "      2000-01-02         1.60   5.33       6.13       3.72         2.41    NaN  \n",
       "      2000-01-03         2.67   7.53       9.52       5.87         3.66    NaN  \n",
       "      2000-01-04         3.59   6.73       9.31       3.74         5.58    1.0  \n",
       "      2000-01-05         1.98   2.94       4.85       0.65         4.19    NaN  \n",
       "...                       ...    ...        ...        ...          ...    ...  \n",
       "56043 2016-12-27         4.02   8.58      10.39       5.92         4.47    0.0  \n",
       "      2016-12-28         7.32   9.92      14.49       7.26         7.22    NaN  \n",
       "      2016-12-29         4.22   6.56      10.07       3.20         6.87    NaN  \n",
       "      2016-12-30         5.35   7.28      10.12       3.24         6.89    NaN  \n",
       "      2016-12-31         3.06   3.37       5.26       0.66         4.60    NaN  \n",
       "\n",
       "[19300680 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation pour les données manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_nans(padata, pkind='linear'):\n",
    "    \"\"\"\n",
    "    see: https://stackoverflow.com/a/53050216/2167159\n",
    "    \"\"\"\n",
    "    aindexes = np.arange(padata.shape[0])\n",
    "    agood_indexes, = np.where(np.isfinite(padata))\n",
    "    f = interp1d(agood_indexes\n",
    "               , padata[agood_indexes]\n",
    "               , bounds_error=False\n",
    "               , copy=False\n",
    "               , fill_value=\"extrapolate\"\n",
    "               , kind=pkind)\n",
    "    return f(aindexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to encode the cycling feature: year-day, using sin/cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_encode(date):\n",
    "    if isinstance(date, str):\n",
    "        date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (\n",
    "        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadXY(\n",
    "    df,\n",
    "    random_state=42,\n",
    "    window_size=180, # how many days in the past (default/competition: 180)\n",
    "    target_size=6, # how many weeks into the future (default/competition: 6)\n",
    "    fuse_past=True, # add the past drought observations? (default: True)\n",
    "    return_fips=False, # return the county identifier (do not use for predictions)\n",
    "    encode_season=True, # encode the season using the function above (default: True) \n",
    "    use_prev_year=False, # add observations from 1 year prior?\n",
    "):\n",
    "    df = dfs[df]\n",
    "    soil_df = dfs[\"soil\"]\n",
    "    time_data_cols = sorted(\n",
    "        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n",
    "    )\n",
    "    static_data_cols = sorted(\n",
    "        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n",
    "    )\n",
    "    count = 0\n",
    "    score_df = df.dropna(subset=[\"score\"])\n",
    "    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n",
    "    X_fips_date = []\n",
    "    add_dim = 0\n",
    "    if use_prev_year:\n",
    "        add_dim += len(time_data_cols)\n",
    "    if fuse_past:\n",
    "        add_dim += 1\n",
    "        if use_prev_year:\n",
    "            add_dim += 1\n",
    "    if encode_season:\n",
    "        add_dim += 2\n",
    "    X_time = np.empty(\n",
    "        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n",
    "    )\n",
    "    y_past = np.empty((len(df) // window_size, window_size))\n",
    "    y_target = np.empty((len(df) // window_size, target_size))\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n",
    "        if random_state is not None:\n",
    "            start_i = np.random.randint(1, window_size)\n",
    "        else:\n",
    "            start_i = 1\n",
    "        fips_df = df[(df.index.get_level_values(0) == fips)]\n",
    "        X = fips_df[time_data_cols].values\n",
    "        y = fips_df[\"score\"].values\n",
    "        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n",
    "        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n",
    "            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n",
    "            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n",
    "            if use_prev_year:\n",
    "                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n",
    "                    continue\n",
    "                X_time[count, :, -len(time_data_cols) :] = X[\n",
    "                    i - 365 : i + window_size - 365\n",
    "                ]\n",
    "            if not fuse_past:\n",
    "                y_past[count] = interpolate_nans(y[i : i + window_size])\n",
    "            else:\n",
    "                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n",
    "                    y[i : i + window_size]\n",
    "                )\n",
    "            if encode_season:\n",
    "                enc_dates = [\n",
    "                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n",
    "                ]\n",
    "                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n",
    "                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n",
    "            temp_y = y[i + window_size : i + window_size + target_size * 7]\n",
    "            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n",
    "            X_static[count] = X_s\n",
    "            count += 1\n",
    "    print(f\"loaded {count} samples\")\n",
    "    results = [X_static[:count], X_time[:count], y_target[:count]]\n",
    "    if not fuse_past:\n",
    "        results.append(y_past[:count])\n",
    "    if return_fips:\n",
    "        results.append(X_fips_date)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "scaler_dict_static = {}\n",
    "scaler_dict_past = {}\n",
    "\n",
    "\n",
    "def normalize(X_static, X_time, y_past=None, fit=False):\n",
    "    for index in tqdm(range(X_time.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n",
    "        X_time[:, :, index] = (\n",
    "            scaler_dict[index]\n",
    "            .transform(X_time[:, :, index].reshape(-1, 1))\n",
    "            .reshape(-1, X_time.shape[-2])\n",
    "        )\n",
    "    for index in tqdm(range(X_static.shape[-1])):\n",
    "        if fit:\n",
    "            scaler_dict_static[index] = RobustScaler().fit(\n",
    "                X_static[:, index].reshape(-1, 1)\n",
    "            )\n",
    "        X_static[:, index] = (\n",
    "            scaler_dict_static[index]\n",
    "            .transform(X_static[:, index].reshape(-1, 1))\n",
    "            .reshape(1, -1)\n",
    "        )\n",
    "    index = 0\n",
    "    if y_past is not None:\n",
    "        if fit:\n",
    "            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n",
    "        y_past[:, :] = (\n",
    "            scaler_dict_past[index]\n",
    "            .transform(y_past.reshape(-1, 1))\n",
    "            .reshape(-1, y_past.shape[-1])\n",
    "        )\n",
    "        return X_static, X_time, y_past\n",
    "    return X_static, X_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3108/3108 [10:32<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 103390 samples\n",
      "train shape (103390, 180, 21)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2261/3108 [00:35<00:13, 64.56it/s]"
     ]
    }
   ],
   "source": [
    "X_tabular_train, X_time_train, y_target_train = loadXY(\"train\")\n",
    "print(\"train shape\", X_time_train.shape)\n",
    "X_tabular_validation, X_time_valid, y_target_valid, valid_fips = loadXY(\"validation\", return_fips=True)\n",
    "print(\"validation shape\", X_time_valid.shape)\n",
    "X_tabular_train, X_time_train = normalize(X_tabular_train, X_time_train, fit=True)\n",
    "X_tabular_validation, X_time_valid = normalize(X_tabular_validation, X_time_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "output_weeks = 6\n",
    "use_static = True\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "ffnn_layers = 2\n",
    "dropout = 0.1\n",
    "one_cycle = True\n",
    "lr = 7e-5\n",
    "epochs = 10\n",
    "clip = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_time_train),\n",
    "    torch.tensor(X_tabular_train),\n",
    "    torch.tensor(y_target_train[:, :output_weeks]),\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.tensor(X_time_valid),\n",
    "    torch.tensor(X_tabular_validation),\n",
    "    torch.tensor(y_target_valid[:, :output_weeks]),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtNetLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size,\n",
    "        num_input_features,\n",
    "        hidden_dim,\n",
    "        n_layers,\n",
    "        ffnn_layers,\n",
    "        drop_prob,\n",
    "        static_dim=0,\n",
    "    ):\n",
    "        super(DroughtNetLSTM, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            num_input_features,\n",
    "            hidden_dim,\n",
    "            n_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fflayers = []\n",
    "        for i in range(ffnn_layers - 1):\n",
    "            if i == 0:\n",
    "                self.fflayers.append(nn.Linear(hidden_dim + static_dim, hidden_dim))\n",
    "            else:\n",
    "                self.fflayers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        self.fflayers = nn.ModuleList(self.fflayers)\n",
    "        self.final = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, static=None):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.to(dtype=torch.float32)\n",
    "        if static is not None:\n",
    "            static = static.to(dtype=torch.float32)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        for i in range(len(self.fflayers)):\n",
    "            if i == 0 and static is not None:\n",
    "                out = self.fflayers[i](torch.cat((out, static), 1))\n",
    "            else:\n",
    "                out = self.fflayers[i](out)\n",
    "        out = self.final(out)\n",
    "\n",
    "        out = out.view(batch_size, -1)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10:  50%|█████     | 404/808 [02:24<22:52,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.40540656447410583, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 1, 'validation_loss': np.float64(0.417090016754641), 'macro_f1': np.float64(0.4940091142245857), 'micro_f1': np.float64(0.7078189300411523), 'mae': np.float64(0.4269706536039974)}\n",
      "{'loss': 0.40540656447410583, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 2, 'validation_loss': np.float64(0.417090016754641), 'macro_f1': np.float64(0.43930610203028847), 'micro_f1': np.float64(0.6696387745770462), 'mae': np.float64(0.44217968909021965)}\n",
      "{'loss': 0.40540656447410583, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 3, 'validation_loss': np.float64(0.417090016754641), 'macro_f1': np.float64(0.39762044439712385), 'micro_f1': np.float64(0.6274577046181985), 'mae': np.float64(0.5103915951933662)}\n",
      "{'loss': 0.40540656447410583, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 4, 'validation_loss': np.float64(0.417090016754641), 'macro_f1': np.float64(0.3536939169764038), 'micro_f1': np.float64(0.5768175582990398), 'mae': np.float64(0.5510240837712312)}\n",
      "{'loss': 0.40540656447410583, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 5, 'validation_loss': np.float64(0.417090016754641), 'macro_f1': np.float64(0.3565602790534191), 'micro_f1': np.float64(0.5837905807041609), 'mae': np.float64(0.5590147382426826)}\n",
      "{'loss': 0.40540656447410583, 'epoch': 0.5, 'step': 404, 'lr': 7.305177512317032e-06, 'week': 6, 'validation_loss': np.float64(0.417090016754641), 'macro_f1': np.float64(0.33083500024621676), 'micro_f1': np.float64(0.586076817558299), 'mae': np.float64(0.5781802531620559)}\n",
      "Validation loss decreased (inf --> 0.417090).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1/10: 100%|██████████| 808/808 [04:52<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.22620344161987305, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 1, 'validation_loss': np.float64(0.2483354107193325), 'macro_f1': np.float64(0.7564181479492601), 'micro_f1': np.float64(0.8683127572016461), 'mae': np.float64(0.22443508443987303)}\n",
      "{'loss': 0.22620344161987305, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 2, 'validation_loss': np.float64(0.2483354107193325), 'macro_f1': np.float64(0.6855000665796022), 'micro_f1': np.float64(0.8176726108824874), 'mae': np.float64(0.26823645023587495)}\n",
      "{'loss': 0.22620344161987305, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 3, 'validation_loss': np.float64(0.2483354107193325), 'macro_f1': np.float64(0.6347174043509513), 'micro_f1': np.float64(0.7790352080475538), 'mae': np.float64(0.3154690905890926)}\n",
      "{'loss': 0.22620344161987305, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 4, 'validation_loss': np.float64(0.2483354107193325), 'macro_f1': np.float64(0.5670381911481271), 'micro_f1': np.float64(0.7421124828532236), 'mae': np.float64(0.3653018911887142)}\n",
      "{'loss': 0.22620344161987305, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 5, 'validation_loss': np.float64(0.2483354107193325), 'macro_f1': np.float64(0.5027521972341027), 'micro_f1': np.float64(0.7088477366255144), 'mae': np.float64(0.407266061297188)}\n",
      "{'loss': 0.22620344161987305, 'epoch': 1.0, 'step': 808, 'lr': 1.9612577643465342e-05, 'week': 6, 'validation_loss': np.float64(0.2483354107193325), 'macro_f1': np.float64(0.42787377568543156), 'micro_f1': np.float64(0.6818701417466849), 'mae': np.float64(0.4425202120875235)}\n",
      "Validation loss decreased (0.417090 --> 0.248335).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10:  50%|█████     | 404/808 [02:27<23:05,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3091033399105072, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 1, 'validation_loss': np.float64(0.23797678591116614), 'macro_f1': np.float64(0.7745300266900594), 'micro_f1': np.float64(0.880315500685871), 'mae': np.float64(0.19007316553764364)}\n",
      "{'loss': 0.3091033399105072, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 2, 'validation_loss': np.float64(0.23797678591116614), 'macro_f1': np.float64(0.7058232436011548), 'micro_f1': np.float64(0.8279606767261088), 'mae': np.float64(0.24571470459828948)}\n",
      "{'loss': 0.3091033399105072, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 3, 'validation_loss': np.float64(0.23797678591116614), 'macro_f1': np.float64(0.649100043056262), 'micro_f1': np.float64(0.7848651120256058), 'mae': np.float64(0.29648405554843155)}\n",
      "{'loss': 0.3091033399105072, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 4, 'validation_loss': np.float64(0.23797678591116614), 'macro_f1': np.float64(0.5842172381818322), 'micro_f1': np.float64(0.7466849565614998), 'mae': np.float64(0.3442971964293888)}\n",
      "{'loss': 0.3091033399105072, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 5, 'validation_loss': np.float64(0.23797678591116614), 'macro_f1': np.float64(0.5378363967549569), 'micro_f1': np.float64(0.7166209419295839), 'mae': np.float64(0.3920822722889564)}\n",
      "{'loss': 0.3091033399105072, 'epoch': 1.5, 'step': 1212, 'lr': 3.6421782399043904e-05, 'week': 6, 'validation_loss': np.float64(0.23797678591116614), 'macro_f1': np.float64(0.49920475727478436), 'micro_f1': np.float64(0.6778692272519433), 'mae': np.float64(0.43711983309695196)}\n",
      "Validation loss decreased (0.248335 --> 0.237977).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2/10: 100%|██████████| 808/808 [04:54<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2376997321844101, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 1, 'validation_loss': np.float64(0.2270330902243006), 'macro_f1': np.float64(0.6335386981742628), 'micro_f1': np.float64(0.8756287151348879), 'mae': np.float64(0.17309437069325576)}\n",
      "{'loss': 0.2376997321844101, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 2, 'validation_loss': np.float64(0.2270330902243006), 'macro_f1': np.float64(0.5577408352846581), 'micro_f1': np.float64(0.823045267489712), 'mae': np.float64(0.23149042904565956)}\n",
      "{'loss': 0.2376997321844101, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 3, 'validation_loss': np.float64(0.2270330902243006), 'macro_f1': np.float64(0.4570009401387251), 'micro_f1': np.float64(0.7765203475080018), 'mae': np.float64(0.2888461066796681)}\n",
      "{'loss': 0.2376997321844101, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 4, 'validation_loss': np.float64(0.2270330902243006), 'macro_f1': np.float64(0.41281365357888417), 'micro_f1': np.float64(0.7407407407407407), 'mae': np.float64(0.3395665675222237)}\n",
      "{'loss': 0.2376997321844101, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 5, 'validation_loss': np.float64(0.2270330902243006), 'macro_f1': np.float64(0.37372705143672696), 'micro_f1': np.float64(0.7115912208504801), 'mae': np.float64(0.3772433755664588)}\n",
      "{'loss': 0.2376997321844101, 'epoch': 2.0, 'step': 1616, 'lr': 5.322514587043574e-05, 'week': 6, 'validation_loss': np.float64(0.2270330902243006), 'macro_f1': np.float64(0.3420750305219771), 'micro_f1': np.float64(0.6826703246456333), 'mae': np.float64(0.4151953195038185)}\n",
      "Validation loss decreased (0.237977 --> 0.227033).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10:  50%|█████     | 404/808 [02:26<23:01,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.25073882937431335, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 1, 'validation_loss': np.float64(0.22737590412514797), 'macro_f1': np.float64(0.7078199458082969), 'micro_f1': np.float64(0.883973479652492), 'mae': np.float64(0.17051862244237584)}\n",
      "{'loss': 0.25073882937431335, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 2, 'validation_loss': np.float64(0.22737590412514797), 'macro_f1': np.float64(0.6423269350899753), 'micro_f1': np.float64(0.8273891175125743), 'mae': np.float64(0.23382204041227642)}\n",
      "{'loss': 0.25073882937431335, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 3, 'validation_loss': np.float64(0.22737590412514797), 'macro_f1': np.float64(0.5872817909270741), 'micro_f1': np.float64(0.7824645633287609), 'mae': np.float64(0.2929405957964503)}\n",
      "{'loss': 0.25073882937431335, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 4, 'validation_loss': np.float64(0.22737590412514797), 'macro_f1': np.float64(0.5558117407745479), 'micro_f1': np.float64(0.7430269775948788), 'mae': np.float64(0.33910340536293215)}\n",
      "{'loss': 0.25073882937431335, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 5, 'validation_loss': np.float64(0.22737590412514797), 'macro_f1': np.float64(0.48273215782035966), 'micro_f1': np.float64(0.711705532693187), 'mae': np.float64(0.38249658498079553)}\n",
      "{'loss': 0.25073882937431335, 'epoch': 2.5, 'step': 2020, 'lr': 6.551658857891442e-05, 'week': 6, 'validation_loss': np.float64(0.22737590412514797), 'macro_f1': np.float64(0.4176943871035268), 'micro_f1': np.float64(0.6786694101508917), 'mae': np.float64(0.4240983392560094)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2585916221141815, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 1, 'validation_loss': np.float64(0.2271125737739646), 'macro_f1': np.float64(0.7757741172943581), 'micro_f1': np.float64(0.8808870598994056), 'mae': np.float64(0.17477316567382534)}\n",
      "{'loss': 0.2585916221141815, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 2, 'validation_loss': np.float64(0.2271125737739646), 'macro_f1': np.float64(0.7116820630992358), 'micro_f1': np.float64(0.8291037951531779), 'mae': np.float64(0.23097523363249625)}\n",
      "{'loss': 0.2585916221141815, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 3, 'validation_loss': np.float64(0.2271125737739646), 'macro_f1': np.float64(0.6688056528716951), 'micro_f1': np.float64(0.7865797896662095), 'mae': np.float64(0.2858168105063925)}\n",
      "{'loss': 0.2585916221141815, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 4, 'validation_loss': np.float64(0.2271125737739646), 'macro_f1': np.float64(0.6008418877593962), 'micro_f1': np.float64(0.7501143118427069), 'mae': np.float64(0.327924747245508)}\n",
      "{'loss': 0.2585916221141815, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 5, 'validation_loss': np.float64(0.2271125737739646), 'macro_f1': np.float64(0.5463867934748045), 'micro_f1': np.float64(0.7211934156378601), 'mae': np.float64(0.37899566322949124)}\n",
      "{'loss': 0.2585916221141815, 'epoch': 3.0, 'step': 2424, 'lr': 6.99999946009513e-05, 'week': 6, 'validation_loss': np.float64(0.2271125737739646), 'macro_f1': np.float64(0.5105921591625193), 'micro_f1': np.float64(0.695016003657979), 'mae': np.float64(0.4146449944425214)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10:  50%|█████     | 404/808 [02:26<22:59,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26915982365608215, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 1, 'validation_loss': np.float64(0.2121999178999576), 'macro_f1': np.float64(0.753308167107217), 'micro_f1': np.float64(0.8910608139003201), 'mae': np.float64(0.15324005312755828)}\n",
      "{'loss': 0.26915982365608215, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 2, 'validation_loss': np.float64(0.2121999178999576), 'macro_f1': np.float64(0.675353898522952), 'micro_f1': np.float64(0.8345907636031092), 'mae': np.float64(0.21276099123102263)}\n",
      "{'loss': 0.26915982365608215, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 3, 'validation_loss': np.float64(0.2121999178999576), 'macro_f1': np.float64(0.6318832386969778), 'micro_f1': np.float64(0.7927526291723822), 'mae': np.float64(0.26472011925474775)}\n",
      "{'loss': 0.26915982365608215, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 4, 'validation_loss': np.float64(0.2121999178999576), 'macro_f1': np.float64(0.5812828705061784), 'micro_f1': np.float64(0.7540009144947416), 'mae': np.float64(0.3167608060436607)}\n",
      "{'loss': 0.26915982365608215, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 5, 'validation_loss': np.float64(0.2121999178999576), 'macro_f1': np.float64(0.5306870273698449), 'micro_f1': np.float64(0.7231367169638775), 'mae': np.float64(0.3666406290141129)}\n",
      "{'loss': 0.26915982365608215, 'epoch': 3.5, 'step': 2828, 'lr': 6.911814926126814e-05, 'week': 6, 'validation_loss': np.float64(0.2121999178999576), 'macro_f1': np.float64(0.4948374312853527), 'micro_f1': np.float64(0.6920438957475995), 'mae': np.float64(0.40960364515934555)}\n",
      "Validation loss decreased (0.227033 --> 0.212200).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17046396434307098, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 1, 'validation_loss': np.float64(0.20936721471556718), 'macro_f1': np.float64(0.8192831504050435), 'micro_f1': np.float64(0.8975765889346137), 'mae': np.float64(0.1570147752547805)}\n",
      "{'loss': 0.17046396434307098, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 2, 'validation_loss': np.float64(0.20936721471556718), 'macro_f1': np.float64(0.7117567426174917), 'micro_f1': np.float64(0.8399634202103338), 'mae': np.float64(0.21640699181080214)}\n",
      "{'loss': 0.17046396434307098, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 3, 'validation_loss': np.float64(0.20936721471556718), 'macro_f1': np.float64(0.617779831763933), 'micro_f1': np.float64(0.7941243712848651), 'mae': np.float64(0.26445058561223667)}\n",
      "{'loss': 0.17046396434307098, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 4, 'validation_loss': np.float64(0.20936721471556718), 'macro_f1': np.float64(0.5333924998432465), 'micro_f1': np.float64(0.7535436671239141), 'mae': np.float64(0.312538976935999)}\n",
      "{'loss': 0.17046396434307098, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 5, 'validation_loss': np.float64(0.20936721471556718), 'macro_f1': np.float64(0.4431605817735494), 'micro_f1': np.float64(0.7210791037951532), 'mae': np.float64(0.3574048774147003)}\n",
      "{'loss': 0.17046396434307098, 'epoch': 4.0, 'step': 3232, 'lr': 6.652548447282524e-05, 'week': 6, 'validation_loss': np.float64(0.20936721471556718), 'macro_f1': np.float64(0.40713329656469804), 'micro_f1': np.float64(0.6910150891632373), 'mae': np.float64(0.39837257518831304)}\n",
      "Validation loss decreased (0.212200 --> 0.209367).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/10:  50%|█████     | 404/808 [02:27<23:01,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.28071296215057373, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 1, 'validation_loss': np.float64(0.20913583882476972), 'macro_f1': np.float64(0.8135486795074095), 'micro_f1': np.float64(0.897119341563786), 'mae': np.float64(0.1343047316907915)}\n",
      "{'loss': 0.28071296215057373, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 2, 'validation_loss': np.float64(0.20913583882476972), 'macro_f1': np.float64(0.7362881763717551), 'micro_f1': np.float64(0.8399634202103338), 'mae': np.float64(0.19634164324250616)}\n",
      "{'loss': 0.28071296215057373, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 3, 'validation_loss': np.float64(0.20913583882476972), 'macro_f1': np.float64(0.6673860132629591), 'micro_f1': np.float64(0.7982395976223137), 'mae': np.float64(0.2514857023306337)}\n",
      "{'loss': 0.28071296215057373, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 4, 'validation_loss': np.float64(0.20913583882476972), 'macro_f1': np.float64(0.5807123292199208), 'micro_f1': np.float64(0.7577732053040696), 'mae': np.float64(0.3024457781298491)}\n",
      "{'loss': 0.28071296215057373, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 5, 'validation_loss': np.float64(0.20913583882476972), 'macro_f1': np.float64(0.534941003252017), 'micro_f1': np.float64(0.727251943301326), 'mae': np.float64(0.35150987158751734)}\n",
      "{'loss': 0.28071296215057373, 'epoch': 4.5, 'step': 3636, 'lr': 6.235200727414045e-05, 'week': 6, 'validation_loss': np.float64(0.20913583882476972), 'macro_f1': np.float64(0.4986511369364566), 'micro_f1': np.float64(0.6989026063100137), 'mae': np.float64(0.3915620260040097)}\n",
      "Validation loss decreased (0.209367 --> 0.209136).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3733613193035126, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 1, 'validation_loss': np.float64(0.21664870940688727), 'macro_f1': np.float64(0.8168460573717534), 'micro_f1': np.float64(0.9003200731595793), 'mae': np.float64(0.13867737110831477)}\n",
      "{'loss': 0.3733613193035126, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 2, 'validation_loss': np.float64(0.21664870940688727), 'macro_f1': np.float64(0.720735042682071), 'micro_f1': np.float64(0.8393918609967993), 'mae': np.float64(0.20460070804762517)}\n",
      "{'loss': 0.3733613193035126, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 3, 'validation_loss': np.float64(0.21664870940688727), 'macro_f1': np.float64(0.659146491980208), 'micro_f1': np.float64(0.7956104252400549), 'mae': np.float64(0.2636768748557184)}\n",
      "{'loss': 0.3733613193035126, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 4, 'validation_loss': np.float64(0.21664870940688727), 'macro_f1': np.float64(0.5892991033723535), 'micro_f1': np.float64(0.7546867855509831), 'mae': np.float64(0.3163243078269773)}\n",
      "{'loss': 0.3733613193035126, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 5, 'validation_loss': np.float64(0.21664870940688727), 'macro_f1': np.float64(0.5467901164197181), 'micro_f1': np.float64(0.7245084590763603), 'mae': np.float64(0.36462259655656365)}\n",
      "{'loss': 0.3733613193035126, 'epoch': 5.0, 'step': 4040, 'lr': 5.680699323887897e-05, 'week': 6, 'validation_loss': np.float64(0.21664870940688727), 'macro_f1': np.float64(0.5116735134517661), 'micro_f1': np.float64(0.6959304983996342), 'mae': np.float64(0.40695117305842116)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/10:  50%|█████     | 404/808 [02:26<23:06,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.22723953425884247, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 1, 'validation_loss': np.float64(0.21212584395771442), 'macro_f1': np.float64(0.8209283580973689), 'micro_f1': np.float64(0.9005486968449932), 'mae': np.float64(0.13764022572509405)}\n",
      "{'loss': 0.22723953425884247, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 2, 'validation_loss': np.float64(0.21212584395771442), 'macro_f1': np.float64(0.7327254653421839), 'micro_f1': np.float64(0.8401920438957476), 'mae': np.float64(0.20198607876860714)}\n",
      "{'loss': 0.22723953425884247, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 3, 'validation_loss': np.float64(0.21212584395771442), 'macro_f1': np.float64(0.6698703307900332), 'micro_f1': np.float64(0.7960676726108825), 'mae': np.float64(0.2594293405944281)}\n",
      "{'loss': 0.22723953425884247, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 4, 'validation_loss': np.float64(0.21212584395771442), 'macro_f1': np.float64(0.5932044896484738), 'micro_f1': np.float64(0.7556012802926383), 'mae': np.float64(0.3171865262164078)}\n",
      "{'loss': 0.22723953425884247, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 5, 'validation_loss': np.float64(0.21212584395771442), 'macro_f1': np.float64(0.5479681246223546), 'micro_f1': np.float64(0.7265660722450846), 'mae': np.float64(0.3643134264606285)}\n",
      "{'loss': 0.22723953425884247, 'epoch': 5.5, 'step': 4444, 'lr': 5.0168492524730965e-05, 'week': 6, 'validation_loss': np.float64(0.21212584395771442), 'macro_f1': np.float64(0.4959485458222705), 'micro_f1': np.float64(0.6934156378600823), 'mae': np.float64(0.40734470459843386)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.241116002202034, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 1, 'validation_loss': np.float64(0.21238080660502115), 'macro_f1': np.float64(0.8192286794250653), 'micro_f1': np.float64(0.8956332876085963), 'mae': np.float64(0.15106211559416877)}\n",
      "{'loss': 0.241116002202034, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 2, 'validation_loss': np.float64(0.21238080660502115), 'macro_f1': np.float64(0.7218114501385862), 'micro_f1': np.float64(0.8367626886145405), 'mae': np.float64(0.21324201691264955)}\n",
      "{'loss': 0.241116002202034, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 3, 'validation_loss': np.float64(0.21238080660502115), 'macro_f1': np.float64(0.6576817035675956), 'micro_f1': np.float64(0.7940100594421582), 'mae': np.float64(0.26489463212439573)}\n",
      "{'loss': 0.241116002202034, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 4, 'validation_loss': np.float64(0.21238080660502115), 'macro_f1': np.float64(0.5947021097884476), 'micro_f1': np.float64(0.7535436671239141), 'mae': np.float64(0.31820558556524187)}\n",
      "{'loss': 0.241116002202034, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 5, 'validation_loss': np.float64(0.21238080660502115), 'macro_f1': np.float64(0.5563434446982098), 'micro_f1': np.float64(0.7239368998628258), 'mae': np.float64(0.3676023477062247)}\n",
      "{'loss': 0.241116002202034, 'epoch': 6.0, 'step': 4848, 'lr': 4.276938727746874e-05, 'week': 6, 'validation_loss': np.float64(0.21238080660502115), 'macro_f1': np.float64(0.5071947729116413), 'micro_f1': np.float64(0.6934156378600823), 'mae': np.float64(0.4083315074493401)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/10:  50%|█████     | 404/808 [02:26<22:55,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2058635652065277, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 1, 'validation_loss': np.float64(0.21062526754710986), 'macro_f1': np.float64(0.8207398132663268), 'micro_f1': np.float64(0.9012345679012346), 'mae': np.float64(0.13006116838171786)}\n",
      "{'loss': 0.2058635652065277, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 2, 'validation_loss': np.float64(0.21062526754710986), 'macro_f1': np.float64(0.7218763120041135), 'micro_f1': np.float64(0.8406492912665752), 'mae': np.float64(0.2001784140687354)}\n",
      "{'loss': 0.2058635652065277, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 3, 'validation_loss': np.float64(0.21062526754710986), 'macro_f1': np.float64(0.6602241695125078), 'micro_f1': np.float64(0.7961819844535893), 'mae': np.float64(0.2558813735814422)}\n",
      "{'loss': 0.2058635652065277, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 4, 'validation_loss': np.float64(0.21062526754710986), 'macro_f1': np.float64(0.5982957995602561), 'micro_f1': np.float64(0.7550297210791038), 'mae': np.float64(0.30868658158483947)}\n",
      "{'loss': 0.2058635652065277, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 5, 'validation_loss': np.float64(0.21062526754710986), 'macro_f1': np.float64(0.5366535126045304), 'micro_f1': np.float64(0.7238225880201189), 'mae': np.float64(0.35834223123453707)}\n",
      "{'loss': 0.2058635652065277, 'epoch': 6.5, 'step': 5252, 'lr': 3.498069953016286e-05, 'week': 6, 'validation_loss': np.float64(0.21062526754710986), 'macro_f1': np.float64(0.4783447226915034), 'micro_f1': np.float64(0.6936442615454961), 'mae': np.float64(0.39944686224653514)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 7/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.34890875220298767, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 1, 'validation_loss': np.float64(0.20927275571486223), 'macro_f1': np.float64(0.8207608602019011), 'micro_f1': np.float64(0.9015775034293553), 'mae': np.float64(0.13284475238731885)}\n",
      "{'loss': 0.34890875220298767, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 2, 'validation_loss': np.float64(0.20927275571486223), 'macro_f1': np.float64(0.7241111479564358), 'micro_f1': np.float64(0.8407636031092821), 'mae': np.float64(0.19834518557729142)}\n",
      "{'loss': 0.34890875220298767, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 3, 'validation_loss': np.float64(0.20927275571486223), 'macro_f1': np.float64(0.6578813826150239), 'micro_f1': np.float64(0.7959533607681756), 'mae': np.float64(0.25585868423998737)}\n",
      "{'loss': 0.34890875220298767, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 4, 'validation_loss': np.float64(0.20927275571486223), 'macro_f1': np.float64(0.6017202664369941), 'micro_f1': np.float64(0.7553726566072245), 'mae': np.float64(0.3096209870378111)}\n",
      "{'loss': 0.34890875220298767, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 5, 'validation_loss': np.float64(0.20927275571486223), 'macro_f1': np.float64(0.5505218203419476), 'micro_f1': np.float64(0.726108824874257), 'mae': np.float64(0.3555723639591826)}\n",
      "{'loss': 0.34890875220298767, 'epoch': 7.0, 'step': 5656, 'lr': 2.7192986609190955e-05, 'week': 6, 'validation_loss': np.float64(0.20927275571486223), 'macro_f1': np.float64(0.5115634204174742), 'micro_f1': np.float64(0.6952446273433928), 'mae': np.float64(0.39692594314371177)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8/10:  50%|█████     | 404/808 [02:26<22:58,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.22240698337554932, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 1, 'validation_loss': np.float64(0.20809780930479368), 'macro_f1': np.float64(0.8248580919619064), 'micro_f1': np.float64(0.9023776863283036), 'mae': np.float64(0.131243526444397)}\n",
      "{'loss': 0.22240698337554932, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 2, 'validation_loss': np.float64(0.20809780930479368), 'macro_f1': np.float64(0.7250273294942948), 'micro_f1': np.float64(0.8403063557384545), 'mae': np.float64(0.2060660828328308)}\n",
      "{'loss': 0.22240698337554932, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 3, 'validation_loss': np.float64(0.20809780930479368), 'macro_f1': np.float64(0.6600873721896237), 'micro_f1': np.float64(0.7951531778692272), 'mae': np.float64(0.2594929841328307)}\n",
      "{'loss': 0.22240698337554932, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 4, 'validation_loss': np.float64(0.20809780930479368), 'macro_f1': np.float64(0.600238217032014), 'micro_f1': np.float64(0.7557155921353452), 'mae': np.float64(0.310372693605485)}\n",
      "{'loss': 0.22240698337554932, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 5, 'validation_loss': np.float64(0.20809780930479368), 'macro_f1': np.float64(0.542812818681228), 'micro_f1': np.float64(0.7253086419753086), 'mae': np.float64(0.35924135128462525)}\n",
      "{'loss': 0.22240698337554932, 'epoch': 7.5, 'step': 6060, 'lr': 1.9796756959067725e-05, 'week': 6, 'validation_loss': np.float64(0.20809780930479368), 'macro_f1': np.float64(0.4864083937401414), 'micro_f1': np.float64(0.6928440786465477), 'mae': np.float64(0.39877438989823766)}\n",
      "Validation loss decreased (0.209136 --> 0.208098).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 8/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.22848336398601532, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 1, 'validation_loss': np.float64(0.20845238794235216), 'macro_f1': np.float64(0.8232155092142336), 'micro_f1': np.float64(0.901920438957476), 'mae': np.float64(0.13034316929794837)}\n",
      "{'loss': 0.22848336398601532, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 2, 'validation_loss': np.float64(0.20845238794235216), 'macro_f1': np.float64(0.7248106791282622), 'micro_f1': np.float64(0.8407636031092821), 'mae': np.float64(0.1975983227562314)}\n",
      "{'loss': 0.22848336398601532, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 3, 'validation_loss': np.float64(0.20845238794235216), 'macro_f1': np.float64(0.6581804557021772), 'micro_f1': np.float64(0.7959533607681756), 'mae': np.float64(0.25811041306974936)}\n",
      "{'loss': 0.22848336398601532, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 4, 'validation_loss': np.float64(0.20845238794235216), 'macro_f1': np.float64(0.591922623809136), 'micro_f1': np.float64(0.7560585276634659), 'mae': np.float64(0.3099453902979085)}\n",
      "{'loss': 0.22848336398601532, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 5, 'validation_loss': np.float64(0.20845238794235216), 'macro_f1': np.float64(0.5394748083776882), 'micro_f1': np.float64(0.7265660722450846), 'mae': np.float64(0.3586317639439716)}\n",
      "{'loss': 0.22848336398601532, 'epoch': 8.0, 'step': 6464, 'lr': 1.316288841841575e-05, 'week': 6, 'validation_loss': np.float64(0.20845238794235216), 'macro_f1': np.float64(0.49447463465232083), 'micro_f1': np.float64(0.6941015089163237), 'mae': np.float64(0.39986690021118887)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9/10:  50%|█████     | 404/808 [02:26<22:54,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.17318235337734222, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 1, 'validation_loss': np.float64(0.21004490144010904), 'macro_f1': np.float64(0.8173206014510397), 'micro_f1': np.float64(0.9014631915866483), 'mae': np.float64(0.13065018267006817)}\n",
      "{'loss': 0.17318235337734222, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 2, 'validation_loss': np.float64(0.21004490144010904), 'macro_f1': np.float64(0.7311173295581028), 'micro_f1': np.float64(0.8413351623228167), 'mae': np.float64(0.19994804528761276)}\n",
      "{'loss': 0.17318235337734222, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 3, 'validation_loss': np.float64(0.21004490144010904), 'macro_f1': np.float64(0.660347767872389), 'micro_f1': np.float64(0.7948102423411065), 'mae': np.float64(0.26080482703905505)}\n",
      "{'loss': 0.17318235337734222, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 4, 'validation_loss': np.float64(0.21004490144010904), 'macro_f1': np.float64(0.6004581325161665), 'micro_f1': np.float64(0.7537722908093278), 'mae': np.float64(0.31351218945038883)}\n",
      "{'loss': 0.17318235337734222, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 5, 'validation_loss': np.float64(0.21004490144010904), 'macro_f1': np.float64(0.5567088776148329), 'micro_f1': np.float64(0.7238225880201189), 'mae': np.float64(0.36171895071657356)}\n",
      "{'loss': 0.17318235337734222, 'epoch': 8.5, 'step': 6868, 'lr': 7.624030856485954e-06, 'week': 6, 'validation_loss': np.float64(0.21004490144010904), 'macro_f1': np.float64(0.5102484559042941), 'micro_f1': np.float64(0.6907864654778235), 'mae': np.float64(0.4055138399146753)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 9/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.23849865794181824, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 1, 'validation_loss': np.float64(0.2083185490058816), 'macro_f1': np.float64(0.8191776634385995), 'micro_f1': np.float64(0.9032921810699589), 'mae': np.float64(0.13007749269767713)}\n",
      "{'loss': 0.23849865794181824, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 2, 'validation_loss': np.float64(0.2083185490058816), 'macro_f1': np.float64(0.7273341930170633), 'micro_f1': np.float64(0.8405349794238683), 'mae': np.float64(0.1989411776995064)}\n",
      "{'loss': 0.23849865794181824, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 3, 'validation_loss': np.float64(0.2083185490058816), 'macro_f1': np.float64(0.6629718413244697), 'micro_f1': np.float64(0.7958390489254686), 'mae': np.float64(0.25643559501065144)}\n",
      "{'loss': 0.23849865794181824, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 4, 'validation_loss': np.float64(0.2083185490058816), 'macro_f1': np.float64(0.6050360181079684), 'micro_f1': np.float64(0.7568587105624143), 'mae': np.float64(0.3113216018773121)}\n",
      "{'loss': 0.23849865794181824, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 5, 'validation_loss': np.float64(0.2083185490058816), 'macro_f1': np.float64(0.5599744429548728), 'micro_f1': np.float64(0.724851394604481), 'mae': np.float64(0.36138923702207454)}\n",
      "{'loss': 0.23849865794181824, 'epoch': 9.0, 'step': 7272, 'lr': 3.4579257196884897e-06, 'week': 6, 'validation_loss': np.float64(0.2083185490058816), 'macro_f1': np.float64(0.5115702332672408), 'micro_f1': np.float64(0.6915866483767719), 'mae': np.float64(0.40498208617219894)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10/10:  50%|█████     | 404/808 [02:26<22:54,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.27015119791030884, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 1, 'validation_loss': np.float64(0.20880755166644635), 'macro_f1': np.float64(0.8252242120088881), 'micro_f1': np.float64(0.9027206218564243), 'mae': np.float64(0.12898026911622468)}\n",
      "{'loss': 0.27015119791030884, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 2, 'validation_loss': np.float64(0.20880755166644635), 'macro_f1': np.float64(0.7238188539615852), 'micro_f1': np.float64(0.840992226794696), 'mae': np.float64(0.19768315465381342)}\n",
      "{'loss': 0.27015119791030884, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 3, 'validation_loss': np.float64(0.20880755166644635), 'macro_f1': np.float64(0.6643762571319954), 'micro_f1': np.float64(0.7957247370827618), 'mae': np.float64(0.2559755084648536)}\n",
      "{'loss': 0.27015119791030884, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 4, 'validation_loss': np.float64(0.20880755166644635), 'macro_f1': np.float64(0.6052190275425062), 'micro_f1': np.float64(0.7561728395061729), 'mae': np.float64(0.31020283490078376)}\n",
      "{'loss': 0.27015119791030884, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 5, 'validation_loss': np.float64(0.20880755166644635), 'macro_f1': np.float64(0.556127135377502), 'micro_f1': np.float64(0.724851394604481), 'mae': np.float64(0.35903643168746274)}\n",
      "{'loss': 0.27015119791030884, 'epoch': 9.5, 'step': 7676, 'lr': 8.734789157224429e-07, 'week': 6, 'validation_loss': np.float64(0.20880755166644635), 'macro_f1': np.float64(0.5015429024539542), 'micro_f1': np.float64(0.692501143118427), 'mae': np.float64(0.4015098805223201)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 10/10: 100%|██████████| 808/808 [04:53<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.21371784806251526, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 1, 'validation_loss': np.float64(0.20844748243689537), 'macro_f1': np.float64(0.8204156835263084), 'micro_f1': np.float64(0.9023776863283036), 'mae': np.float64(0.12869707940342906)}\n",
      "{'loss': 0.21371784806251526, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 2, 'validation_loss': np.float64(0.20844748243689537), 'macro_f1': np.float64(0.7285784985633622), 'micro_f1': np.float64(0.8413351623228167), 'mae': np.float64(0.1975264953271462)}\n",
      "{'loss': 0.21371784806251526, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 3, 'validation_loss': np.float64(0.20844748243689537), 'macro_f1': np.float64(0.6652830890569245), 'micro_f1': np.float64(0.7967535436671239), 'mae': np.float64(0.2559223513724822)}\n",
      "{'loss': 0.21371784806251526, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 4, 'validation_loss': np.float64(0.20844748243689537), 'macro_f1': np.float64(0.6012065469382237), 'micro_f1': np.float64(0.7564014631915866), 'mae': np.float64(0.3100353408457146)}\n",
      "{'loss': 0.21371784806251526, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 5, 'validation_loss': np.float64(0.20844748243689537), 'macro_f1': np.float64(0.5556403241254756), 'micro_f1': np.float64(0.7251943301326017), 'mae': np.float64(0.3591161923027969)}\n",
      "{'loss': 0.21371784806251526, 'epoch': 10.0, 'step': 8080, 'lr': 2.853990486928992e-10, 'week': 6, 'validation_loss': np.float64(0.20844748243689537), 'macro_f1': np.float64(0.5003630152617276), 'micro_f1': np.float64(0.6921582075903063), 'mae': np.float64(0.40173477063572716)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"using CPU\")\n",
    "static_dim = 0\n",
    "if use_static:\n",
    "    static_dim = X_tabular_train.shape[-1]\n",
    "model = DroughtNetLSTM(\n",
    "    output_weeks,\n",
    "    X_time_train.shape[-1],\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    ffnn_layers,\n",
    "    dropout,\n",
    "    static_dim,\n",
    ")\n",
    "model.to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "if one_cycle:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "counter = 0\n",
    "valid_loss_min = np.inf\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "\n",
    "    for k, (inputs, static, labels) in tqdm(\n",
    "        enumerate(train_loader),\n",
    "        desc=f\"epoch {i+1}/{epochs}\",\n",
    "        total=len(train_loader),\n",
    "    ):\n",
    "        model.train()\n",
    "        counter += 1\n",
    "        if len(inputs) < batch_size:\n",
    "            h = model.init_hidden(len(inputs))\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels, static = (\n",
    "            inputs.to(device),\n",
    "            labels.to(device),\n",
    "            static.to(device),\n",
    "        )\n",
    "        model.zero_grad()\n",
    "        if use_static:\n",
    "            output, h = model(inputs, h, static)\n",
    "        else:\n",
    "            output, h = model(inputs, h)\n",
    "        loss = loss_function(output, labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        if one_cycle:\n",
    "            scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if k == len(train_loader) - 1 or k == (len(train_loader) - 1) // 2:\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                labels = []\n",
    "                preds = []\n",
    "                raw_labels = []\n",
    "                raw_preds = []\n",
    "                for inp, stat, lab in valid_loader:\n",
    "                    if len(inp) < batch_size:\n",
    "                        val_h = model.init_hidden(len(inp))\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    inp, lab, stat = inp.to(device), lab.to(device), stat.to(device)\n",
    "                    if use_static:\n",
    "                        out, val_h = model(inp, val_h, stat)\n",
    "                    else:\n",
    "                        out, val_h = model(inp, val_h)\n",
    "                    val_loss = loss_function(out, lab.float())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                    for labs in lab:\n",
    "                        labels.append([int(l.round()) for l in labs])\n",
    "                        raw_labels.append([float(l) for l in labs])\n",
    "                    for pred in out:\n",
    "                        preds.append([int(p.round()) for p in pred])\n",
    "                        raw_preds.append([float(p) for p in pred])\n",
    "                # log data\n",
    "                labels = np.array(labels)\n",
    "                preds = np.clip(np.array(preds), 0, 5)\n",
    "                raw_preds = np.array(raw_preds)\n",
    "                raw_labels = np.array(raw_labels)\n",
    "                for i in range(output_weeks):\n",
    "                    log_dict = {\n",
    "                        \"loss\": float(loss),\n",
    "                        \"epoch\": counter / len(train_loader),\n",
    "                        \"step\": counter,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                        \"week\": i + 1,\n",
    "                    }\n",
    "                    # w = f'week_{i+1}_'\n",
    "                    w = \"\"\n",
    "                    log_dict[f\"{w}validation_loss\"] = np.mean(val_losses)\n",
    "                    log_dict[f\"{w}macro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"macro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}micro_f1\"] = f1_score(\n",
    "                        labels[:, i], preds[:, i], average=\"micro\"\n",
    "                    )\n",
    "                    log_dict[f\"{w}mae\"] = mean_absolute_error(\n",
    "                        raw_labels[:, i], raw_preds[:, i]\n",
    "                    )\n",
    "                    print(log_dict)\n",
    "                    writer.add_scalars(\"Loss(MSE)\", {'train': loss,\n",
    "                                                     'validation': log_dict[f\"{w}validation_loss\"]},\n",
    "                                                     counter)\n",
    "                    writer.add_scalars(\"F1(MSE)\", {'macro': log_dict[f\"{w}macro_f1\"],\n",
    "                                                   'micro': log_dict[f\"{w}micro_f1\"]},\n",
    "                                                   counter)\n",
    "                    writer.add_scalar(\"MAE\", log_dict[f\"{w}mae\"],\n",
    "                                      counter)\n",
    "                    writer.add_scalar(\"Learning-Rate\", log_dict[\"lr\"],\n",
    "                                      counter)\n",
    "                    for j, f1 in enumerate(\n",
    "                        f1_score(labels[:, i], preds[:, i], average=None)\n",
    "                    ):\n",
    "                        log_dict[f\"{w}{id2class[j]}_f1\"] = f1\n",
    "                    model.train()\n",
    "                if np.mean(val_losses) <= valid_loss_min:\n",
    "                    torch.save(model.state_dict(), \"./state_dict.pt\")\n",
    "                    print(\n",
    "                        \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n",
    "                            valid_loss_min, np.mean(val_losses)\n",
    "                        )\n",
    "                    )\n",
    "                    valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, static=None):\n",
    "    if static is None:\n",
    "        out, _ = model(torch.tensor(x), val_h)\n",
    "    else:\n",
    "        out, _ = model(torch.tensor(x), val_h, static)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation predictions...:   0%|          | 0/69 [00:00<?, ?it/s]/tmp/ipykernel_1060677/3413005742.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out, _ = model(torch.tensor(x), val_h, static)\n",
      "validation predictions...: 100%|██████████| 69/69 [00:11<00:00,  6.22it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_map = {\n",
    "    \"y_pred\": [],\n",
    "    \"y_pred_rounded\": [],\n",
    "    \"fips\": [],\n",
    "    \"date\": [],\n",
    "    \"y_true\": [],\n",
    "    \"week\": [],\n",
    "}\n",
    "i = 0\n",
    "for x, static, y in tqdm(\n",
    "    valid_loader, # ou valid_loader\n",
    "    desc=\"validation predictions...\",\n",
    "):\n",
    "    val_h = tuple([each.data.to(device) for each in model.init_hidden(len(x))])\n",
    "    x, static, y = x.to(device), static.to(device), y.to(device)\n",
    "    with torch.no_grad():\n",
    "        if use_static:\n",
    "            pred = predict(x, static).clone().detach()\n",
    "        else:\n",
    "            pred = predict(x).clone().detach()\n",
    "    for w in range(output_weeks):\n",
    "        dict_map[\"y_pred\"] += [float(p[w]) for p in pred]\n",
    "        dict_map[\"y_pred_rounded\"] += [int(p.round()[w]) for p in pred]\n",
    "        dict_map[\"fips\"] += [f[1][0] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"date\"] += [f[1][1] for f in valid_fips[i : i + len(x)]]\n",
    "        dict_map[\"y_true\"] += [float(item[w]) for item in y]\n",
    "        dict_map[\"week\"] += [w] * len(x)\n",
    "    i += len(x)\n",
    "df = pd.DataFrame(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week 1 MAE 0.133 F1 0.816\n",
      "Week 2 MAE 0.201 F1 0.723\n",
      "Week 3 MAE 0.258 F1 0.65\n",
      "Week 4 MAE 0.312 F1 0.574\n",
      "Week 5 MAE 0.36 F1 0.542\n",
      "Week 6 MAE 0.403 F1 0.499\n"
     ]
    }
   ],
   "source": [
    "for w in range(6):\n",
    "    wdf = df[df['week']==w]\n",
    "    mae = mean_absolute_error(wdf['y_true'], wdf['y_pred']).round(3)\n",
    "    f1 = f1_score(wdf['y_true'].round(),wdf['y_pred'].round(), average='macro').round(3)\n",
    "    print(f\"Week {w+1}\", f\"MAE {mae}\", f\"F1 {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
